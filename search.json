[
  {
    "objectID": "docs/version-control/workflows.html",
    "href": "docs/version-control/workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "Now that we know some basic Git principles, how do we make a workflow for a given project? First, let’s start by talking about some key principles for any workflow, then we can get into some of the specific considerations for a project team at RFF.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#key-principles",
    "href": "docs/version-control/workflows.html#key-principles",
    "title": "Workflows",
    "section": "1 Key Principles",
    "text": "1 Key Principles\n\nUsers should commit changes frequently, with descriptive messages.\n\nFrequent commits make it so that it is easier to find discrete check-points in the repository, rather than having everything committed at the same time.\nTry to have commits provide incremental improvements. For example, a commit might include the addition of a single function, or a 2-line fix to an existing script.\nIf you find yourself coding for hours between commits, consider trying to break up your task into discrete sub-tasks.\n\nPull often to ensure that you are incorporating the changes of others.\nA local clone of a repository should generally ONLY have a single user.\n\nThis is why in the Directory Structure section of the Data Management guidance, we recommend that each user works from a repository clone stored in their own individual subfolder within the L:/ drive project folder.\n\n\nTake care with encoding file paths within a repository. You want other people to be able to use the repository without having to move files around or change all the file paths in the repository. You could either:\n\nUse relative file paths to point to other files that are located in the repository.\nUse absolute file paths (i.e. on a shared network drive like the L drive) and consider making a variable for the path directory, especially if used multiple times. That way, if a collaborator has their data at a different path on their system, they only need to change a single path.\ndata_path = “L:/ProjectName/Data”\ndata_file_1 = joinpath(data_path, “data_file_1.csv”)\ndata_file_2 = joinpath(data_path, “data_file_2.csv”)\nIf you use relative file paths pointing to files outside the repository, specify in the project README how the external files and the repository should be positioned relative to each other.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#workflow-planning",
    "href": "docs/version-control/workflows.html#workflow-planning",
    "title": "Workflows",
    "section": "2 Workflow Planning",
    "text": "2 Workflow Planning\nWhen a project team is deciding to use Git, it is important to think through the following considerations. We recommend answering these questions at the beginning of a version-controlled project.\n\n2.1 Where should each team member store their local repositories?\nHere are two examples of how a project / team might choose to store their repositories.\n\nGenerally Recommended: Each team member keeps their own working version of a repository on a shared drive, i.e. the L drive, which could (but probably shouldn’t) be accessed by other team members., i.e. inside of a larger project directory\n\nPros: individual users’ repositories are available on all lab computers; repos are stored within the project folder, allowing easy access to shared project files (e.g. raw or processed data files)\nCons: files could be accidentally changed by others if they are working in the incorrect version of the repository\n\nEach team member keeps their own working version of a repository somewhere private, either on the C:/ drive of their personal computer, or on the RFF servers, with the intention of no other team members having access to it. This is suitable for teams that have many repositories where it would get very confusing to store all the repositories in the same location for the whole team.\n\nPros: it is “clean”, with no accidental file changes from other users.\nCons: It would require care to be taken for access (file paths) to non-version-controlled data files.\n\n\n\n\n2.2 Where should the GitHub repository be hosted?\n\nDo you work on a team with their own GitHub Organization (i.e. E4ST)?\n\nIf so, verify with the other members of the team that it would be acceptable to use the organization to host the repository\nIf your team doesn’t already have one, does it make sense to create an organization for your team? Are there likely to be multiple repositories that would make sense to group under the same organization? If so, follow the steps in the GitHub Documentation for creating organizations.\n\nDoes this repository belong in the RFF Organization?\n\nGenerally, if the repository is associated with a publication, the code should be forked or transferred to the organization.\nWe suggest creating the repository as a personal repository and changing ownership or forking.\nSee the Data Management section on code publication for additional guidance.\n\nIf it doesn’t make sense to host the repository in an organization, we recommend having one of project members host it with their GitHub account.\n\n\n\n2.3 Should our repository be public or private?\nPublic repositories are visible to anyone, whereas private repositories allows for controlled access. For public repositories, it is still possible to control who is able to contribute to them, while private reporitories allow you to control who can even see them.\nGenerally, select for your repository to be private for repositories that will contain sensitive/proprietary information, and public if the project requires it to be public. Availability of data/code is important to ensure reproducability so making a repository public is a good thing in most cases. If you have sensitive data but would like for the repository to remain public, consider storing code and documentation in the repository, and sensitive data in a different way. See the Sensitive and Proprietary data section It is easy to change from private to public later on, so when in doubt choose private.\nIf you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\n\n\n2.4 Should our team use branches?\nTo learn about branches, what they are for, and how to use them, see the Branching section of the Tutorial. There are a few aspects to consider with this question.\n\nHow many people will be making commits?\n\nIf only one person will be making commits, it may save time and effort to maintain a single branch, without creating additional branches.\nWith more people making commits, using multiple branches may help avoid unnecessary merge conflicts.\n\nIs it important to have a fully functional version at all times?\n\nBranches can be very good for preserving a functional version of the code, while allowing for risk-free experimental features to be developed.\n\nIs it important to have code review for this project?\n\nBranches facilitate streamlined code review, where reviewers only review sections that have changed (via Pull Requests).\n\n\nHere are some examples of when a branch might be useful:\n\nMajor Changes/Experimental Feature: Say you have an idea on how to improve your code, and it requires significantly re-vamping the code. It would make sense to create a branch for this experimental feature, so that you can develop the feature without worrying about making changes to the main branch. Then the whole team can use the Pull Request to evaluate whether or not they’d like to merge into the main branch.\nNew Team Member: You have an existing model that is already working well. A new researcher joins the project team and is tasked with a code improvement. They are new to the model, so you want to review their changes carefully before applying them to the main branch. It makes sense to have them create a branch for their development, and review it with the pull request.\nConcurrent Development: Your team has a couple of features that need to be developed in tandem. To prevent extensive conflicts and streamline the process of combining the new features, each feature gets its own branch.\nProject: If your team has a project that requires making project-specific changes, it could be a good idea to create a project branch, with no intention to merge the project branch into main. ⚠️However, for features that would be useful to have in the main branch, implementing those features in the project branch would make it difficult to merge them back into the main branch later on. In that case, it is best to implement those features in the main branch or in a separate branch (to be merged with main), and merge the main branch back into the project branch. It is difficult to selectively merge changes from the project branch into the main branch.\n\n\n\n2.5 If using branches, how do we want to handle code review?\nIt would be good to talk about the following questions with your team:\n\nShould code review be required for every change to the code?\n\nOne example of this would be requiring that every change be made in a branch and submitted via Pull Request, and that at least one person who did not author the Pull Request must approve it in order for the branch to be merged into the main branch.\nYour team may decide to only require a Pull Request and code review for major features.\n\nWho is allowed/required to approve Pull Requests?\n\nOne example would be to require every Pull Request created by a newer team member to be reviewed by a more experienced team member.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html",
    "href": "docs/version-control/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Before using GitHub, it is necessary to have a GitHub account. To make an account, go to the GitHub website and click Sign Up, following the prompts to create your account. As part of the process you will need to verify your email address. Note that GitHub accounts are associated with people, along the lines of a LinkedIn account, and it may be helpful to have a single personal GitHub account even if you plan to use it outside of your work at RFF. It is simple to associate multiple email addresses with your account later on, so it is unimportant which email address you create the account with.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#make-a-github-account",
    "href": "docs/version-control/setup.html#make-a-github-account",
    "title": "Setup",
    "section": "",
    "text": "Before using GitHub, it is necessary to have a GitHub account. To make an account, go to the GitHub website and click Sign Up, following the prompts to create your account. As part of the process you will need to verify your email address. Note that GitHub accounts are associated with people, along the lines of a LinkedIn account, and it may be helpful to have a single personal GitHub account even if you plan to use it outside of your work at RFF. It is simple to associate multiple email addresses with your account later on, so it is unimportant which email address you create the account with.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#git-installation-5-10-minutes",
    "href": "docs/version-control/setup.html#git-installation-5-10-minutes",
    "title": "Setup",
    "section": "2 Git Installation (5-10 minutes)",
    "text": "2 Git Installation (5-10 minutes)\nIt is necessary to install Git onto any computer from which you wish to interact with local repositories. Often, IT has installed Git on RFF computers. You can check to see if you have Git installed by pressing the Windows key and typing “Git”, and see if applications like “Git Bash”, “Git Gui”, and “Git CMD” pop up.\nIf Git is not yet installed, it is very simple to install!\n\nNavigate to this website and download the latest version of Git.\nDouble click the downloaded installer to launch the install menu.\nAccept all the defaults except the following:\n\n\nDefault Editor: By default, Git uses the “Vim” editor, which a powerful, but very old-school and difficult to use. It is recommended to select “notepad” or some other editor that you are familiar with. \n\nDefault Initial Branch: By default, Git uses an initial branch name of “master”. The industry has been moving towards more inclusive branch naming, and we recommend changing the default to “main”. \n\n\n\nUpon completing the install, you will be prompted to restart your computer.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#choose-a-user-interface",
    "href": "docs/version-control/setup.html#choose-a-user-interface",
    "title": "Setup",
    "section": "3 Choose a User Interface",
    "text": "3 Choose a User Interface\nWhen you install Git, it generally will already install a couple of applications you can use to interface with git.\n\nGit Bash/CMD – (Installed with Git) These options allow you to enter text-based commands for Git in an interface similar to the command prompt window in Windows. Git Bash allows for git commands in addition to standard UNIX commands, whereas Git CMD allows for git commands in addition to standard windows CMD prompt commands. This is a great environment for learning the main processes of Git as a new user, and we will use Git Bash for tutorials on this site.\n\n\n\n\nGit GUI – (Installed with Git) This is a minimalist graphical user interface letting you use git without having to use a command line. It also allows for simplistic visualization of branches. While our tutorials will use Git Bash, it should be fairly straightforward to use the Git GUI instead.\n\n\n\n\nRStudio Git Integration - (Installed Separately) RStudio, a popular editor for R code, comes with a Git user interface built in. This may be a good option for R users once they have learned the fundamentals with Git Bash.\n\nIf you wish to learn a different user interface, check out the list here, and check with the IT team before installing. Most of the concepts will be fairly similar to what we share in the guide.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "href": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "title": "Setup",
    "section": "4 Optional: Set up SSH Keys",
    "text": "4 Optional: Set up SSH Keys\nIf you will be using Git with private repositories, it is worth setting up a key on your PC so that you can freely pull and push from a remote repository without having to enter your password each time. Follow these instructions for setting up an SSH key, and these instructions for adding it to your GitHub account.\nThere is a small configuration change we need in order to enable the SSH protocol to work with RFF IT systems. From a git bash window, run the following lines:\n\ntouch ~/.ssh/config - This command creates a new empty file in your user home folder /.ssh directory.\nprintf \"Host github.com\\n  Hostname ssh.github.com\\n  Port 443\" &gt;&gt; ~/.ssh/config - This command writes 3 lines to the newly created config file.\n\nNote that these commands can be run from a Git Bash window open to any directory. The ~ sign represents your home directory. After running the commands, if you were to open the file in a text editor, it would look like:\nHost github.com\n  Hostname ssh.github.com\n  Port 443\nOnce you have that set up, when cloning a repository to your local folder, simply select the “SSH” option before copying the URL.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html",
    "href": "docs/version-control/faq.html",
    "title": "Tips and FAQ",
    "section": "",
    "text": "In general, git works well with:\n\nSmall files. It is not advisable to store large data files in git, because it makes the repository large and subsequent operations get slow and unwieldy. For storing large files, see the Storage Options section\nHuman-readable files. One of the biggest perks of version control is the ability to see specific changes to files. That feature only works for files composed of human-readable characters, examples of which include comma-separated values (.csv) and plain text (.txt), as well as source code. However, there are some commonly-used filetypes that are not human-readable, and thus do not work very well with version control, including:\n\nAny Microsoft office format (.docx, .xlsx, .pptx, etc.)\nMATLAB notebooks (.mlx)\n\n\n\n\n\nGreat question! If I may want to add that file to the repository later, I could simply avoid staging it for current commit, and stage it when it is ready later. If that file or folder should never be tracked, you can also create a file called .gitignore in the repository.\nLearn more about .gitignore files.\n\n\n\nThe importance and benefits of code review was presented in the Software Quality section of the guidance &lt;TODO: LINK TO SUBSECTION&gt;. If you don’t have anyone working on a project with you who is available to review your code, feel free to reach out to &lt;TODO: figure out if this is something we can support&gt;",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#faq",
    "href": "docs/version-control/faq.html#faq",
    "title": "Tips and FAQ",
    "section": "",
    "text": "In general, git works well with:\n\nSmall files. It is not advisable to store large data files in git, because it makes the repository large and subsequent operations get slow and unwieldy. For storing large files, see the Storage Options section\nHuman-readable files. One of the biggest perks of version control is the ability to see specific changes to files. That feature only works for files composed of human-readable characters, examples of which include comma-separated values (.csv) and plain text (.txt), as well as source code. However, there are some commonly-used filetypes that are not human-readable, and thus do not work very well with version control, including:\n\nAny Microsoft office format (.docx, .xlsx, .pptx, etc.)\nMATLAB notebooks (.mlx)\n\n\n\n\n\nGreat question! If I may want to add that file to the repository later, I could simply avoid staging it for current commit, and stage it when it is ready later. If that file or folder should never be tracked, you can also create a file called .gitignore in the repository.\nLearn more about .gitignore files.\n\n\n\nThe importance and benefits of code review was presented in the Software Quality section of the guidance &lt;TODO: LINK TO SUBSECTION&gt;. If you don’t have anyone working on a project with you who is available to review your code, feel free to reach out to &lt;TODO: figure out if this is something we can support&gt;",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#tips",
    "href": "docs/version-control/faq.html#tips",
    "title": "Tips and FAQ",
    "section": "2 Tips",
    "text": "2 Tips\n\n2.1 Commit Messages\nIf you want to learn to write better commit messages, this is a great resource!\n\n\n2.2 .gitignore Files\nGit provides us with the option of including a file that specifies files, filetypes, or folders, to automatically exclude from version control. Files covered by the .gitignore file will not show up when you check the git status, but they may still be manually added and committed. These .gitignore files can be life-savers, preventing the user from accidentally committing large files, which can create problems down the road.\nInside the file is simply the names of the file(s), folder(s), or file extension types (i.e. *.log) that Git will ignore. Generally, it is good to ignore files/folders that get automatically generated by existing scripts, are too big for Git, or are not relevant to other users of the repository. Here are some examples of files that you may consider adding to a .gitignore file:\n\na folder containing intermediate data files\na folder containing dependencies that get downloaded using a script\na project file created by a code editor\n\nThe easiest way to set up a .gitignore file is to select the template corresponding to your desired programming language upon creation of the repository. To add a .gitignore file to an existing repository, you can find the GitHub .gitignore template corresponding to your programming language.\n\n\n\n\n\n\nTipExpand to see a sample of the .gitignore template for R, from GitHub’s Website\n\n\n\n\n\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n.RDataTmp\n\n# User-specific files\n.Ruserdata\n\n# Example code in package build process\n*-Ex.R\n\n# Output files from R CMD build\n/*.tar.gz\n\n# Output files from R CMD check\n/*.Rcheck/\n\n# RStudio files\n.Rproj.user/\n\n# produced vignettes\nvignettes/*.html\nvignettes/*.pdf\n\n# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3\n.httr-oauth\n\n# knitr and R markdown default cache directories\n*_cache/\n/cache/\n\n# Temporary files created by R markdown\n*.utf8.md\n*.knit.md\n\n# R Environment Variables\n.Renviron\n\n# pkgdown site\ndocs/\n\n# translation temp files\npo/*~\n\n# RStudio Connect folder\nrsconnect/\n\n\n\nWe also recommend appending any output file types that can grow large to the .gitignore file. For example, you could append the following to your .gitignore file:\n# Output File Types\n*.svg\n*.csv\n*.xlsx\n\n\n2.3 Branch Naming\nHere is a great resource for learning more about branch naming conventions.\n\n\n2.4 Tags\nSometimes it is helpful to name a specific commit of a repository. For example, say you have a version of the repository that you used to generate analysis for a publication. You can easily use the git tag command to “name” that specific commit. For example,\ngit tag &lt;tag-name&gt;\ngit push --tags\n\n\n2.5 Aliases\nAn Alias is like creating a shortcut for a git command so that you don’t have to type out the whole command.\nFor example, we type git status fairly often and it would be nice to simply type git st. We can tell Git that we want to create an alias called git st that simply executes git status instead. Here is how we tell Git Bash to save that into its configuration file.\ngit config --global alias.st status\n\nCommit Message Alias\nThis alias makes it easier to type a commit message without opening notepad.\n\nSetup\ngit config --global alias.cm 'commit -m'\n\n\nUsage\ngit cm \"Commit message here\"\n\n\n\nTree Visualization\nGit Bash offers tools to visualize commits/branches. Usually these are based on the command git log, which has many different options for customization. Below is a customized git log command that is nicely color-coded, shows user, date, and commit message.\ngit log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\n\n\nAs you can see, it prints the commit hash, the date of the commit, the author, the name of the branch, and the commit message, as well as the branching structure at the far left.\n\nSetup\nTo make an alias for it with the name tree, enter the following command in Git Bash:\ngit config --global alias.tree \"log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\"\n\n\nUsage\nNow, to print the visualization, simply enter:\ngit tree\ngit tree -10     # this option makes it only print the latest 10 commits.",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/software/computational-efficiency.html",
    "href": "docs/software/computational-efficiency.html",
    "title": "Computational Efficiency",
    "section": "",
    "text": "Projects at RFF increasingly rely on large datasets and computationally intensive models. As a result, the performance of analysis code may be constrained by available computing resources. Inefficient workflows can take hours or days to run, consume excessive memory, or fail altogether. In this section, we provide guidance on diagnosing and improving computational efficiency, broadly referring to how effectively code uses available resources. These resources primarily include time (how long a task takes to run) and memory (how much data can be processed at once).\nIn this context, memory refers to the computer’s Random Access Memory (RAM). RAM is a fast workspace where data and program objects are stored while code is executing. This is fundamentally different from disk storage (such as hard drives or SSDs), which is designed for long-term storage. Because read and write operations to RAM are orders of magnitude faster than those to disk, most statistical computing environments—including R and Python—load data into memory before performing computations. Therefore, available physical memory plays an important role in determining the size of datasets and intermediate objects that can be processed efficiently. When memory limits are exceeded, performance may degrade sharply or the session may fail altogether.",
    "crumbs": [
      "Software Quality",
      "Computational Efficiency"
    ]
  },
  {
    "objectID": "docs/software/computational-efficiency.html#general-practices",
    "href": "docs/software/computational-efficiency.html#general-practices",
    "title": "Computational Efficiency",
    "section": "1 General practices",
    "text": "1 General practices\nThe need to manage computational efficiency differs by project and should be approached on a case-by-case basis. Not all projects require fully optimizing the computational efficiency of the code. The figure here organizes a set of general practices in a flow chart to help researchers decide on next steps.\n\n\n\nFlow chart for managing computational efficiency\n\n\n\nAssess computational need against capacity.\nMany analyses can be anticipated to stretch available computational resources. In such cases, pay special attention to managing computational efficiency from the start.\nChoose a server computer based on capacity needs.\nRFF’s server computers differ in CPU and memory capacity (see here for a list of server computers). Choose a machine appropriate to the expected workload. Before running any analysis, check Task Manager to ensure that active sessions from other users leave sufficient capacity for your tasks.\nTest the code for correctness and efficiency before full implementation.\nBefore running the full analysis, test the code on a small subset of the data. This helps estimate expected runtime and identify potential errors early, preventing wasted time on long re-runs.\nMonitor the computational process.\nMonitor resource usage during execution to determine whether revisions are needed to improve efficiency or avoid exceeding the machine’s capacity.\nRevise code to improve efficiency.\nStrategies may include using functions and loops to reduce memory usage, or implementing parallel processing when tasks involve many identical subtasks. The specific revisions will depend on the programming language and packages used. We provide more specific suggestions for programming in R below.\nCoordinate with the IT team if needed.\nIf none of the above solutions suffice, the IT team can assist in configuring computational resources to meet the analysis needs.",
    "crumbs": [
      "Software Quality",
      "Computational Efficiency"
    ]
  },
  {
    "objectID": "docs/software/computational-efficiency.html#r-coding-practices-for-efficiency",
    "href": "docs/software/computational-efficiency.html#r-coding-practices-for-efficiency",
    "title": "Computational Efficiency",
    "section": "2 R coding practices for efficiency",
    "text": "2 R coding practices for efficiency\nBelow we outline a set of recommended practices for improving R code efficiency. With the exception of parallel computing, these are best treated as default habits during development, rather than techniques reserved for when performance problems emerge.\n\n2.1 Use vectorization over loops\nIn R, vectorization means operating on entire vectors, matrices, or data structures at once rather than iterating element by element in explicit loops. Vectorization is more efficient and should be used instead of for loops whenever possible. Applications include:\n\nMany base R functions are inherently vectorized, including arithmetic operations (e.g., +, -, *) , summary functions (e.g., sum(), mean(), rowSums()), and math functions (e.g., log(), exp(), sqrt()).\nUse vectorized conditions such as ifelse() over a list of instead of looping with if / else.\nUse apply family functions (e.g., apply(), lapply(), or sapply()) over vectors or lists rather than looping.\n\nExample: lapply() vs. a loop to standardize multiple numeric columns\ndf &lt;- data.frame(\n  a = rnorm(1e5),\n  b = rnorm(1e5),\n  c = rnorm(1e5),\n  group = sample(letters[1:3], 1e5, replace = TRUE)\n)\n\nnum_cols &lt;- c(\"a\", \"b\", \"c\")\n\n# Using a loop (inefficient)\nfor (col in num_cols) {\n  df[[col]] &lt;- (df[[col]] - mean(df[[col]])) / sd(df[[col]])\n}\n\n# Using lapply() (efficient)\ndf[num_cols] &lt;- lapply(df[num_cols], function(x) {\n  (x - mean(x)) / sd(x)\n})\n\n\n2.2 Avoid growing objects inside loops\nIn R, repeatedly expanding an object inside a loop (e.g., using c(), rbind(), or append() on each iteration) is slow and memory-intensive because R often needs to allocate new memory and copy the existing object each time it grows. A more efficient approach is to preallocate objects to their final size.\nExample: grow a vector in a loop vs. preallocate\n# Grow the vector each iteration (inefficient)\nvec &lt;- numeric(0)\nfor (i in 1:1000) {\n  vec &lt;- c(vec, i^2)\n}\n\n# Preallocate and fill (efficient)\nvec &lt;- numeric(1000)\nfor (i in 1:1000) {\n  vec[i] &lt;- i^2\n}\nWhen combining many results, store them in a list and use do.call(rbind, ...) or rbindlist() rather than repeatedly appending rows.\nExample: collect results, then combine once\nresult_list &lt;- vector(\"list\", 1000)\n\nfor (i in 1:1000) {\n  result_list[[i]] &lt;- data.frame(id = i, value = i^2)\n}\n\nresult &lt;- do.call(rbind, result_list)\n\n\n2.3 Use efficient data structures\nA data structure is a format for organizing, retrieving, and storing data in a computer. Some common R data structures include vectors, matrices, lists, data.frames, and data.tables. Selecting appropriate data structures can substantially improve runtime and memory efficiency, particularly when working with large datasets. Some R data structures are optimized for specific operations and should be preferred when performance matters. For example:\n\nUse data.table for large tabular data rather than data.frame when speed and memory efficiency are important. data.table is optimized for fast grouping, joins, and in-place updates.\nUse factors for categorical variables instead of character strings when appropriate. Factors store categories as integer codes, reducing memory usage in grouping and modeling operations.\n\n\n\n2.4 Profile and benchmark code\nProfiling and benchmarking can help pinpoint runtime and memory bottlenecks and quantify whether revisions improve performance. Options include:\n\nFor quick timing checks, use the built-in base R function system.time().\n\nsystem.time({\n  out &lt;- df$x^2 + log(df$y)\n})\n\nFor workflows with multiple steps, tictoc can help track the runtime of individual blocks in longer scripts.\n\nlibrary(tictoc)\n\ntic(\"Step 1: transform\")\ndf$x2 &lt;- df$x^2\ntoc()\n\ntic(\"Step 2: summarize\")\nm &lt;- mean(df$x2)\ntoc()\n\nFor complex scripts with multiple potential bottlenecks, profvis provides a visual breakdown of time spent in functions and can also identify memory-intensive operations.\n\nlibrary(profvis)\n\ntimes &lt;- 4e5\ncols &lt;- 150\ndata &lt;- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))\ndata &lt;- cbind(id = paste0(\"g\", seq_len(times)), data)\n\nprofvis({\n  data1 &lt;- data   # Store in another variable for this run\n  \n  # Get column means\n  means &lt;- apply(data1[, names(data1) != \"id\"], 2, mean)\n  \n  # Subtract mean from each column\n  for (i in seq_along(means)) {\n    data1[, names(data1) != \"id\"][, i] &lt;- data1[, names(data1) != \"id\"][, i] - means[i]\n  }\n})\n\n\n2.5 Use efficient input/output (I/O)\nInput/output (I/O) operations (i.e., reading data from disk and writing results) can be a major source of computational overhead in data-intensive workflows. Inefficient I/O can dominate runtime even when the underlying analysis code is well optimized. The following practices help reduce unnecessary disk access:\n\nUse high-performance read and write functions for large datasets. Functions such as data.table::fread() / fwrite() or readr::read_csv() are generally more efficient than base R functions like read.csv(), particularly for large files.\nSave intermediate data to avoid repeated reads of large raw files. Save them in formats such as .fst, .rds, or .parquet, which are typically faster to read and write and more memory-efficient than plain text formats (e.g., .csv, .txt). See Data & File Types for more details on data formats.\nRead only the data you need. Some read functions (e.g., data.table::fread() and readr::read_csv()) can scan the data structure and selectively load columns or rows without reading the entire dataset into memory.\n\nExample: read only selected columns\nDT &lt;- fread(\"large_data.csv\", select = c(\"id\", \"year\", \"value\"))\n\n\n2.6 Manage garbage collection\nR automatically manages memory through garbage collection (GC), a process that reclaims memory from objects that are no longer referenced. Garbage collection is triggered based on internal thresholds and requires R to pause execution and scan memory for unused objects. While GC typically runs in the background, it can become a performance bottleneck in memory-intensive workflows, particularly when many large temporary objects are created and discarded. GC-related issues often appear as unexpectedly long runtimes, intermittent pauses during execution, or persistently high memory usage rather than explicit error messages.\nThe most effective way to limit the runtime cost of garbage collection is to reduce unnecessary object creation. Avoiding large intermediate objects and minimizing repeated allocation and copying can substantially lower GC overhead.\nR also allows users to trigger garbage collection explicitly via the gc() function. In most cases, automatic garbage collection is sufficient. Explicit calls to gc() are best reserved for two situations:\n\nAfter removing very large objects that are no longer needed\nBefore starting a memory-intensive computation\n\nBecause garbage collection itself is computationally expensive, frequent or routine calls can substantially slow execution, especially inside loops. The gc() function should therefore be used deliberately and sparingly.\nExample: garbage collection after removing large objects\nrm(large_object)\ngc()\n\n\n2.7 Use parallel computation when appropriate\nParallel computation can substantially reduce runtime by distributing independent tasks across multiple CPU cores. However, it increases coding complexity and resource demands and should therefore be treated as an optimization step rather than a default choice. Parallelization is possible when the same operation can be applied independently across many units, such as:\n\nBatch processing of many files, for example workflows that read, clean, or transform large numbers of similarly structured files.\nUnit-level analyses, such as applying the same procedure separately to spatial units (e.g., watersheds) or administrative units (e.g., states, ZIP codes).\nSimulation-based analyses, including Monte Carlo simulations, bootstrap procedures, or permutation tests that involve many repeated runs of the same computation with different random draws.\n\nWhen using parallel computation, pay particular attention to the following considerations:\n\nMemory usage and data transfer. Each parallel worker may require its own copy of data, which can substantially increase memory consumption. Parallelization may therefore be counterproductive when working with very large objects or on machines with limited memory.\nShared computing environments. When working on shared servers, ensure that parallel execution does not monopolize system resources or interfere with other users. Limit the number of cores used when appropriate.\n\nIn R, packages such as future, furrr, or base R’s parallel provide relatively simple interfaces for parallel execution without requiring low-level thread management.\nExample: parallelizing independent tasks with future and furrr\nlibrary(future)\nlibrary(furrr)\n\n# Set up a parallel plan (adjust workers as appropriate)\nplan(multisession, workers = 4)\n\nunits &lt;- unique(df$county_id)\n\n# Apply the same function independently to each unit\nresults &lt;- future_map(units, function(u) {\n  sub &lt;- df[df$county_id == u, ]\n  mean(sub$value, na.rm = TRUE)\n})\nIn this example, each county is processed independently, making the task well suited for parallel execution. The results are equivalent to those from a sequential loop but can be completed substantially faster when the number of units is large and each task is computationally intensive. Note that computing a mean by county is typically a small and fast operation, and parallelization might not be necessary in practice; this example is for demonstration purposes.\n\n\n2.8 Additional resources\n\nImproving performance from Advanced R by Hadley Wickham\n\nEfficient R Programming by Colin Gillespie",
    "crumbs": [
      "Software Quality",
      "Computational Efficiency"
    ]
  },
  {
    "objectID": "docs/software/readability.html",
    "href": "docs/software/readability.html",
    "title": "Readability",
    "section": "",
    "text": "“Good coding style is like correct punctuation: you can manage without it,\nbutitsuremakesthingseasiertoread.”\n— Tidyverse Style Guide\nReadable code reduces the time collaborators and future developers spend deciphering complex code and ensures continuity even if the original author is unavailable. To accomplish this, we recommend:",
    "crumbs": [
      "Software Quality",
      "Readability"
    ]
  },
  {
    "objectID": "docs/software/readability.html#modularizing-code",
    "href": "docs/software/readability.html#modularizing-code",
    "title": "Readability",
    "section": "1 Modularizing code",
    "text": "1 Modularizing code\nModularizing code is the practice of organizing an entire research project into small, self-contained, and logically structured components, each with a clear and focused purpose. This applies at multiple levels: separating the overall codebase into well-defined scripts (e.g., raw data acquisition and import, cleaning, analysis, visualization), breaking scripts into coherent code blocks, and further decomposing repeated or complex operations into functions with clearly defined inputs and outputs. Together, these layers of modularization make the structure of the project transparent and easier for others to understand, review, and reuse.\nEach script, code block, or function should be accompanied by descriptive in-line comments or documentation explaining its role, assumptions, and expected behavior. Modularization also improves debugging and maintenance: when something goes wrong, issues can be isolated to a specific component rather than requiring developers to trace through the entire codebase. Over time, this structure supports collaborative development, facilitates testing, and allows individual pieces of the workflow to evolve without disrupting the rest of the project.",
    "crumbs": [
      "Software Quality",
      "Readability"
    ]
  },
  {
    "objectID": "docs/software/readability.html#consistent-code-style",
    "href": "docs/software/readability.html#consistent-code-style",
    "title": "Readability",
    "section": "2 Consistent code style",
    "text": "2 Consistent code style\nCode style refers to the conventions that govern how code is written and formatted. This includes:\n\nformatting choices (e.g., indentation, spacing, line length),\nnaming conventions (for variables, functions, datasets, etc.), and\ndocumentation and commenting practices.\n\nWhile personal preferences vary, the key to readability is consistency.\nBelow are established style guides relevant to programming languages commonly used at RFF:\n\nR: Tidyverse Style Guide by Hadley Wickham\n\nPython: Google Python Style Guide\n\nStata: Suggestions on Stata programming style by Nicholas Fox\n\nOther languages: Google style guides for other languages\n\nIn particular, we recommend using consistent, distinctive, and meaningful names for variables, functions, datasets, and files:\n\nVariable or object names should be descriptive of their content, avoiding overly vague or generic terms (e.g., discount_rate instead of val)\nFunction names should describe their action or output (e.g., calculate_average_price()).\nDatasets and files should follow a systematic naming pattern that includes relevant identifiers (e.g., county_population_2022.csv instead of data_final.csv). See also the Naming folders, files, and scripts subsection under Data Management.",
    "crumbs": [
      "Software Quality",
      "Readability"
    ]
  },
  {
    "objectID": "docs/software/readability.html#in-code-documentation",
    "href": "docs/software/readability.html#in-code-documentation",
    "title": "Readability",
    "section": "3 In-code documentation",
    "text": "3 In-code documentation\nIn-code documentation refers to comments written directly within the source code to explain its purpose, functionality, and usage. We recommend incorporating three types of documentation:\n\nScript headers\nInclude a header at the top of each script outlining key metadata such as the script’s objective, author, and start date.\nBlock-level comments\nUse comments to describe the intent and logic of each major code block.\nInline comments\nAdd comments on individual lines of code, especially when the functionality is not obvious or when there are potential limitations.",
    "crumbs": [
      "Software Quality",
      "Readability"
    ]
  },
  {
    "objectID": "docs/software/accuracy.html",
    "href": "docs/software/accuracy.html",
    "title": "Accuracy",
    "section": "",
    "text": "Accuracy refers to the correctness and precision of written code in executing its intended functions without errors or unintended consequences. Below we discuss three practices that promote accuracy, particularly in collaborative environments typical of RFF projects:",
    "crumbs": [
      "Software Quality",
      "Accuracy"
    ]
  },
  {
    "objectID": "docs/software/accuracy.html#pseudocoding",
    "href": "docs/software/accuracy.html#pseudocoding",
    "title": "Accuracy",
    "section": "1 Pseudocoding",
    "text": "1 Pseudocoding\nPseudocode is a step-by-step, high-level description of an algorithm written in plain English. It resembles a programming language but is not actual code. Its main purpose is to help researchers clarify and organize the logical structure of a complex algorithm before writing any executable code. In a research team, pseudocode also facilitates consensus among collaborators. The author of the pseudocode does not have to be the person who eventually writes the code, but all collaborators should review and agree on the pseudocode before it is translated into a programming language.\nWe recommend using pseudocoding in situations where shared understanding is the primary goal, such as collaborative learning, planning or reasoning through complex analytical tasks, and jointly establishing methodological choices before implementation.\n\nExample: psuedocode and code\nPsuedocode\n# Goal: Download migration data for years 2011-2021 from IRS website and save to disk\n\nLoad libraries\nSet working directory\n\n# Create function to download to IRS data\ndownload_irs_data &lt;- function(IRS data year, file destination) {\n  url &lt;- Web address to IRS data for IRS data year \n  download file at url and save to file destination\n}\n\n# Execute function for years 2011-2021\ndst &lt;- selected destination\ncreate dst folder if it doesn't already exist \n\nfor year in 2011-2021 { \n  download_irs_data(IRS data year = year, file destination = dst)\n}\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr) \n\n# Set the new working directory to path where script is saved\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path)) \n\n# Function to download IRS inflow migration data\ndownload_irs_data &lt;- function(year1, dst) {\n  year2 &lt;- sprintf(\"%02d\", (year1 + 1) %% 100) year1 &lt;- sprintf(\"%02d\", year1 %% 100)\n  link &lt;- sprintf(\"https://www.irs.gov/pub/irs-soi/countyinflow%s%s.csv\", year1, year2) \n  download.file(link, sprintf(\"%s/countyinflow%s%s.csv\", dst, year1, year2))\n}\n\n# Download inflow data for years 2011-2021\ndst &lt;- \"migration-data\" \ndir.create(dst, showWarnings = FALSE, recursive = TRUE) \nlapply(seq(11, 21), download_irs_data, dst = dst)",
    "crumbs": [
      "Software Quality",
      "Accuracy"
    ]
  },
  {
    "objectID": "docs/software/accuracy.html#testing-and-debugging-during-code-development",
    "href": "docs/software/accuracy.html#testing-and-debugging-during-code-development",
    "title": "Accuracy",
    "section": "2 Testing and debugging during code development",
    "text": "2 Testing and debugging during code development\nRobust code development requires systematic attention to both testing and debugging. Debugging is essential not only when execution halts with an error, but also to ensure that the program achieves its intended purpose. Code that runs without syntax errors may still contain flawed logic and produce misleading results. As best practice, each step of the analysis should be tested and verified. While the exact approach depends on the analysis, systematically checking intermediate outputs and validating assumptions at every stage helps identify and resolve errors early, improving the reliability of the final results.\nHere are some general best practices:\n\nPay attention to error and warning messages.\nSoftware systems often provide error and warning messages that indicate where and why a problem occurs. These messages can guide you toward a fix. Even warnings that do not stop execution may flag subtle issues that could lead to incorrect results later.\nUse logs and diagnostic print statements. Temporary print statements or structured logging can help verify assumptions, inspect intermediate results, and identify where errors arise during code development. In long or computationally intensive loops, logging can be useful for tracking progress, diagnosing performance bottlenecks, and pinpointing the specific step or iteration that is failing.\nTest incrementally.\nBuild and run code in small, manageable pieces rather than writing long scripts all at once. This makes it easier to isolate where problems occur. Individual functions should also be validated using test cases with known inputs and expected outputs.\nInspect intermediate and final outputs.\nReview summaries, counts, and simple plots at key steps to confirm that data transformations behave as expected. Unexpected values, incorrect data types, missing data, or extreme outliers often indicate problems in earlier processing steps and should be investigated before proceeding.\nCheck data dimensions.\nKeep track of the number of rows and columns in your data, especially after merges, joins, reshaping, or filtering. Unexpected changes in dimensions often signal data loss, duplication, or unintended filtering.\n\nAlso see Quality and Preparation under Data Management for related discussion.",
    "crumbs": [
      "Software Quality",
      "Accuracy"
    ]
  },
  {
    "objectID": "docs/software/accuracy.html#code-review",
    "href": "docs/software/accuracy.html#code-review",
    "title": "Accuracy",
    "section": "3 Code review",
    "text": "3 Code review\nCode review is the practice of having a colleague—someone other than the original author—examine major coding components. This process supports mutual learning, helps catch bugs early, and identifies opportunities for improvement. We recommend setting up a code review system at the start of a project so that review becomes a regular and expected part of the workflow.\n\nCode Review Approaches\nCode review can take different forms depending on the team’s workflow and preferences:\n\nGitHub pull requests.\nPull requests are a GitHub feature that allow programmers to propose changes to the code and request feedback before merging. This is often the most efficient and structured way to conduct review for teams using Git. Reviewers can add comments directly on specific lines of code within a pull request. Also see Pull Requests under Version Control for related discussion.\n\n\n\n\n\nSource: Reviewing proposed changes in a pull request\n\nWritten feedback. For relatively straightforward code or experienced programmers, reviews can be exchanged through email or other written channels.\nIn-person meetings. When code is complex and requires alignment among team members, in-person walkthroughs can cut down on written exchanges.\nPair programming. Two programmers collaborate directly on a single script. This method is especially valuable as a hands-on learning opportunity for less experienced team members.\nReplication. A team member reruns the analysis to verify that results are reproducible. This provides strong assurance of transparency, though full replication may be impractical for code with very long runtimes—in such cases, partial replication can still be useful.\nCheck output. A reviewer inspects key outputs—such as tables, figures, or summary statistics—to confirm they make sense given the intended analysis.\n\nWhichever method is used, it is important to ensure that all team members have access to the code base and that review responsibilities are clearly assigned.\n\n\nCode review checklist\n\n\nWhen reviewing code, consider the following:\n\nCorrectness: Does the code do what it’s supposed to? Are edge cases handled?\nClarity: Are variable and function names clear? Is the logic easy to follow?\nReproducibility: Can someone else run the code with the provided inputs and get the same result?\nEfficiency: Is the code written in a reasonably efficient way (avoiding unnecessary loops or redundancy)?\nConsistency: Does the code follow agreed formatting, naming, and documentation conventions?\nDocumentation: Are inputs, outputs, and assumptions documented? Are comments clear and useful?\n\n\n\nAdditional resources\n\nGoogle’s code review guidelines\nTidyteam code review principles\nGithub - Code Review\nGithub Copilot",
    "crumbs": [
      "Software Quality",
      "Accuracy"
    ]
  },
  {
    "objectID": "docs/software/index.html",
    "href": "docs/software/index.html",
    "title": "Software Quality",
    "section": "",
    "text": "Due to the computational nature of our research, software development (writing code and scripts) is a primary component of everyday work at RFF. Best practices in scientific coding are designed to:\n\nensure accuracy and minimize errors,\nenhance code readability and flexibility,\nimprove reproducibility, and\nimprove efficiency in computation.\n\nThe other components of this guidance (Data Management and Version Control) are all part of practices for high-quality software. This section focuses specifically on the code-writing process, providing general principles to promote robust coding practices while recognizing that best practices vary by discipline and by software/language.",
    "crumbs": [
      "Software Quality"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html",
    "href": "docs/data-management/publication.html",
    "title": "Publication",
    "section": "",
    "text": "Publication is making code/data available to the broader community, often through formal dissemination channels such as data repositories, journal articles, or public databases. Publication ensures that data is discoverable and can be accessed by other researchers, stakeholders, or the public. Documentation and metadata are included to facilitate understanding and reuse. Publication may also involve adherence to specific standards and best practices to enhance the visibility and impact of the data.\nWhile RFF does not mandate the publication of code and data, it is highly encouraged.\nIncreasingly, journals require code and data to be submitted along with the article. In addition, many funders and stakeholders value open source software and data availability. Planning for this in the early stages of a project facilitates reproducibility, access, and the ability of others to use and cite your work.",
    "crumbs": [
      "Data Management",
      "Publication"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html#licensing",
    "href": "docs/data-management/publication.html#licensing",
    "title": "Publication",
    "section": "1 Licensing",
    "text": "1 Licensing\nA well-chosen license clarifies permissions, prevents misunderstandings, and encourages responsible use. The next section provides guidance on selecting and attaching appropriate licenses to ensure your data and code remain accessible and properly credited.\n\n1.1 Rights and ownership at RFF\nAll agreements that involve the creation of RFF research or a similar work product must have terms that define intellectual property (IP) rights.\nBefore proceeding with choosing licenses, confirm that your team owns IP rights to the work produced and has full discretion over licensing the project’s data and code without restrictions from funders, institutional policies, or legal/data agreements.\nIn some cases the research team may have joint IP ownership with partners or funders. In this case, licensing options must be agreed upon by both parties.\nIn other cases, the research team may have licensing rights to software/code developed, but not data products. Data license restrictions generally do not restrict code licensing / publication.\n\n\n\n\n\n\nNote\n\n\n\nData and code products are licensed separately from RFF publication products. All work on the RFF.org and Resources.org websites (working papers, reports, issue briefs, explainers, Common Resources blog posts, Resources magazine articles, Resources Radio podcast episodes, graphs, charts, photographs, audio, and video) are listed under the Deed - Attribution-NonCommercial-NoDerivatives 4.0 International - Creative Commons license. This Creative Commons license is not suitable for either software or data.\n\n\n\n\n1.2 Choose appropriate licenses\nFor software and data, there are three license suites common in the academic space: MIT, GNU General Public License (GPL), and Open Data Commons (ODC). MIT and GNU GPL are separate software licenses, while ODC has two commonly-used data licenses. All four are described below.\n\nSoftware licenses\n\n\n\nAttribute\nMIT\nGNU General Public Use (GPL)\n\n\n\n\nLink to license text\nMIT\nGNU GPL\n\n\nDescription\nThe more permissive and flexible. Users, including commercial entities, can view, use, modify, and distribute the work freely.\nUsers, including commercial entities, can view, use, modify, and distribute the work freely, but are subject to copyleft restrictions.\n\n\nAttribution required\n✅ Yes\n✅ Yes\n\n\nEnclosure allowed\n✅ Yes\n❌ No\n\n\nCopyleft required (viral)\n❌ No\n✅ Yes\n\n\nBest uses\nLibraries and tools where broad reuse is encouraged, including in proprietary or closed-source contexts.\nModels, scripts, and workflows where preserving long-term openness and share-alike reuse is a priority.\n\n\n\nTerms:\n\nEnclosure: The act of applying legal or technical restrictions—like proprietary licensing or copyright—to limit access, reuse, or modification of software or data that was previously open.\nAttribution: The requirement to credit the original creator or source as specified in the license. It ensures recognition while allowing reuse and modification.\nCopyleft: A license condition that requires derivative works to be distributed under the same terms as the original. It ensures continued openness by mandating that source code remains available and free to reuse.\nViral: Describes licenses (like GPL or ODbL) that require any derivatives to adopt the same license. This spreads the original license’s terms to all adaptations, preserving openness but limiting reuse.\n\n\n\nData licenses\n\n\n\nAttribute\nODC-By\nODCbL\n\n\n\n\nLink to license text\nODC-By\nODCbL\n\n\nDescription\nA permissive license for databases. Users, including commercial entities, may use, modify, redistribute, and build upon the data.\nA copyleft license for databases. Users may use, modify, redistribute, but derivative databases must be shared under the same license. Ensures openness through a share-alike (viral) requirement.\n\n\nAttribution required\n✅ Yes\n✅ Yes\n\n\nEnclosure allowed\n✅ Yes\n❌ No\n\n\nShare-alike required (viral)\n❌ No\n✅ Yes\n\n\nBest uses\nWhen the goal is to encourage broad academic and public reuse of a dataset, including by commercial entities, without requiring openness in derived products\nCollaborative academic projects where ensuring that all derivative datasets remain openly accessible under the same terms is critical.\n\n\n\n\nShare-alike: A condition applied mainly to data and content (e.g., under CC BY-SA or ODbL), requiring that any adapted works use the same or a compatible open license (analogous to copyleft).\n\n\n\nOther licenses\nIf there are other requirements or the team would like to review a broader range of software licenses and their specifications, visit ChooseALicense.\n\n\n\n1.3 Create and customize license files\n\nOnce the license has been selected, download or create a .txt license file from the license website. Save the file as LICENSE.txt in the project folder and/or repository.\nReview the license terms and modify where necessary (most open-source and open-data licenses are designed to be used as-is, but others may require you to fill in specific details, such as name or organization).\nIf adding additional terms, include them in a separate README or license appendix to avoid conflicting with the main license.",
    "crumbs": [
      "Data Management",
      "Publication"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html#publishing",
    "href": "docs/data-management/publication.html#publishing",
    "title": "Publication",
    "section": "2 Publishing",
    "text": "2 Publishing\nFor publishing both code and data, ensure that the project-level README is up to date.\n\n2.1 Code\nWhen your project is ready to publish code stored in a GitHub repository (whether alongside a journal article, report, or other research output), it can be tagged to associate it with a publication and linked to the RFF GitHub organization.\nNote that some journals require authors to:\n\nMake the codebase publicly accessible\nInclude a link to the GitHub code repository in the manuscript\nReference the codebase DOI in the manuscript\n\n\nOptions to share repositories via the RFF GitHub Organization\nYou can link your GitHub repository to the RFF GitHub organization in one of two ways: by forking it into the organization or by transferring ownership.\n\nOption 1: Fork into the RFF GitHub Organization\nForking creates a new copy under the RFF GitHub organization’s namespace. It can be kept in sync with the original or left unchanged.\nThis is the most flexible option and is well-suited when:\n\nYou want to retain full ownership and admin control of the original repository.\nThe original repo belongs to an external collaborator or other GitHub organization.\nYou want to create a static snapshot of a repo to archive a version associated with a publication (e.g., v1.0-paper-release).\nYou intend to continue developing the code for purposes unrelated to RFF.\n\nTo connect it to a research output:\n\nTag a release (e.g., v1.0).\nAdd a note in the README describing its relationship to the publication and original source (preferably, including a DOI).\n\nWhile the fork is housed in the organization, you retain full control of the upstream repo.\n\n\nOption 2: Transfer Ownership to the RFF Organization\nThis option is best for long-term institutional or collaborative projects where:\n\nThe repository should be owned and managed by the organization rather than an individual.\nContinuity is important (e.g., when researchers move institutions).\nYou’re finished working on the project and no longer want to manage the repository.\n\nKey characteristics:\n\nThe original repo is moved to the GitHub organization, including its issues, branches, and history.\nCollaborators retain their roles. Former repository owners can request to be made co-administrators (along with the organization), granting control over collaborator roles.\nThe URL changes but GitHub automatically redirects old links.\n\n\n\n\nRecommended Publishing Workflow\n\nIf you are not already a member of the RFF GitHub organization, request membership by emailing DGWG.\nAttach the appropriate license to the repository, if you have not done so already.\n\nIf your repository is already in the RFF GitHub organization:\n\nMake the repository public once you’re ready to publish if it isn’t already.\nTag the publication version (e.g., v1.0-publication-name) as described here. (Note: only admins can do this, request admin privileges if needed.)\n\nIf your repository is under a personal GitHub account:\n\nDecide between forking or transferring the repo to the RFF GitHub organization (see explanation of these options above).\nContact the RFF GitHub organization admins at DGWG@rff.org to initiate the fork or transfer and to add a tag (note: tags do not transfer automatically when a repo is forked).\n(Optional step) Archive the tagged version on Zenodo and link the DOI in your README. This enables persistent citation, version-specific reference, and supports scholarly credit.\n\n\n\n\n2.2 Data\n\nData-level documentation (metadata)\nIt is recommended to attach metadata files to published datasets. Metadata (“data about data”) documents the “who, what, when, where, how, and why” of a data resource. Metadata not only allows users (your future self included) to understand and use datasets, but also facilitates search and retrieval of the data when deposited in a data repository.\nBelow are the key components of metadata. They can be stored in a simple text or markdown file.\n\nTitle: Descriptive name of the dataset.\nDOI number: Associated with the final publication, dataset, or both\nAbstract: Summary of the dataset’s content\nKeywords: Relevant terms for search and discovery\nTemporal Extent: Time period covered by the data\nData Format: File format\nData Source(s): Origin of the data\nAccuracy and Precision: Information about data quality\nAccess Constraints: Restrictions on data use\nAttribute / field definitions: Define all abbreviations and coded entries\nAdditional geospatial metadata components, if applicable\nSpatial Extent: Geographic coverage (bounding coordinates)\nProjection Information: Coordinate system details\n\nIn some contexts, generating machine-readable metadata that adheres to certain disciplinary standards is useful. There are various metadata formats and standards for specific disciplines. Additional guidance and resources for generating machine-readable metadata are here: Metadata and describing data – Cornell Data Services.\n\n\nUploading data to Zenodo\nZenodo is an online repository for sharing research data, software, and other scientific outputs. It has a broad disciplinary focus and is safe, citeable (every upload is assigned a DOI), compatible with GitHub, and free for up to 50GB of storage.\n\nStep 1: Prepare the research data\nBefore uploading, ensure that:\n\nThe data is well-organized (e.g., structured folders, clear file names).\nMetadata files are prepared for each data file or sets of data files.\nA license is attached.\nAny sensitive or restricted data is removed or anonymized (if applicable).\nThe project-level README is up to date.\n\nFor guidance on choosing which files to publish and how to handle API-accessed data, see Finalize data organization.\n\n\nStep 2: Create a Zenodo account & access the upload dashboard\n\nGo to Zenodo and sign in (or create an account). Note that you can create an account using your GitHub profile.\nClick the “New Upload” button on the Zenodo dashboard.\n\n\n\nStep 3: Upload the data files, fill in metadata, set access\n\nUpload data, metadata, and the project-level README file.\nEnter metadata information in applicable fields (contributors, associated journal article or conference presentation, etc.)\nInclude a link to the GitHub code repository.\nChoose an Access Level\n\nOpen Access: Publicly available for anyone.\nEmbargoed: Set a release date if the data must remain private for a certain period.\nRestricted Access: Requires users to request access.\n\n\n\n\nStep 4: Publish & Get a DOI\n\nReview all details and make any necessary edits.\nClick “Publish” to finalize the upload.\nZenodo will generate a DOI — use this when citing the dataset in publications.\n\n\n\nVersioning & Updates\nIf the dataset needs to be updated:\n\nUse the “New Version” option in Zenodo instead of creating a separate upload.\nZenodo will link versions together and maintain persistent DOIs.",
    "crumbs": [
      "Data Management",
      "Publication"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html",
    "href": "docs/data-management/storage-options.html",
    "title": "Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.\n\n\n\n\n\n\n\nComponent\nPrimary use\nNotes\n\n\n\n\nCore tools\n\n\n\n\nL:/ drive\nData and script storage\n- Use for all projects- Regular backups- Secure (RFF-only access)- See guidance for setup instructions\n\n\nGitHub\nScript version control, script sharing\n- Use for all projects- Supports collaboration, history tracking, and (optionally) project management- Can store and track small datasets\n\n\nAdditional tools for projects with external collaborators\n\n\n\n\nOneDrive\nSharing essential data with external partners\n- Good for smaller files- Shared access with external collaborators\n\n\nOther cloud options\nSharing large datasets externally\n- Azure, Google Bucket, AWS S3, Dropbox- Contact ITHelp@rff.org for Azure setup\n\n\nArcGIS Online\nSharing spatial data\n- Share and visualize spatial data over web browser- Contact Thompson@rff.org for setup",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#summary",
    "href": "docs/data-management/storage-options.html#summary",
    "title": "Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.\n\n\n\n\n\n\n\nComponent\nPrimary use\nNotes\n\n\n\n\nCore tools\n\n\n\n\nL:/ drive\nData and script storage\n- Use for all projects- Regular backups- Secure (RFF-only access)- See guidance for setup instructions\n\n\nGitHub\nScript version control, script sharing\n- Use for all projects- Supports collaboration, history tracking, and (optionally) project management- Can store and track small datasets\n\n\nAdditional tools for projects with external collaborators\n\n\n\n\nOneDrive\nSharing essential data with external partners\n- Good for smaller files- Shared access with external collaborators\n\n\nOther cloud options\nSharing large datasets externally\n- Azure, Google Bucket, AWS S3, Dropbox- Contact ITHelp@rff.org for Azure setup\n\n\nArcGIS Online\nSharing spatial data\n- Share and visualize spatial data over web browser- Contact Thompson@rff.org for setup",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#internal-rff-projects",
    "href": "docs/data-management/storage-options.html#internal-rff-projects",
    "title": "Storage Options",
    "section": "2 Internal RFF Projects",
    "text": "2 Internal RFF Projects\n\n2.1 Data: L:/ drive\nThe L:/ drive is the primary location for storing project data and code. All internal projects should have a designated L:/ drive folder—even when working with external collaborators.\nThe L:/ drive is optimized for data-intensive workflows. It offers:\n\nHigh storage capacity\nRegular backups\nEnhanced security compared to personal drives\n\nAccess to the L:/ drive requires an RFF network account.\nSee instructions for setting up a project L:/ drive folder.\n\n\n2.2 Code: L:/ drive and GitHub\nProject code (e.g., .R, .py, .do files) should be stored in the project’s L:/ drive folder alongside data, and should be version controlled within a Git repository synced to GitHub.\nGitHub’s distributed version control system allows team members to:\n\nWork on scripts independently without disrupting others\nTrack changes with clear commit messages\nReconcile and sync updates across folders\n\nFor example, you can revise a script locally and commit changes, with documentation, when they’re ready—without interfering with your colleague’s workflow.\nTo prevent users from simultaneously editing scripts stored within a shared folder, and to support use of version control, each team member who will be viewing, running, or editing code should have their own personal folder containing a clone of the project’s GitHub repository. Specific suggestions for how to organize your project folder are in the Organization section.\nSee Version Control for setup instructions.\n\n\n\n\n\n\nNote\n\n\n\nFor renv R package users: Accessing libraries and packages stored on the L:/ drive can slow down R performance, especially when working remotely. Consider storing the renv/library on a local drive (e.g., C:/). You can redirect the project’s library path by setting it in a .Renviron file.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#collab",
    "href": "docs/data-management/storage-options.html#collab",
    "title": "Storage Options",
    "section": "3 Solutions for collaborative projects",
    "text": "3 Solutions for collaborative projects\n\n3.1 Sharing code\nThe GitHub repository method (see Organization section) for storing and sharing scripts is ideal for collaborating with people who do not have access to the RFF network and L:/ drive. If they are made collaborators on the GitHub repository, they’re able to clone a copy of the repository codebase to their local folder directly from the web browser.\n\n\n3.2 Sharing and accessing data\nIn addition to storing data and code on the L:/ drive project folder, it may be necessary to store, access, and share files in other ways when collaborating with team members without RFF network access. In that case, there are a few options.\n\nAccessing raw data via Application Program Interfaces (APIs)\nAPIs (Application Programming Interfaces) allow programs to access data directly from remote servers. Many public datasets — such as the US Census or USDA NASS — can be queried through APIs, enabling efficient workflows without needing to download and store large raw files. If APIs are not available, similar workflows can often be built by downloading data directly from URLs using tools like curl.\nBenefits of Using APIs\n\nReduced storage: APIs allow you to retrieve only the data you need, rather than storing large or comprehensive datasets locally.\nUp-to-date data: API calls can return the most recent available data each time code is run.\nReproducibility: API-based workflows can support reproducible research when combined with version control and well-documented queries.\n\nConsiderations and Caveats\n\nStability and longevity: APIs can change or become deprecated. Keep this in mind when building workflows that rely on long-term access.\nAccessibility: Not all APIs have well-documented libraries in your programming language of choice. Some require constructing URLs manually or navigating limited documentation.\nReproducibility and archiving API-sourced data: To support reproducibility, consider the following approaches when publishing or archiving projects that rely on API-sourced data. Choose the strategy that best balances reproducibility, storage constraints, and project scope.\n\nDownload and archive locally: Save a copy of the queried dataset in your project directory at the time of analysis or publication.\nDeposit in a repository: Archive the data on platforms like Zenodo using tools such as zen4R (R) or zenodo-client (python).\nDocument query details: If data is too large and/or static, document the API source, parameters, and access date to support partial reproducibility.\n\n\n\n\nOneDrive\nOneDrive can be used to share data with external collaborators, but the L:/ drive should remain the primary storage location due to better access from RFF lab computers, greater computing capacity, more storage, fewer sync issues, and improved data security.\nStorage considerations for OneDrive:\n\nProject folders start with 5 TB of storage (expandable).\nFiles over 10 GB may fail to sync — OneDrive is not ideal for large files.\nShare only essential files (e.g., inputs/outputs — not intermediate files).\nMirror (replicate) the folder structure between L:/ and OneDrive to ensure clarity and consistency.\nSensitive data should be handled carefully; set access permissions appropriately.\n\nHow to set up collaboration in OneDrive.\n\n\nGitHub\nWhile we recommend only using GitHub to version control scripts, figures and tables intended for publication, and code documentation, experienced users can also use it to share and version control other small data files (well below 100 MB). This is also discussed in the Organization section.\nConsider security, sensitivity, and license restrictions when hosting data on GitHub.\nFiles larger than 100 MB should be shared using other tools.\n\n\nOther cloud storage options for large data\nAlternatives to OneDrive, such as Azure, Google Bucket, AWS S3, or Dropbox may be well suited to your project, especially for short-term storage and file transfer. However, note that these storage options may incur additional costs, depending on data size (even the “free tiers” of these services may incur pay-as-you-go costs). For Azure setups, contact IT at IThelp@rff.org.\n\n\nArcGIS Online for sharing and exploring spatial data\nArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular). In order to access the data and exploratory mapping interface, users need an ArcGIS Online account. Online accounts cost $100 per user and data storage costs vary by data size. Contact RFF’s GIS Coordinator at Thompson@rff.org for more information.\n\n\nEnabling external collaborator access to the L:/ drive\nWhile not recommended, it is possible to enable access to the L:/ drive for non-RFF staff. See Server Access for Non-Employees. Temporary accounts can be requested by contacting IT at IThelp@rff.org.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#L-drive-storage-request",
    "href": "docs/data-management/storage-options.html#L-drive-storage-request",
    "title": "Storage Options",
    "section": "4 L:/ drive storage request",
    "text": "4 L:/ drive storage request\nAt the start of a project (or upon major changes to specifications, such as timeline or disk space), email IThelp@rff.org with the following information. Example answers are provided.\n\n\n\n\n\n\n\nField\nResponse\n\n\n\n\nProject name\nLUFA-AgSubsidies\n\n\nStorage location\nL drive\n\n\nFolder name\nL:/Project-lufa-agsubsidies\n\n\nShort description\nThis project analyzes how variations in agricultural subsidy structures across U.S. counties influence land-use change\n\n\nPrincipal Investigator(s)\nOtgonbayar Aquila and Léonce Dominique\n\n\nRFF collaborators\nEvelyn Loren\n\n\nExternal collaborators\nSeveral University of Eastern Colorado collaborators but they will not need access to the project folder\n\n\nData types\nR, Stata, GIS datasets\n\n\nSize requested\n80 GB\n\n\nEstimated max. size\n150GB\n\n\nArchival date\nDecember 2028\n\n\nData agreement or sensitive data security considerations\nProprietary data will be in raw data folder and will need to be made read-only",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html",
    "href": "docs/data-management/quality-preparation.html",
    "title": "Quality & Preparation",
    "section": "",
    "text": "Always use and save a scripted program for data processing and analysis. Although it may seem more expeditious to take manual steps, writing code creates a documented and repeatable account of the processing steps taken and will save time and effort in the long-run.\nIf it is impossible to write code for processing steps, create a detailed record of the workflow in a document.",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "href": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "title": "Quality & Preparation",
    "section": "1 Quality Assurance for Data Integrity",
    "text": "1 Quality Assurance for Data Integrity\nQuality assurance (QA) is ensuring the accuracy, consistency, and reliability of data. Quality assurance measures should be implemented on both raw data from external sources and your project’s subsequent datasets; for example, after a major processing step such as data merging.\nBasic quality assurance measures are listed below, some of which were adapted from (Hutchinson et al. 2015).\n\nRead documentation / metadata that accompanies source datasets. It often comes in separate files (text, pdf, word, xml, etc.).\nAlways keep raw data in its original form. Do not modify raw datasets; save modified versions in the project’s “intermediate” data folder.\nVisually inspect data throughout processing. Visual checks can reveal issues with the data (e.g., repeated values or delimitation errors) that would affect analysis. This habit not only aids debugging processing code but also builds an understanding of the dataset.\nAssure data are delimited and line up in proper columns. Check that data is correctly delimited and parsed when imported into the processing program.\nCheck for missing values. Identify any missing or NA values in critical fields that could impact analysis. If there are missing values, identify the type of missingness and discuss solutions (applied example in R).\nIdentify impossible and anomalous values. Anomalies include values that are outside the expected range, logically impossible, outliers, or inconsistently formatted. In addition to checking for errors, identifying outliers can aid in data exploration by flagging rare events, errors, or interesting phenomena that require further investigation.\nPerform and review statistical summaries. Generate summary statistics to understand data distribution and identify inconsistencies or errors. Use these summaries to guide further cleaning, transformation, or data integrity checks.\nVisualize data through maps, boxplots, histograms, etc.\nFollow good software quality practices described in the software quality section of this guidance, such as pseudocoding, code review, and defensive programming.",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#tidy-data",
    "href": "docs/data-management/quality-preparation.html#tidy-data",
    "title": "Quality & Preparation",
    "section": "2 Tidy Data",
    "text": "2 Tidy Data\nRaw data rarely comes in structures compatible with your team’s analysis needs. Once the raw data has been checked for quality, additional processing may be required to start exploration and analysis.\nTidy data is a framework for data organization that facilitates efficient exploration, wrangling, and analysis (Wickham 2014). The benefits of storing data in the tidy format are:\n\nEasier data exploration and analysis\nSimpler manipulation and transformation of data\nCompatibility with data analysis tools (e.g., R and Python)\nImproved reproducibility of analysis\n\nData in this format are easy to work with, analyze, and combine with other datasets. However, once analyses and data merges start taking place, the structure of newly generated datasets are likely to be more complex and dependent upon the modeling or analysis needs. Discuss the role of tidy data with your team, and if/when in the process project datasets should deviate from the tidy data structure.\n\n2.1 The Three Core Principles of Tidy Data\n\nEach variable forms a column: In a tidy dataset, each variable has its own dedicated column. This means all values associated with that variable are listed vertically within that single column. These are also often referred to as fields or attributes.\nEach observation forms a row: Define an observation and emphasize that each row should represent a single data point. In a tidy dataset, each observation occupies a single row in the table. All the information pertaining to that specific observation is listed horizontally across the columns. These are also often referred to as records.\nEach type of observational unit forms a table: In a tidy dataset, data pertaining to different types of observational units should be separated into distinct tables.\n\n\n\n\nimage from https://r4ds.had.co.nz/tidy-data.html#tidy-data-1\n\n\n\n\n2.2 Practical applications\n\nR: The R tidyverse is a set of R packages designed to work together within the tidy data framework. It includes dplyr, readr, ggplot2, and other packages useful for wrangling data.\nPython: The Python pandas library is useful for creating and working with tidy data, as it uses data frames and includes functions for cleaning, transforming, and manipulating data.\nJulia: The DataFrames.jl library is useful for working with tabular tidy data, and has many powerful tools for manipulating data. The Tidier.jl framework builds on DataFrames.jl and emulates the R tidyverse.\n\n\n\n2.3 Resources\n\nGeneral:\n\nIntroduction to Data Wrangling and Tidying | Codecademy\nA Gentle Introduction to Tidy Data in R | by Arimoro Olayinka | Medium\n\nR focus:\n\nTidy data | tidyr\nData tidying – R for Data Science\nHelpful libraries and functions:\n\ntidyr::separate\njanitor::clean_names\n\n\nPython focus:\n\nTidy Data in Python",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "href": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "title": "Quality & Preparation",
    "section": "3 Data Type Conversion and Standardization",
    "text": "3 Data Type Conversion and Standardization\nData types, which define how data is stored and interpreted, were presented in the previous section. This section introduces data type conversion and standardization in ensuring consistent and meaningful analysis.\nData type conversion, or typecasting, is the process of transforming a value from one data type to another. Converting data types ensures compatibility between different datasets and allows for proper analysis. For example, converting age values from string (“25 years”) to integer (25) enables mathematical operations.\nWarning: Data type conversion can sometimes lead to loss of information, so it’s crucial to understand the implications of conversion before applying it. Examples:\n\nWhen converting the age column from string (“25 years”) to integer (25), information about the unit (years) was lost.\nConverting a float (2.96) to an integer (3) truncates decimals.\nIf a date is formatted as “DD/MM/YYYY” (03/12/2015) but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect (2015-03-12, instead of the accurate 2015-12-03).\n\nProper type conversion ensures data is correctly interpreted and can prevent errors in calculations, data analysis, and visualization.\n\n3.1 Key Points for Type Conversion\n\nUnderstand the source and target types: Knowing the data types involved in conversion helps ensure accurate transformations.\nHandle missing or invalid data: Make sure to manage missing or improperly formatted data that could cause errors during conversion.\nTest conversions: Always verify that conversions produce the expected results to avoid downstream errors in your analysis.\nRemember to always use the ISO 8601 standard format for dates (YYYY-MM-DD).\nBe aware that importing, reimporting, or saving files in certain formats can lead to loss or changes in column data types. For instance, saving data in CSV format often results in date columns being interpreted as text upon re-import, or numeric columns losing precision. This issue arises because formats like CSV lack built-in metadata to store data types, meaning they rely on the importing program to infer types, which can cause inconsistencies and data integrity issues over time. In these cases, columns may need to be re-typecast.\n\n\n\n3.2 Resources\n\nPlotting and Programming in Python: Data Types and Type Conversion\nR for Data Science - Transform",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "href": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "title": "Quality & Preparation",
    "section": "4 Preparation for Analysis",
    "text": "4 Preparation for Analysis\nAdditional cleaning and transformation steps (often referred to as “data wrangling”) are often necessary, and are highly variable depending on project needs. Examples include:\n\nfiltering based on criteria\nrestructuring/reshaping/pivoting\nremoving duplicates\ncorrecting errors in the source data (e.g. misspellings)\nmerging\n\n\n4.1 Resources\nThe approach depends on specific project and data needs. The following resources go into greater detail, with examples:\n\nR\n\nNCEAS Learning Hub’s coreR Course - 7 Cleaning & Wrangling Data (ucsb.edu)\nTransform – R for Data Science (2e) (hadley.nz)\nR for Reproducible Scientific Analysis: Subsetting Data (swcarpentry.github.io)\nR for Reproducible Scientific Analysis: Data Frame Manipulation with dplyr (swcarpentry.github.io)\nData Carpentry: R for Social Scientists\n\nPython\n\nData Analysis and Visualization in Python for Ecologists: Indexing, Slicing and Subsetting DataFrames in Python (datacarpentry.org)\nData Analysis and Visualization in Python for Ecologists: Combining DataFrames with Pandas (datacarpentry.org)",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html",
    "href": "docs/data-management/documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).\n\n\n\nImage from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#introduction",
    "href": "docs/data-management/documentation.html#introduction",
    "title": "Documentation",
    "section": "",
    "text": "“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).\n\n\n\nImage from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#types-of-documentation",
    "href": "docs/data-management/documentation.html#types-of-documentation",
    "title": "Documentation",
    "section": "2 Types of documentation",
    "text": "2 Types of documentation\nThis summary can help guide team conversations around documentation strategies.\n\n2.1 Preliminary\nPreliminary documentation refers to early-stage descriptions created during the planning phase. This often includes a data management plan (DMP), which outlines data collection, organization, storage, and sharing strategies.\n\n\n2.2 Process\nProcess documentation involves capturing step-by-step procedures and workflows used to collect, process, analyze, and model data, including:\n\nDocumenting raw data sources\nDocumenting methods with code comments\nDocumenting methods with version control tools\n\n\n\n2.3 Product\nProduct documentation provides information to accompany code and data of completed projects, ensuring usability and transparency. This documentation can be a part of or accompanying written products.",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#core-documentation",
    "href": "docs/data-management/documentation.html#core-documentation",
    "title": "Documentation",
    "section": "3 Core documentation",
    "text": "3 Core documentation\nproject_name/\n├── README.txt &lt;&lt;&lt;&lt;&lt;&lt;&lt;\n├── data/\n│   ├── raw/\n│   │   ├── README_raw_dataset.txt &lt;&lt;&lt;&lt;&lt;&lt;&lt;\n│   ├── intermediate/\n│   ├── clean/\n│   │   ├── metadata.txt &lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\n3.1 The project-level README file\nAt a minimum, the project should have a README file in text or markdown format with the information listed below. This can be an evolving, living document. While it’s technically product documentation, it’s easiest to start it early in the project’s development.\nInclude the following in the README file:\n\nProject name and description\nProject PI(s) and contact information\nList of staff responsible for data management and code development\nAssociated final product(s), date of release, and DOI (if applicable)\nLink to published data/code (if applicable)\nLicense(s) associated with final product(s)\nNature of sensitive or proprietary data (if applicable)\nAny other important notes for navigating folder or using data/code\n\n\n\n3.2 Raw data\nBest practices:\n\nThe source of all downloaded raw datasets should be documented in a README file.\nCreate a README file associated with each raw data file, or each logical “cluster” of related raw data files, in the same folder as the data.\nIf there are multiple data files in a folder, name the README so that it is easily associated with the data file(s) it describes (e.g., README_PRISM_Daily_Temperature.txt).\nFormat README files consistently.\nWrite the README document in an plain text and open source file format, such as .txt or .md.\n\nBelow is a README template and example. Include in the README file the information shown in the example.\nFilename: README_PRISM_Daily_Temperature.txt\nDataset name & format: PRISM_Daily_Temperature_2024.csv\n\nData source: Downloaded from the PRISM Climate Group website: https://prism.oregonstate.edu/ \n\nDate acquired: 2024-02-28.\n\nAcquired by: [Name of researcher who downloaded the data]\n\nData description: This dataset contains daily minimum and maximum temperature data for Washington State for the year 2024, with a spatial resolution of 4km.\n\nPreprocessing: No modifications were made to the raw dataset. The file is stored exactly as downloaded from the source.\n\nLicense & Usage Restrictions: This dataset is publicly available under the PRISM Climate Group's data use policy. Refer to https://prism.oregonstate.edu/documents/PRISM_terms_of_use.pdf for more details.\n\n\n\n\n\n\nNote\n\n\n\nAs described in the Organization section, all raw data should be retained in their raw form and not directly modified.\n\n\n\n\n3.3 Methods\nThere are two critical strategies for documenting methods effectively:\n\nUse version control to track and explain changes to scripts over time (e.g., through clear and consistent commit messages).\nUse code comments to document the purpose and logic of scripts and key functions, helping others (and your future self) understand how the code works.\n\n\n\n3.4 Data products\nMetadata documentation should be included with final data products, and is described in the Publication section.",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html",
    "href": "docs/data-management/file-formats.html",
    "title": "Data & File Types",
    "section": "",
    "text": "In coding environments and other software, data are classified into specific “types”, which determine how the values are interpreted and stored in memory. Recognizing the data types of variables in a dataset is essential for ensuring analytical accuracy, understanding precision, and avoiding errors during data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData type\nDefinition\nTypical memory\nPrecision\nCommon operations\nExamples\n\n\n\n\nCharacter (string/text)\ntext or string values\n~1 byte per character\nNot applicable (stores characters, not numeric values)\nConcatenation, substring, pattern matching\n\"Hello, world!\",'#23',\"'Why?', they asked.\"\n\n\nInteger\nWhole numbers (no decimal point)\n4 bytes (32-bit)\nExact for values within allowed range\nArithemtic, comparisons, indexing\n0, 42, -6,2e+30\n\n\nFloating point\nNumbers with decimals (real numbers, including fractions)\n4 bytes (float), 8 bytes (double)\nApproximate - may introduce small rounding errors\nArithmetic, scientific calculations\n-54.3,3.14159\n\n\nBoolean / Logical\nBinary values\nTypically 1 byte\nExact\nLogical operations (e.g., AND, OR, NOT)\nTRUE, FALSE,0, 1\n\n\nDate/time\nCalendar dates and/or clock times\nVaries by system (~8+ bytes)\nHigh precision in supported range\nOften require specialized functions; format inconsistencies can cause import errors\n2025-05-29, 1990-01-24 14:30:00\n\n\n\n\n\n\n\nFloating point rounding errors Floating point types represent real numbers, including fractions. However, not all decimal values can be represented exactly in binary. As a result, small rounding errors may occur during arithmetic operations. These are usually minor but can accumulate in complex calculations. Example: 0.1 + 0.2 might result in 0.30000000000000004 due to binary representation limits.\nDate formats\n\nUse the conventional ISO 8601 format (YYYY-MM-DD)\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as DD/MM/YYYY but is mistakenly interpreted as MM/DD/YYYY during conversion to YYYY-MM-DD, the resulting date will be incorrect.\n\nChanging data types Changing a variable’s data type (e.g., from float to integer or from number to string) can alter how the data is stored and interpreted. Even if the displayed values seem the same (e.g., 5, 5.0, or \"5\"), the underlying representation differs—and in some cases (like converting from float to integer), it may result in loss of precision or information. Always check whether a conversion is appropriate for your analysis.\nfloat_value = 3.7\nint_value = int(float_value)  # Becomes 3\nMemory efficiency Choosing efficient data types can improve performance for large datasets. For example, storing whole numbers (like counts of people) as double-precision floats instead of integers can use more memory than necessary. While both floats and integers can be 4 or 8 bytes depending on the system, using types with more precision than needed (e.g., doubles for integers) can lead to unnecessary memory overhead.",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-types",
    "href": "docs/data-management/file-formats.html#data-types",
    "title": "Data & File Types",
    "section": "",
    "text": "In coding environments and other software, data are classified into specific “types”, which determine how the values are interpreted and stored in memory. Recognizing the data types of variables in a dataset is essential for ensuring analytical accuracy, understanding precision, and avoiding errors during data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData type\nDefinition\nTypical memory\nPrecision\nCommon operations\nExamples\n\n\n\n\nCharacter (string/text)\ntext or string values\n~1 byte per character\nNot applicable (stores characters, not numeric values)\nConcatenation, substring, pattern matching\n\"Hello, world!\",'#23',\"'Why?', they asked.\"\n\n\nInteger\nWhole numbers (no decimal point)\n4 bytes (32-bit)\nExact for values within allowed range\nArithemtic, comparisons, indexing\n0, 42, -6,2e+30\n\n\nFloating point\nNumbers with decimals (real numbers, including fractions)\n4 bytes (float), 8 bytes (double)\nApproximate - may introduce small rounding errors\nArithmetic, scientific calculations\n-54.3,3.14159\n\n\nBoolean / Logical\nBinary values\nTypically 1 byte\nExact\nLogical operations (e.g., AND, OR, NOT)\nTRUE, FALSE,0, 1\n\n\nDate/time\nCalendar dates and/or clock times\nVaries by system (~8+ bytes)\nHigh precision in supported range\nOften require specialized functions; format inconsistencies can cause import errors\n2025-05-29, 1990-01-24 14:30:00\n\n\n\n\n\n\n\nFloating point rounding errors Floating point types represent real numbers, including fractions. However, not all decimal values can be represented exactly in binary. As a result, small rounding errors may occur during arithmetic operations. These are usually minor but can accumulate in complex calculations. Example: 0.1 + 0.2 might result in 0.30000000000000004 due to binary representation limits.\nDate formats\n\nUse the conventional ISO 8601 format (YYYY-MM-DD)\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as DD/MM/YYYY but is mistakenly interpreted as MM/DD/YYYY during conversion to YYYY-MM-DD, the resulting date will be incorrect.\n\nChanging data types Changing a variable’s data type (e.g., from float to integer or from number to string) can alter how the data is stored and interpreted. Even if the displayed values seem the same (e.g., 5, 5.0, or \"5\"), the underlying representation differs—and in some cases (like converting from float to integer), it may result in loss of precision or information. Always check whether a conversion is appropriate for your analysis.\nfloat_value = 3.7\nint_value = int(float_value)  # Becomes 3\nMemory efficiency Choosing efficient data types can improve performance for large datasets. For example, storing whole numbers (like counts of people) as double-precision floats instead of integers can use more memory than necessary. While both floats and integers can be 4 or 8 bytes depending on the system, using types with more precision than needed (e.g., doubles for integers) can lead to unnecessary memory overhead.",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-file-formats",
    "href": "docs/data-management/file-formats.html#data-file-formats",
    "title": "Data & File Types",
    "section": "2 Data file formats",
    "text": "2 Data file formats\nIn general, data should be stored and/or archived in open formats. Open formats are non-proprietary, and therefore maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them. The best file format to use will depend on the type and structure of data. The section on data formats from the Standard Operating Procedures of emLab (at UC Santa Barbara) gives a few examples.\n\n2.1 Key characteristics of data file formats\n\nProprietary vs. non-proprietary: Non-proprietary software formats can be easily imported and accessed using open-source software. This enhances their interoperability, or how easily a file format can be used across different software platforms and systems. Formats that are widely supported and compatible with various tools are generally more versatile.\nTabular vs. hierarchical: Tabular data is organized into rows and columns, resembling a table, while hierarchical data is organized in a tree-like structure, with elements nested within others.\nStructured vs. unstructured: Structured data refers to data that is organized in a predefined format, typically in rows and columns, like databases or spreadsheets, which allows for easy search, analysis, and processing. Unstructured data, on the other hand, lacks a predefined format and is often textual or multimedia in nature, such as emails, social media posts, or video files. Semi-structured data has tags or markers but not a rigid structure.\nRetention of data types: Some file formats retain metadata about data types (e.g., whether a column is an integer or string), while others lose this information upon saving.\n\n\n\n2.2 Tabular formats\nUse open-source formats whenever possible.Plain text formats like CSV (.csv) are preferred for their transparency, interoperability, and long-term accessibility. Excel (.xlsx) and Stata (.dta) are proprietary and should generally be avoided—except when essential features (like Stata variable labels) are required.\nChoosing an open source format.The preferred open source format will depend on the project team’s preferences for accessibility, software, interoperability, need for computational efficiency, etc. The features of common formats are described below.\n\n\n\n\n\n\nNote\n\n\n\nIn cases where the native format of source data is in a proprietary software format, it is often necessary to use that software to view and edit data. For example, Stata dataset variables may have labels, a kind of embedded metadata that can only be accessed in Stata.\n\n\n\nCharacteristics of tabular formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtensions\nOpen-source or proprietary\nRetains data types\nLevel of structure\n\n\n\n\nComma or tab-separated values\n.csv, .tsv\nOpen-source\nNo\nStructured\n\n\nPlain text\n.txt\nOpen-source\nNo\nSemi-structured\n\n\nMicrosoft Excel spreadsheet/workbook\n.xls or .xlsx\nProprietary\nYes\nStructured\n\n\nFeather\n.feather\nOpen-source\nYes\nStructured\n\n\nParquet\n.parquet\nOpen-source\nYes\nStructured\n\n\nRData\n.rdata or .rds\nOpen-source\nYes\nStructured\n\n\nLightning Fast Serialization of Data Frames\n.fst\nOpen-source\nYes\nStructured\n\n\nSQLite\n.sqlite, .db\nOpen-source\nYes\nStructured\n\n\nStata data file\n.dta\nProprietary\nYes\nStructured\n\n\nSAS dataset\n.sas7bdat\nProprietary\nYes\nStructured\n\n\nDatabase File\n.dbf\nOpen-source\nYes\nStructured\n\n\n\n\n\nDescriptions of tabular formats\n\nText-based formats These formats are highly accessible and can be opened with common tools like Excel, Notepad, or any text editor, making them ideal for sharing output/final datasets. Text-based formats also work well with version control systems. However, be aware of their drawbacks and follow best practices.\n\nComma-separated values (.csv) delimited text files widely used for data exchange and simple data storage. Each row contains the same number of values separated by commas.\nTab-separated values (.tsv) files similar to CSV files but with values separated by tabs.\nPlain text (.txt) files which can contain unformatted or formatted (schema) text. Not recommended for storing complex datasets.\n\nExcel spreadsheets/workbooks (.xls, .xlsx) are files designed for use with Microsoft Excel software. XLS is a binary file format compatible only with Excel, both older and newer versions. XLSX was developed more recently. It is XML-based, making it compatible with open-source software such Google Sheets as well as versions of Excel released since 2007. Generally avoid relying on these files for data storage due to complex formatting, data formats, formulas, etc. They also complicate quality assurance. XLS is not version-control friendly and XLSX requires special version-control techniques because it is stored in a compressed state. Excel spreadsheets can easily be exported to CSV files.\nCommon data science formats These formats are well suited for working with data during analysis and collaboration (e.g., intermediate data), because they retain metadata (e.g., data types) and are optimized for reading and writing. They are more computationally efficient in terms of input/output speed and file size, but often less suited to version control than text-based formats.\n\nFeather (.feather) a fast, lightweight binary columnar data format used for data exchange between data analysis languages like R and Python. Optimized for performance and efficiency, especially when working with large tables of data. Faster than Parquet at in-memory work, reading, and writing.\nParquet (.parquet) a binary columnar data format designed for efficient storage and processing of large datasets. Supports compression and is optimized for performance across data tools like R, Python, and SQL engines. Better file size efficiency than Feather.\nRData (.rds, .rdata) files used to store one R object (.rds) or an R environment with several objects (.rdata). Useful if working within an R project for efficiency and organization features, but providing limited interoperability.\nLightning Fast Serialization of Data Frames (.fst) a fast, compressed, columnar binary format designed specifically for R, optimized for high-speed reading and writing of large data frames. Ideal for efficient storage and selective column access in R workflows, but not designed for cross-language interoperability.\nSQLite (.sqlite, .db) files used by the SQLite relational database engine, which supports SQL queries and transactions and is used for lightweight, portable databases.\nStata data file (.dta) binary files created by the statistical analysis software Stata. Note that they sometimes include metadata (e.g., variable labels) that isn’t automatically loaded when importing into other software (e.g. R using the haven package).\nSAS Dataset (.sas7bdat) the proprietary file format used by SAS for storing datasets. It supports metadata and variable attributes. Datasets should be converted to open-source formats after processing.\nDatabase File (.dbf) files used by dBASE and other database systems to store tabular data. They support a fixed schema and metadata. DBF files cannot store full precision. Avoid creating this type of file.\n\n\n\n\nWorking with text-based file formats\n\nLimitations of text-based formats\nText-based formats like CSVs do not store data type metadata.\nBe cautious when reading and writing CSVs or other plain-text formats. Understand that data types will be inferred, not preserved, and this may introduce rounding or formatting errors. All values are saved as character strings in text-based file formats — there is no embedded information about whether something is an integer, float, etc.\nThis has several implications:\n\nType guessing on import When reading a CSV, software tools typically infer column types automatically. This can result in:\n\nInconsistent interpretation across tools (e.g., a column read as numeric in one program might be read as text in another).\nConversion errors, especially with dates, floating points, or missing values.  \n\nPrecision loss on export If you’re exporting a floating point column to CSV, the software may:\n\nRound or truncate values.\nDrop trailing digits or use limited decimal precision.\nThese changes might not be obvious but can affect downstream calculations.  \n\nNo built-in support for special types Since all values are strings, CSV files cannot inherently distinguish between:\n\n3 as an integer vs. 3.0 as a float\n2023-01-01 as a string vs. a date object\nTRUE as a logical value vs. a text label\n\n\n\n\nPractices for working with text-based formats\n\nReading/importing text-based data\n\nExplicitly specify column types when importing data (e.g., using read_csv(..., col_types=...) in R or dtype=... in python pandas). Another strategy is to specify all column types as character (col_types = 'character), view how they are stored, and then decides which columns to convert and how.\nValidate types after import, either by manually inspecting or using scripted tests.  \n\nExporting/sharing text-based data\n\nUse text-based formats only when useful, such as when publishing / sharing data with those without specialized software, or when version-controlling data.\nUse formats that preserve numeric precision (e.g., binary formats like .rds, .feather, .parquet) during analysis.\nClearly document data types in metadata or a readme when sharing text-based formats.\n\n\n\n\n\n\n2.3 Hierarchical formats\nHierarchical data formats are best suited for storing and exchanging complex, nested data structures with parent-child relationships, such as configurations, scientific datasets, or web APIs, where flexibility and the ability to represent variable levels of detail are essential.\n\nCharacteristics of hierarchical formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtensions\nOpen-source or proprietary\nRetains data types\nLevel of structure\n\n\n\n\nHierarchical Data Format version 5 (HDF5)\n.h5, .hdf5\nOpen-source\nYes\nStructured\n\n\nNetwork Common Data Form (NetCDF)\n.nc\nOpen-source\nYes\nStructured\n\n\nJavaScript Object Notation\n.json\nOpen-source\nNo\nSemi-structured\n\n\neXtensible Markup Language\n.xml\nOpen-source\nNo\nSemi-structured\n\n\nYAML\n.yml or .yaml\nOpen-source\nNo\nUnstructured\n\n\n\n\n\nDescriptions of hierarchical formats:\n\nHierarchical Data Format version 5 (.hdf5, .h5), commonly called HDF5, files for storing complex and hierarchical datasets, supporting large data volumes and complex data structures.\nNetwork Common Data Form (.nc), commonly called NetCDF, files designed for array-oriented scientific data. They work especially well for multi-dimensional data like time-series and spatial data.\nJavaScript Object Notation (.json), text-based files used for storing structured data. Often used to transfer data between a server and a web application, as well as when sending and receiving data via an API. (See tips for working with JSON in R and Python.)\neXtensible Markup Language (.xml), files organizing data hierarchically with customizable tags, making them both machine-readable and human-readable. XML is widely used in web services, data exchange, and configuration files.\nYAML (.yaml or .yml), human-readable files using a data serialization format well suited for configuration files and data exchange. It uses indentation to define structure and supports key-value pairs, lists, and nested data, making it simpler and more concise compared to XML or JSON. “YAML” is a recursive acronym: YAML Ain’t Markup Language.\n\n\n\n\n2.4 Geospatial file formats\nGeospatial data are stored as either vector data or raster data. The format of input spatial data typically dictates which geospatial tools can be applied to it.\n\nVector data\n\nVector data is stored as pairs of coordinates. Points, lines, and polygons are all vector data.\nRecommended open-source vector file formats:\n\nGeopackage (.gpkg, recommended for its advantages over the shapefile format)\nKeyhole markup language (e.g., .kml, .kmz)\nGeoJSON (e.g., .json, .geojson)\nTables with coordinates (e.g., .csv)\n\nCommon proprietary vector file formats:\n\nShapefiles (.shp)\nFeature classes in geodatabases (.gdb)\n\n\n\n\n\n\n\nNote\n\n\n\nA shapefile is actually a collection of several files, including geometry (.shp), projection information (.prj), tabular data (.dbf), and more. Make sure to store all component files together within the same folder.\n\n\nAll vector data files should have three critical metadata components:\n\nCoordinate reference system\nExtent: the geographic area covered by the data, represented by four coordinate pairs\nObject type: whether the data consists of points, lines, or polygons\n\n\n\nRaster data\n \nRaster data formats store values across a regular grid containing cells of equal size, with each cell containing a value. A raster is similar to a pixelated image, except that it’s accompanied by information linking it to a particular geographic location. All cell values within a single raster variable are of the same scalar data type (integer, float, string, etc.). Common examples of raster data are elevation, land cover, and satellite imagery.\nThe recommended general purpose raster file format is GeoTIFF (.tif), as it supports multiple bands, retention of spatial reference metadata, large file sizes, high compression, and use in a variety of languages/software. Other formats may work better for specific use cases. All of the following common formats are open-source:\n\nGeoTIFF (.tif), the most widely used format for raster data\nASCII grid (.asc), plain-text-based files for elevation models and basic raster grids\nNetCDF (.nc) and HDF5 (.hdf5, .h5), both described in Section 2.3\n\nAvoid saving rasters as proprietary software file formats, including ESRI grid/tile and ERDAS Imagine (.img) files.\nAll raster files should have five critical metadata components:\n\nCoordinate reference system\nExtent: how large the raster is, often represented by the number of rows and columns\nOrigin point: a pair of coordinates pinpointing the bottom left corner of the image\nResolution: cell size\nNo data value: the value that represents when a cell’s data value is missing",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#figure-file-formats",
    "href": "docs/data-management/file-formats.html#figure-file-formats",
    "title": "Data & File Types",
    "section": "3 Figure file formats",
    "text": "3 Figure file formats\nIt is helpful to think ahead when generating and saving data visualizations and plots. Academic journals often accept TIFF and PNG formats, but they frequently have resolution and dimension requirements. Export figures with a minimum resolution of 300 dots per inch (DPI).\nFor RFF communications, however, vector formats are best because they can be easily modified as needed. These include:\n\nScalable Vector Graphics (.svg)\nEncapsulated PostScript (.eps)\n\nConsider that you may want to be able to share the underlying data with the RFF Communications team so that they and their external design partners can create custom figures for presentation on the website, in the magazine, etc. This means clearly documenting the processing code that created the underlying data / figures, so that output data can be easily reproduced and shared as needed. If figure data is time-consuming to reproduce, you may want to save a copy of it to the L:/ drive or to your GitHub repository.\nFor more information on figure (and table) style guidelines, refer to the RFF Style Guide.\n\n“Sharing the underlying data of any maps and figures is always helpful for the Communications Team!”\n– Elizabeth Wason (Editorial Director, RFF)",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#resources",
    "href": "docs/data-management/file-formats.html#resources",
    "title": "Data & File Types",
    "section": "4 Resources",
    "text": "4 Resources\n\nR-focused:\n\nR for Data Science: Data import\nR for Data Science: Transforming data types\n\nPython-focused:\n\nSoftware Carpentry: Data types and type conversion\npandas.pydata.org: Data file formats and input/output tools\n\nSpatial data types\n\nIntroduction to Geospatial Concepts: Summary and Setup (datacarpentry.org)\nRFF GIS Training.",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#what-is-version-control",
    "href": "presentations/learning-lab-vc1/index.html#what-is-version-control",
    "title": "Version Control Tutorial: Basics",
    "section": "What is Version Control?",
    "text": "What is Version Control?\n\n\nSort of like how Microsoft Word will periodically save versions that you can return to.\n\nVersion control is a way of recording changes to a file or set of files over time so that you can recall specific versions later.\nThe most common VC systems are:\n\n\nGit\n\nMercurial\nSubversion (SVN)"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#why-should-i-use-git-and-github",
    "href": "presentations/learning-lab-vc1/index.html#why-should-i-use-git-and-github",
    "title": "Version Control Tutorial: Basics",
    "section": "Why should I use Git and GitHub?",
    "text": "Why should I use Git and GitHub?\n\n\nPeace of Mind - work is stored safely and can be easily recalled\nCollaboration - multiple versions can be developed in tandem, allowing parallel feature development\nReview and Discussion - GitHub allows for easy review of code changes before incorporating into the main branch of the repository\nDocumentation - shows when, why, and by whom specific changes were made\nConsolidation - Project code and documentation can be stored in the same location.\nPreservation and Access - Easy web browser access to all code versions, can control whether public or private."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#git-versus-github",
    "href": "presentations/learning-lab-vc1/index.html#git-versus-github",
    "title": "Version Control Tutorial: Basics",
    "section": "Git versus GitHub",
    "text": "Git versus GitHub\n\nGit works by monitoring the changes of the contents of an otherwise ordinary file folder, called a repository. In a Git repository, a user can tell Git which file changes to keep, which to discard, and can label those changes, go back to previous file versions, and much more!\n\n\n\nGit is a popular version control system, which is software we can download and use on our computer.\n\n\n\nGitHub is a website that hosts Git repositories, and provides convenient ways to host documentation, discuss code changes, and report bugs."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#the-basic-git-workflow",
    "href": "presentations/learning-lab-vc1/index.html#the-basic-git-workflow",
    "title": "Version Control Tutorial: Basics",
    "section": "The basic Git workflow",
    "text": "The basic Git workflow\n\nThis is the fundamental workflow you’ll use repeatedly: make changes locally, stage them, commit them with a message, then push to share with others.\n\nMinimalist Git use in your coding workflow involves only a few concepts, all of which we’ll cover today:\n\nRepository - A folder for storing code, with a special subdirectory containing the Git magic\nStaging - Reviewing your changes and selecting which ones to “add”\nLocal Copy - A copy (clone) of the repository on your computer where you “commit” your changes\nRemote Copy - The copy of the repo on a server (GitHub) where you “push” your changes to share them with others"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#the-basic-git-workflow-1",
    "href": "presentations/learning-lab-vc1/index.html#the-basic-git-workflow-1",
    "title": "Version Control Tutorial: Basics",
    "section": "The basic Git workflow",
    "text": "The basic Git workflow"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#in-practice",
    "href": "presentations/learning-lab-vc1/index.html#in-practice",
    "title": "Version Control Tutorial: Basics",
    "section": "In practice",
    "text": "In practice\n\nGit works by creating snapshots of your project at different points in time. Each commit represents a complete snapshot of your files. The branching system allows multiple versions to coexist and later be merged together.\n\n\nThe repository accumulates changes incrementally as you build upon the project.\nWhen you commit changes, Git tracks insertions and deletions instead of saving multiple copies, minimizing memory use.\nGit also makes it easy to checkout past versions of code to diagnose issues or to branch multiple versions to develop in parallel."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#in-practice-1",
    "href": "presentations/learning-lab-vc1/index.html#in-practice-1",
    "title": "Version Control Tutorial: Basics",
    "section": "In practice",
    "text": "In practice\n\nThe repository accumulates changes incrementally as you build upon the project.\nWhen you commit changes, Git tracks insertions and deletions instead of saving multiple copies, minimizing memory use.\nGit also makes it easy to checkout past versions of code to diagnose issues or to branch multiple versions to develop in parallel.\n\n\n\nBranching will be covered in the next workshop. See the RFF data guidance tutorial if you want to learn more in the meantime."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#git-bash",
    "href": "presentations/learning-lab-vc1/index.html#git-bash",
    "title": "Version Control Tutorial: Basics",
    "section": "Git Bash",
    "text": "Git Bash\nThere are many ways of using the Git software, but we will focus on Git Bash, a command line Git interface that comes with Git.\nFirst, open Git Bash. (Hit the Windows key and type “bash”)"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#git-bash-1",
    "href": "presentations/learning-lab-vc1/index.html#git-bash-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Git Bash",
    "text": "Git Bash\nThere are many ways of using the Git software, but we will focus on Git Bash, a command line Git interface that comes with Git.\nFirst, open Git Bash. (Hit the windows key and type “bash”)\nLet’s start by asking Git what version we are running:\n$ git --version"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#git-bash-2",
    "href": "presentations/learning-lab-vc1/index.html#git-bash-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Git Bash",
    "text": "Git Bash\n\nGit Bash is one way to access Git. It is not the only way, nor necessarily the way you will always use. Many common code editors like RStudio and Visual Studio Code have some form of Git interface built in, and you may consider using those. For the tutorial, however, we will focus on Git Bash because it comes with Git and it is good for teaching the fundamentals of Git.\n\nThere are many ways of using the Git software, but we will focus on Git Bash, a command line Git interface that comes with Git.\nFirst, open Git Bash. (Hit the windows key and type “bash”)\nLet’s start by asking Git what version we are running:\n$ git --version\ngit version 2.47.0.windows.2"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\n\nFirst, check if Git is already configured:\n\n$ git config --global --list\nIf you see existing username and email settings, you can skip the next step."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-1",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\n\nReplace the example username and email with your own GitHub username and email address.\n\n\nFirst, check if Git is already configured:\nConfigure using your username and email (if needed):\n\n$ git config --global user.name 'jmwingenroth'\n$ git config --global user.email 'my_github_account_email@aol.com'\n\n\nIt is best to use the same username and email as your GitHub account to keep them consistent."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-2",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\n\nReplace the example username and email with your own GitHub username and email address.\n\n\nFirst, check if Git is already configured:\nConfigure using your username and email (if needed):\nVerify your configuration:\n\n$ git config --global --list\nuser.name=jmwingenroth\nuser.email=my_github_account_email@aol.com\nGit will now include your name and email when you publish changes!\n\n\nIt is best to use the same username and email as your GitHub account to keep them consistent."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nNavigate to github.com, log in, and click the plus button in the top right corner"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-1",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nGitHub provides a structure called “Organizations”, which can have multiple users with different roles, and can even own repositories. Your project teams can discuss whether or not it makes sense to have a GitHub organization.\nSo, for the owner, you could either select your user account, or an organization you are a member of. The repository owner will show up in the URL of the repository\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\n\nThe owner’s name will appear in the URL of the repository:\ngithub.com/jmwingenroth/repository-name"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-2",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nWe recommend all lowercase names with dashes separating words, like my-repo-name. The only exception is when a programming language’s best practices prescribe a specific repository naming convention. (i.e. julia package repositories are supposed to be camelcase as in MyRepoName.jl)\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\nChoose a repository name.\n\nWe recommend choosing something concise and descriptive, all lowercase, with dashes separating words, like my-repo-name."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-3",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-3",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nGenerally, choose private for repositories that will contain sensitive information, and public if the project requires it to be public. It is easy to change from private to public later on, so when in doubt choose private.\nFor public repositories, you would also need to select which software license to use, and you can find more information about that on the website.\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\nChoose a repository name.\nChoose whether the repository is to be public or private.\n\nGenerally, choose private for repositories that will contain sensitive information, and public if the project requires it to be public. For today, choose private."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-4",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-4",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nThis will create a file called README.md located in the repository’s root folder, where you can later add basic documentation for the repository.\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\nChoose a repository name.\nChoose whether the repository is to be public or private.\nCheck the box to add a README file.\n\nWe will use this to store documentation for the repository."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-5",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-5",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nA .gitignore file specifies certain file types that Git will not track the changes of, by default. For example, it is best to ignore an auto-created config file made by R studio that is user-specific. Generally it is a good idea to select the .gitignore template for the programming language you will be using.\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\nChoose a repository name.\nChoose whether the repository is to be public or private.\nCheck the box to add a README file.\nChoose a .gitignore template from the dropdown menu.\n\nSelect the .gitignore template for the programming language you will be using. Any language works for today."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-6",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-repository-in-github-6",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Repository in GitHub",
    "text": "Creating a Repository in GitHub\n\nThis should take you to the home page of your new repository\n\n\nNavigate to github.com, log in, and click the plus button in the top right corner\nSelect the repository owner, yourself!\nChoose a repository name.\nChoose whether the repository is to be public or private.\nCheck the box to add a README file.\nChoose a .gitignore template from the dropdown menu.\nClick the “Create Repository” button!!"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git",
    "href": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git",
    "title": "Version Control Tutorial: Basics",
    "section": "Cloning Your First Repository With Git",
    "text": "Cloning Your First Repository With Git\n\nNow that we’ve made a remote repository, let’s get it copied onto our computer. Copying a remote Git repository is called cloning. This process will work the same way for any existing repository, including the one that we just made.\n\nCloning = making a copy of a repository on your computer"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-1",
    "href": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Cloning Your First Repository With Git",
    "text": "Cloning Your First Repository With Git\n\nFirst, it’s important to choose a good file location to store the git repository. While it’s not required, many people like to have a single folder to store all of their Git repositories. An alternative would be to store the Git repository in an associated project folder.\nGit Bash has a working directory, which is the location that it is operating in. To see what the current working directory is, we can enter the pwd command, which stands for primary working directory.\n\n\nCopy the repository URL to the clipboard.\n\nFrom the repository home page, click the green button, then copy the URL to clipboard by clicking the logo with intersecting squares."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-2",
    "href": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Cloning Your First Repository With Git",
    "text": "Cloning Your First Repository With Git\n\nCopy the repository URL to the clipboard.\n\nFrom the repository home page, click the green button, then copy the URL to clipboard by clicking the logo with intersecting squares."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-3",
    "href": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-3",
    "title": "Version Control Tutorial: Basics",
    "section": "Cloning Your First Repository With Git",
    "text": "Cloning Your First Repository With Git\n\nCopy the repository URL to the clipboard.\nCreate a folder called “repos”\n\nOpen File Explorer, navigate to the folder where you’d like to store your code, create a folder named repos, and copy the file path using the right-click menu.\nThen, navigate there using the cd command in Git Bash:\ncd C:/Users/Jordan/repos\n\n\nFor today, put this somewhere on your local hard drive. However, the data governance website provides guidelines for an L:/ drive team project folder hierarchy that is recommended for actual Git implementations."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-4",
    "href": "presentations/learning-lab-vc1/index.html#cloning-your-first-repository-with-git-4",
    "title": "Version Control Tutorial: Basics",
    "section": "Cloning Your First Repository With Git",
    "text": "Cloning Your First Repository With Git\n\n\n\n\nCopy the repository URL to the clipboard.\nChoose where to store the git repository and navigate there.\nEnter the git clone &lt;repo-url&gt; command in Git Bash.\n\n\nReplace the example URL with your actual repository URL from your clipboard.\n\nIn the Git bash window, type git clone then paste in the repository URL from your clipboard by right-clicking:\ngit clone https://github.com/jmwingenroth/my-first-repo\n\n\nSince we set the repository to “private”, you may be prompted to enter your GitHub credentials."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#exploring-your-cloned-repository",
    "href": "presentations/learning-lab-vc1/index.html#exploring-your-cloned-repository",
    "title": "Version Control Tutorial: Basics",
    "section": "Exploring Your Cloned Repository",
    "text": "Exploring Your Cloned Repository\n\nNow that we have cloned the repository, let’s explore what Git sees. The git status command is one of the most useful commands - it tells you the current state of your repository.\n\nAfter cloning, navigate into your repository folder:\ncd &lt;repository-name&gt;\nNow let’s check the status of our repository:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nThis tells us we’re on the main branch and everything is up to date!"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#making-changes",
    "href": "presentations/learning-lab-vc1/index.html#making-changes",
    "title": "Version Control Tutorial: Basics",
    "section": "Making Changes",
    "text": "Making Changes\n\nNow let’s make a change to our repository and learn how to commit it. We’ll edit the README file to add some content.\n\nLet’s make a change to the README.md file. Open it in a text editor and add some content:\n# My First Repository\n\nThis is my first Git repository for learning version control!\nNow let’s see what Git thinks about our changes:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#viewing-changes-with-git-diff",
    "href": "presentations/learning-lab-vc1/index.html#viewing-changes-with-git-diff",
    "title": "Version Control Tutorial: Basics",
    "section": "Viewing Changes with git diff",
    "text": "Viewing Changes with git diff\n\nBefore staging changes, it’s often helpful to review exactly what has changed in your files. The git diff command shows you the specific lines that were added, removed, or modified.\n\nTo see the specific changes you made, use git diff:\n$ git diff\ndiff --git a/README.md b/README.md\nindex e69de29..9f4d96d 100644\n--- a/README.md\n+++ b/README.md\n@@ -0,0 +1,2 @@\n # My First Repository\n+\n+This is my first Git repository for learning version control!\nThe output shows lines added (in green with +) and lines removed (in red with -), as well as the unchanged title line that GitHub added automatically."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#staging-changes-with-git-add",
    "href": "presentations/learning-lab-vc1/index.html#staging-changes-with-git-add",
    "title": "Version Control Tutorial: Basics",
    "section": "Staging Changes with git add",
    "text": "Staging Changes with git add\n\nBefore we can commit changes, we need to stage them. This tells Git which changes we want to include in our next commit.\n\nGit detected our changes! Now we need to stage them before committing:\n$ git add README.md\nLet’s check the status again:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   README.md\nOur changes are now staged and ready to commit!"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-commit-with-git-commit",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-commit-with-git-commit",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Commit with git commit",
    "text": "Creating a Commit with git commit\n\nA commit creates a permanent snapshot of the staged changes along with a descriptive message explaining what was changed and why.\n\nNow let’s create a commit with a descriptive message:\n$ git commit -m \"Add description to README file\"\n[main a1b2c3d] Add description to README file\n 1 file changed, 2 insertions(+)\nLet’s check our status again:\n$ git status\nOn branch main\nYour branch is ahead of 'origin/main' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#creating-a-commit-with-git-commit-1",
    "href": "presentations/learning-lab-vc1/index.html#creating-a-commit-with-git-commit-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Creating a Commit with git commit",
    "text": "Creating a Commit with git commit\n\nA commit creates a permanent snapshot of the staged changes along with a descriptive message explaining what was changed and why.\n\n\n\n\n\n\n\n\nFor some advice on writing reader-friendly commit messages, see this guide."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push",
    "href": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push",
    "title": "Version Control Tutorial: Basics",
    "section": "Sharing Changes with git push",
    "text": "Sharing Changes with git push\n\nOur commit exists locally, but we need to push it to GitHub so others can see our changes and so they’re backed up remotely.\n\nOur local repository now has changes that aren’t on GitHub yet. Let’s push them:\n$ git push"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push-1",
    "href": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Sharing Changes with git push",
    "text": "Sharing Changes with git push\n\nOur commit exists locally, but we need to push it to GitHub so others can see our changes and so they’re backed up remotely.\n\nOur local repository now has changes that aren’t on GitHub yet. Let’s push them:\n$ git push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 10 threads\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 314 bytes | 314.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nTo https://github.com/jmwingenroth/my-first-repo.git\n   9c99bfd..aa653de  main -&gt; main\n\n\nThe text response may vary slightly depending on your Git version. So long as there are no obvious errors, you are fine for today."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push-2",
    "href": "presentations/learning-lab-vc1/index.html#sharing-changes-with-git-push-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Sharing Changes with git push",
    "text": "Sharing Changes with git push\nNow let’s check our status one final time:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n\nPerfect! Our changes are now saved both locally and on GitHub. Open or refresh the repo page in your web browser to see the changes."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#recap",
    "href": "presentations/learning-lab-vc1/index.html#recap",
    "title": "Version Control Tutorial: Basics",
    "section": "Recap",
    "text": "Recap\n\nYou’ve learned the fundamental Git workflow that you’ll use daily. Remember that Git is a powerful tool that becomes easier with practice. The key is to commit early and often with clear, descriptive messages.\n\nAt this point you should have:\n\n\n✓ Created a repository on GitHub\n✓ Cloned it to your local machine\n✓ Made changes to files\n✓ Staged changes with git add\n✓ Committed changes with git commit\n✓ Pushed changes to GitHub with git push\n✓ Given yourself a pat on the back! 🎉"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#next-steps",
    "href": "presentations/learning-lab-vc1/index.html#next-steps",
    "title": "Version Control Tutorial: Basics",
    "section": "Next Steps",
    "text": "Next Steps\nAt the next workshop, we will focus on creating and managing branches and pull requests, which are key to collaborating with other team members on GitHub."
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#questions",
    "href": "presentations/learning-lab-vc1/index.html#questions",
    "title": "Version Control Tutorial: Basics",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#why-branch",
    "href": "presentations/learning-lab-vc2/index.html#why-branch",
    "title": "Version Control Tutorial: Branching",
    "section": "Why Branch?",
    "text": "Why Branch?\n\n\nBranching enables safe experimentation and effective collaboration. It’s the foundation of modern software development workflows. Start with a real-world scenario.\n\nImagine: You’re collaborating on a shared project. You want to add a new feature, but your teammate needs the current code to keep working. How do you add your feature without disrupting their work?\n\nAnswer: Branching!\n\n\nBranches let multiple people work on the same project simultaneously - you develop your feature on one branch while your teammates continue using the stable main branch."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#what-is-branching",
    "href": "presentations/learning-lab-vc2/index.html#what-is-branching",
    "title": "Version Control Tutorial: Branching",
    "section": "What is Branching?",
    "text": "What is Branching?\n\nBranching is one of Git’s most powerful features. It allows you to create parallel versions of your code without affecting the main branch.\n\n\nGit allows us to create different working versions, or branches, of our repository.\nThis lets you take a snapshot of the repository and try out a new idea without affecting your main branch.\nIf you are collaborating with others, they can keep using (and editing) the main branch while you work in parallel.\nOnce satisfied with your changes, you can merge them back into main using a Pull Request."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#example-git-branching",
    "href": "presentations/learning-lab-vc2/index.html#example-git-branching",
    "title": "Version Control Tutorial: Branching",
    "section": "Example Git Branching",
    "text": "Example Git Branching"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#the-branching-workflow",
    "href": "presentations/learning-lab-vc2/index.html#the-branching-workflow",
    "title": "Version Control Tutorial: Branching",
    "section": "The Branching Workflow",
    "text": "The Branching Workflow\n\nThis workflow ensures that changes are properly reviewed and tested before being merged into the main branch.\n\n\nPull any code updates! git pull\nCreate and check out a new branch. git branch and git checkout\nMake and push commits to the new branch. git add, git commit, and git push\nWhen ready to merge, create a Pull Request in GitHub\nReview the Pull Request\nMerge the Pull Request\nCheck out and pull main branch. git checkout and git pull"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#understanding-the-main-branch",
    "href": "presentations/learning-lab-vc2/index.html#understanding-the-main-branch",
    "title": "Version Control Tutorial: Branching",
    "section": "Understanding the Main Branch",
    "text": "Understanding the Main Branch\n\nYou may have realized that Git creates a branch by default, called main. Think of it as the trunk of a tree.\n\nGit creates a default branch called main.\nYou could think of the main branch as the trunk of a tree.\n\n\nOlder repositories may use master as the default branch name. GitHub changed the default to main in mid-2020."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#understanding-the-main-branch-1",
    "href": "presentations/learning-lab-vc2/index.html#understanding-the-main-branch-1",
    "title": "Version Control Tutorial: Branching",
    "section": "Understanding the Main Branch",
    "text": "Understanding the Main Branch\nGit creates a default branch called main.\nYou could think of the main branch as the trunk of a tree.\n\nGit Bash shows the active branch name to the right of the prompt."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-branches",
    "href": "presentations/learning-lab-vc2/index.html#creating-branches",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating Branches",
    "text": "Creating Branches\n\nCreating a branch is simple. Most people use the shortcut command to create and check out in one step.\n\nCreate and check out a new branch in Git Bash in one step:\ngit checkout -b your-name-here\n\nSwitched to a new branch 'your-name-here'\n\n\nAfter checking out your branch, the branch name should show up next to the command line if you are using Git Bash."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#for-our-exercise",
    "href": "presentations/learning-lab-vc2/index.html#for-our-exercise",
    "title": "Version Control Tutorial: Branching",
    "section": "For Our Exercise",
    "text": "For Our Exercise\n\nCreate a blank file called your-name.txt in the poems/ subfolder.\nFind a poem on the internet. If you need inspiration, check out poets.org/poems\nCopy-paste the text into your-name.txt\nInject the text “TYPO” somewhere in the poem\nSave the file\nStage and commit your changes"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#pushing-your-branch",
    "href": "presentations/learning-lab-vc2/index.html#pushing-your-branch",
    "title": "Version Control Tutorial: Branching",
    "section": "Pushing Your Branch",
    "text": "Pushing Your Branch\n\nThe first time you push a new branch, Git needs to know where to send it on the remote server.\n\nAfter making commits on your branch, push them to GitHub.\nThe first time you push a new branch, Git will give you an error about no “upstream” branch. Simply run the command Git suggests:\ngit push --set-upstream origin your-name-here\n\n\nAfter the first push, you can use git push normally for subsequent pushes to this branch."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#what-is-a-pull-request",
    "href": "presentations/learning-lab-vc2/index.html#what-is-a-pull-request",
    "title": "Version Control Tutorial: Branching",
    "section": "What is a Pull Request?",
    "text": "What is a Pull Request?\n\nIt’s important to clarify what a Pull Request is, since the name can be confusing and is different from git pull.\n\nA Pull Request (or “PR”) is a GitHub feature that lets you propose merging your branch into another branch (usually main).\n\nIt’s a request for your code changes to be reviewed and merged.\nIt provides a space for discussion, code review, and approval.\nSome might say it is a bit of a misnomer. It isn’t related to using git pull to bring changes from GitHub to your local machine."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\n\nOnce you’ve made commits and are ready to merge back into main, you’ll create a Pull Request on GitHub.\n\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-1",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-1",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\n\nOnce you’ve made commits and are ready to merge back into main, you’ll create a Pull Request on GitHub.\n\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page\nClick the “Pull requests” tab"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-2",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-2",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page\nClick the “Pull requests” tab\nClick “New pull request” button"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-3",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-3",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page\nClick the “Pull requests” tab\nClick “New pull request” button\nSelect main as the “base” branch and your new branch as the “compare” branch"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-4",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-4",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page\nClick the “Pull requests” tab\nClick “New pull request” button\nSelect main as the “base” branch and your new branch as the “compare” branch\nAdd a title and description for the Pull Request"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-5",
    "href": "presentations/learning-lab-vc2/index.html#creating-a-pull-request-5",
    "title": "Version Control Tutorial: Branching",
    "section": "Creating a Pull Request",
    "text": "Creating a Pull Request\nOnce your changes are ready, create a Pull Request in GitHub:\n\nNavigate to the repository GitHub page\nClick the “Pull requests” tab\nClick “New pull request” button\nSelect main as the “base” branch and your new branch as the “compare” branch\nAdd a title and description for the Pull Request\nClick “Create pull request”"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#reviewing-pull-requests",
    "href": "presentations/learning-lab-vc2/index.html#reviewing-pull-requests",
    "title": "Version Control Tutorial: Branching",
    "section": "Reviewing Pull Requests",
    "text": "Reviewing Pull Requests\n\nCode review can be daunting, but GitHub makes it easy by highlighting exactly what changed.\n\nPull requests provide a great opportunity for collaborators to review code and ensure accuracy.\nGitHub makes this process easy by:\n\nIdentifying lines of code that have changed\nAllowing collaborators to comment on specific lines\nShowing additions, deletions, and modifications clearly\nProviding a structured review workflow"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#how-to-review-code",
    "href": "presentations/learning-lab-vc2/index.html#how-to-review-code",
    "title": "Version Control Tutorial: Branching",
    "section": "How to Review Code",
    "text": "How to Review Code\n\nWalk through the code review process step by step, showing how to comment and suggest changes.\n\nTo review a pull request:\n\nNavigate to the Pull Request and click “Files changed”"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#how-to-review-code-1",
    "href": "presentations/learning-lab-vc2/index.html#how-to-review-code-1",
    "title": "Version Control Tutorial: Branching",
    "section": "How to Review Code",
    "text": "How to Review Code\nTo review a pull request:\n\nNavigate to the Pull Request and click “Files changed”\nComment on your favorite line by clicking the + next to the line number"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#how-to-review-code-2",
    "href": "presentations/learning-lab-vc2/index.html#how-to-review-code-2",
    "title": "Version Control Tutorial: Branching",
    "section": "How to Review Code",
    "text": "How to Review Code\nTo review a pull request:\n\nNavigate to the Pull Request and click “Files changed”\nComment on your favorite line by clicking the + next to the line number\nFind the line with “TYPO” injected"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#how-to-review-code-3",
    "href": "presentations/learning-lab-vc2/index.html#how-to-review-code-3",
    "title": "Version Control Tutorial: Branching",
    "section": "How to Review Code",
    "text": "How to Review Code\nTo review a pull request:\n\nComment on your favorite line by clicking the + next to the line number\nFind the line with “TYPO” injected\nSuggest a code change to remove the typo using GitHub’s suggestion feature"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#how-to-review-code-4",
    "href": "presentations/learning-lab-vc2/index.html#how-to-review-code-4",
    "title": "Version Control Tutorial: Branching",
    "section": "How to Review Code",
    "text": "How to Review Code\nTo review a pull request:\n\nFind the line with “TYPO” injected\nSuggest a code change to remove the typo using GitHub’s suggestion feature\nComplete your review: approve, request changes, or just comment. For today, approve.\n\n\n\nLearn more in the GitHub documentation"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#addressing-review-feedback",
    "href": "presentations/learning-lab-vc2/index.html#addressing-review-feedback",
    "title": "Version Control Tutorial: Branching",
    "section": "Addressing Review Feedback",
    "text": "Addressing Review Feedback\n\nIf a reviewer requests changes, you can simply make more commits to the same branch.\n\nIf the reviewer requests changes:\n\nMake the requested changes in your branch\nCommit and push to the same branch\nThe Pull Request will automatically update with your new commits\nOnce approved, you can merge!"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#merging-pull-requests",
    "href": "presentations/learning-lab-vc2/index.html#merging-pull-requests",
    "title": "Version Control Tutorial: Branching",
    "section": "Merging Pull Requests",
    "text": "Merging Pull Requests\n\nOnce the Pull Request has been reviewed and approved, you can merge it back into main.\n\nOnce approved, merge your changes:\n\nGo to the “Conversation” tab of the Pull Request"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#merging-pull-requests-1",
    "href": "presentations/learning-lab-vc2/index.html#merging-pull-requests-1",
    "title": "Version Control Tutorial: Branching",
    "section": "Merging Pull Requests",
    "text": "Merging Pull Requests\n\nOnce the Pull Request has been reviewed and approved, you can merge it back into main.\n\nOnce approved, merge your changes:\n\nGo to the “Conversation” tab of the Pull Request\nClick the “Merge pull request” button at the bottom\n\nThe button should be green because your reviewer approved your changes."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#merging-pull-requests-2",
    "href": "presentations/learning-lab-vc2/index.html#merging-pull-requests-2",
    "title": "Version Control Tutorial: Branching",
    "section": "Merging Pull Requests",
    "text": "Merging Pull Requests\nOnce approved, merge your changes:\n\nGo to the “Conversation” tab of the Pull Request\nClick the “Merge pull request” button at the bottom\nYour branch’s changes are now in the main branch!"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#after-merging",
    "href": "presentations/learning-lab-vc2/index.html#after-merging",
    "title": "Version Control Tutorial: Branching",
    "section": "After Merging",
    "text": "After Merging\n\nAfter merging on GitHub, update your local repository to reflect the changes.\n\nAfter merging on GitHub, update your local repository:\ngit checkout main\ngit pull"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#after-merging-1",
    "href": "presentations/learning-lab-vc2/index.html#after-merging-1",
    "title": "Version Control Tutorial: Branching",
    "section": "After Merging",
    "text": "After Merging\nAfter merging on GitHub, update your local repository:\ngit checkout main\ngit pull\nNow your local main branch has all the changes from your merged branch!"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#cleaning-up-branches",
    "href": "presentations/learning-lab-vc2/index.html#cleaning-up-branches",
    "title": "Version Control Tutorial: Branching",
    "section": "Cleaning Up Branches",
    "text": "Cleaning Up Branches\n\nAfter merging, you can delete the branch to keep your repository tidy.\n\nIdeally, branches are deleted after merging to keep things tidy.\nDelete the remote branch on GitHub:\nAfter merging, GitHub shows a “Delete branch” button in the Pull Request.\nDelete your local branch:\ngit branch -d &lt;branch-name&gt;\n\n\nUse -D instead of -d to force delete an unmerged branch (be careful!)."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#handling-conflicts",
    "href": "presentations/learning-lab-vc2/index.html#handling-conflicts",
    "title": "Version Control Tutorial: Branching",
    "section": "Handling Conflicts",
    "text": "Handling Conflicts\n\nSometimes changes in your branch conflict with changes that were made to main. This requires manual resolution.\n\nSometimes the main branch has changes that conflict with your branch.\nGitHub will indicate when there are conflicts that prevent automatic merging."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#handling-conflicts-1",
    "href": "presentations/learning-lab-vc2/index.html#handling-conflicts-1",
    "title": "Version Control Tutorial: Branching",
    "section": "Handling Conflicts",
    "text": "Handling Conflicts\nWhen conflicts occur:\ngit checkout main\ngit pull\ngit checkout &lt;new-branch-name&gt;\ngit merge main\n\n\nGit will mark the conflicting sections in your files. Edit them to resolve the conflicts, then commit the resolution."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#recap",
    "href": "presentations/learning-lab-vc2/index.html#recap",
    "title": "Version Control Tutorial: Branching",
    "section": "Recap",
    "text": "Recap\n\nYou’ve learned the branching workflow that enables safe, collaborative development with Git.\n\nAt this point you should understand:\n\n\n✓ Why branching is essential for collaboration\n✓ How to create and check out branches\n✓ How to create and review Pull Requests\n✓ What to look for when reviewing code\n✓ How to merge PRs and clean up branches\n✓ How to handle merge conflicts"
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#next-steps",
    "href": "presentations/learning-lab-vc2/index.html#next-steps",
    "title": "Version Control Tutorial: Branching",
    "section": "Next Steps",
    "text": "Next Steps\nContinue practicing the branching workflow in your projects!\nFor more guidance, visit the RFF Data Governance website."
  },
  {
    "objectID": "presentations/learning-lab-vc2/index.html#questions",
    "href": "presentations/learning-lab-vc2/index.html#questions",
    "title": "Version Control Tutorial: Branching",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Learning Lab: Data Management\nLearning Lab: Version Control Part 1, Using Git\nLearning Lab: Version Control Part 2, Branching and Pull Requests\nLearning Lab: Software Quality",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-this-resource",
    "href": "index.html#about-this-resource",
    "title": "Home",
    "section": "",
    "text": "This guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-outline",
    "href": "index.html#site-outline",
    "title": "Home",
    "section": "Site Outline",
    "text": "Site Outline\n\nFoundations\nData Management\nVersion Control\nSoftware Quality",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-and-feedback",
    "href": "index.html#questions-and-feedback",
    "title": "Home",
    "section": "Questions and feedback",
    "text": "Questions and feedback\nThis is a living resource — it will continue to evolve as needs shift and feedback is incorporated.\nTo submit questions, bugs (e.g., broken hyperlinks), suggestions, or feedback on this guidance, click Report an issue on the right-hand side of this page and submit an issue to the repository. Note that your comment will be publicly visible.\nTo submit a question or comment over email, reach out to the Data Governance Working Group.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-authors",
    "href": "index.html#site-authors",
    "title": "Home",
    "section": "Site authors",
    "text": "Site authors\nCurrent members of the RFF Data Governance Working Group\n\nAris Awang\nPenny Liao\nMcKenna Peplinski\nMatthew Wibbenmeyer\nAlexandra Thompson\n\nPast members\n\nCaroline Hamilton\nEthan Russell\nMike Toman\nJohn Valdez\nJordan Wingenroth",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#see-also",
    "href": "index.html#see-also",
    "title": "Home",
    "section": "See also",
    "text": "See also\n\nRFF Research Integrity Guidance\nRFF Information Technology (IT) Best Practices\nRFF Artifical Intelligence (AI) Policy\nRFF Communication Norms and Guidelines",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/data-management/index.html",
    "href": "docs/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Data management encompasses the methods used to collect, store, organize, and use data. Please review Foundations for an overview of the role of data management in the research process.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html",
    "href": "docs/data-management/organization.html",
    "title": "Organization",
    "section": "",
    "text": "Standardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#summary",
    "href": "docs/data-management/organization.html#summary",
    "title": "Organization",
    "section": "1 Summary",
    "text": "1 Summary\n\nKeep raw data in a dedicated folder (raw/) and never modify it directly.\nUse a simple, descriptive folder structure that reflects project logic and is easy for new users to understand.\nSupport version control by organizing code and outputs in a way that works with Git, i.e. by creating separate repository folders for each user.\nDesign for flexibility, allowing new data, methods, or collaborators without major reorganization.\nName files/folders to be human- and machine-readable: use clear names, avoid spaces and special characters, and include numeric prefixes for order.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#sec-directory-structure",
    "href": "docs/data-management/organization.html#sec-directory-structure",
    "title": "Organization",
    "section": "2 Directory structure",
    "text": "2 Directory structure\nRegardless of the specific method deployed, your data project organization should have the following qualities:\n\nRaw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here.\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L:/ drive. This version illustrates a personal repository folder model of code version control, which operates best on the L:/ drive.\nproject_name/\n├── README.txt\n├── data/\n│   ├── raw/\n│   │   ├── README_raw_dataset.txt\n│   ├── intermediate/\n│   ├── clean/ (optional)\n│   │   ├── metadata.txt\n├── results*/\n├── repos/\n│   ├── baker/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n│   ├── johnson/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n\n2.1 Data folder\n\nThe data folder contains all project data sets.\nRaw data is preserved in its own subfolder. This folder also contains raw data documentation README files.\nThe intermediate folder contains datasets created during data cleaning and processing.\nIf practical, a clean folder can contain cleaned output datasets, but note that it’s often unclear when datasets are truly “clean” until late project stages. If this folder contains final data products to be shared, be sure to include metadata documentation.\n\n\n\n\n\n\n\nNote\n\n\n\nFor projects with small datasets, this folder can be version controlled, with larger files ignored using the .gitignore file. In this case, this folder would be a subfolder of individuals’ repository folders.\n\n\n\n\n2.2 Results folder\n\nThe results folder contains analysis results, and model outputs. For example, it can be used to store tables, figures, and model estimates.\n*For internal RFF projects, the results folder can be stored in the main directory and not within repositories.\n**For projects with external collaborators, it may be useful to store the results folder within repositories. These files are generally smaller than 100 MB and can be stored in a main repository using GitHub; however, some formats (such as SVG) can be quite large. The .gitignore file can be configured to ignore certain file types (such as SVG files, when both PNG and SVG file formats are generated).\n\n\n\n2.3 Repos folder\n\nThis directory structure is based on personal repositories. The repos, or repositories, directory contains version-controlled files. To allow individuals to work on version controlled files without interfering with others’ versions, each researcher should have their own folder that’s linked to the GitHub remote repository. Team members can then clone the project’s Git repository into their respective folders and work exclusively within their own copies. Changes can be synced and reconciled using GitHub, preventing simultaneous edits of the same file and ensuring effective version control.\nIndividual folders (e.g. Edgar, Caudle) should have mirrored (the same) directory structures.\n\n\n\n2.4 Scripts folders\n\nThe scripts folder contains all code files (e.g. .R, .py, .do) for the project. These scripts provide explicit instructions for processing data and performing analyses. It should be possible to reproduce the entirety of the project’s processed data sets and results using only these scripts and the raw data.\nThe scripts folder can be parsed into subfolders containing scripts that process raw data (processing), analyze processed data (analysis), and tools. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software.\n\n\n\n2.5 Docs folder\nThe documents folder should contain any version-controlled shared documents (e.g. LaTeX, Markdown, Overleaf).\n\n\n2.6 Other project files\nThis template does not include specific folders for meeting notes, literature reviews, presentations, project management, etc., because those types of files are not the focus of this guidance.\nWe recommend storing these types of files in the project-level Microsoft Teams folder. See the RFF Communication Norms and Guidance for more information.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#other-organization-practices",
    "href": "docs/data-management/organization.html#other-organization-practices",
    "title": "Organization",
    "section": "3 Other organization practices",
    "text": "3 Other organization practices\n\n3.1 Subfolders\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (e.g., regressions or projections).\n\nIdeally, project folders should be organized so that each file’s complete path is informative about the role of the folder’s contents in the project. Some examples are:\n\nA script for cleaning Corelogic transactions data: scripts/corelogic/01_cleaning/01_clean_trans_data.R\nRaw Corelogic transactions data from Los Angeles County (FIPS code 06037): raw_data/corelogic/transactions/06037.dta\nA regression table for the outcome variable property values, with heterogeneity results by region: results/regressions/property_value/table_region.tex",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "href": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "title": "Organization",
    "section": "4 Naming folders, files and scripts",
    "text": "4 Naming folders, files and scripts\nWhen creating folders:\n\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders.\n\nWhen creating data or script files, make them:\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content. For example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you must create different versions of files, make sure to document the distinction in a README file.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a project-level README file.\n\nMachine readable:\n\nUse only ASCII characters (letters, numbers, and underscores).\nDo not include spaces or special characters (/  : * ? &lt;&gt; &).\nUse hyphens or underscores instead of spaces (the “snake_case” method).\nFiles and folders should be easy to search and filter based on name using structured file names. The ability to sort and read files by name is useful and helps organization but requires specific conventions. See examples in the table below, keeping in mind that:\n\nIt is not recommended to use dates in script file names to denote when a script was created or modified. Instead, leverage version control to save and document different script versions.\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the file name prefixes enter the double digits.\n\n\n\n\n\n\n\n\n\n\n\nPractice for structured file names\nDescription\nExample\n\n\n\n\nID-based names\nStructure file paths and names in a way that makes them easy to access programmatically—e.g., enabling batch loading or iteration across identifiers like years/dates, counties, or scenarios. For example, storing county-level data as shown would allow data to be read into memory by simply looping over FIPS codes as they appear in the directory file names.\n53019_data.csv53033_data.csv53061_data.csv\n\n\nChronological order\nUse ISO 8601 format for date-based files: YYYY-MM-DD. This ensures dates sort correctly by default.\n2021_01_01_precipitation_mm.csv2021_01_02_precipitation_mm.csv2021_01_01_temperature_statistics_f.csv2021_01_02_temperature_statistics_f.csv\n\n\nLogical processing order\nFor scripts or folders that must run in sequence, use numeric prefixes to indicate the intended order of execution.\n01_clean_raw_data.R02_merge_clean_data.R03_descriptive_statistics.R04_regressions.R",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html",
    "href": "docs/data-management/sensitive-proprietary.html",
    "title": "Sensitive & Proprietary Data",
    "section": "",
    "text": "When using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "href": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "title": "Sensitive & Proprietary Data",
    "section": "1 Identify and categorize sensitive data",
    "text": "1 Identify and categorize sensitive data\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.\n\nTo determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "href": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "title": "Sensitive & Proprietary Data",
    "section": "2 Document data sensitivity and restrictions",
    "text": "2 Document data sensitivity and restrictions\n\nDocument data sensitivity class and details in both the project-level README and, if the data are secondary, the associated raw data README file. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "href": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "title": "Sensitive & Proprietary Data",
    "section": "3 Determine appropriate security and privacy measures",
    "text": "3 Determine appropriate security and privacy measures\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "href": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "title": "Sensitive & Proprietary Data",
    "section": "4 Data derivatives: masking and aggregation",
    "text": "4 Data derivatives: masking and aggregation\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "href": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "title": "Sensitive & Proprietary Data",
    "section": "5 Review before publication or sharing",
    "text": "5 Review before publication or sharing\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html",
    "href": "docs/data-management/archival.html",
    "title": "Archival & Disposal",
    "section": "",
    "text": "Archiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:\n\nensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\n\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see sharing API data.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\n\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\n\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#archival",
    "href": "docs/data-management/archival.html#archival",
    "title": "Archival & Disposal",
    "section": "",
    "text": "Archiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:\n\nensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\n\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see sharing API data.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\n\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\n\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#disposal",
    "href": "docs/data-management/archival.html#disposal",
    "title": "Archival & Disposal",
    "section": "2 Disposal",
    "text": "2 Disposal\nSome data may need to be deleted to protect sensitive information or comply with regulations, data agreements, or funder requirements. This is often referred to as data disposition. If any of these requirements applies to a project, follow these best practices when deleting data:\n\nVerify Requirements: Confirm funder agreements and legal obligations regarding data retention and deletion.\nSource Deletion: Confirm with IT that the files were fully deleted in accordance with requirements (e.g., backup files).\nDocumentation: Record when and how data was deleted.\n\nThese practices should apply to data stored the RFF network, OneDrive, or Microsoft Teams.",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/foundations/index.html",
    "href": "docs/foundations/index.html",
    "title": "Foundations",
    "section": "",
    "text": "Good data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):\n\nShort-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#introduction",
    "href": "docs/foundations/index.html#introduction",
    "title": "Foundations",
    "section": "",
    "text": "Good data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):\n\nShort-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#the-data-life-cycle",
    "href": "docs/foundations/index.html#the-data-life-cycle",
    "title": "Foundations",
    "section": "The data life cycle",
    "text": "The data life cycle\nA common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. Most activities in the data life cycle come before the analysis phase and are closely tied to data management. There are many different models of the data life cycle, and the relevant model for your individual project will vary. A general data life cycle is depicted below (see also Langseth et al. 2015).\n\n\n\n\n\ngraph LR\nA(Plan) --&gt; B(Collect)\nB --&gt; C(Process)\nC --&gt; D(Explore / Visualize)\nD --&gt; E(Analyze / Model)\nE --&gt; F(Archive, Publish, Share)\nE --&gt; C\nE --&gt; A\n\n\n\n\n\n\nThe data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:\n\nStep 1: Plan Identify data that will be collected and how it will be managed. Create a data management plan.\nStep 2: Collect Acquire and store raw data.\n\na. Acquire Retrieve data from the appropriate source.\nb. Describe Document the raw data source, format, variables, measurement units, coded values, and known problems.\nc. Quality assurance Inspect the raw data for quality and fit for analysis purpose.\nd. Store Store the raw data in the appropriate folder, as determined in the planning stage. Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. Make raw data files read-only so they cannot be accidentally modified.\n\nStep 3: Process Prepare the data for exploration and analysis.\n\na. Clean Preprocess the data to correct errors, standardize missing values, standardize formats, etc.\nb. Transform Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).\nc. Integrate Combine datasets.\n\nStep 4: Explore Describe, summarize, and visualize statistics and relationships.\nStep 5: Analyze / Model Develop, refine, and implement analysis and model specifications.\nStep 6: Archive, Publish, and Share Finalize documentation (project-level README and metadata). Dispose and/or archive data. Publish final data products and documentation.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#sec-dmp",
    "href": "docs/foundations/index.html#sec-dmp",
    "title": "Foundations",
    "section": "The first step: Data management planning for reproducibility",
    "text": "The first step: Data management planning for reproducibility\nData management planning, a form of preliminary documentation, is the process of thinking ahead about how your team will access, use, create, modify, store, share, and describe data related to your research project. Data management plans (DMPs) enhance collaboration by establishing baseline expectations, make projects resilient to turnover, and save time in the long run. In addition, many funders require data management plans be submitted with grant proposals, so thinking about these issues early can facilitate the proposal process.\nThis guidance resource provides general instructions for data practices and addresses many of the core questions that are part of the DMP process. At a minimum, the questions below should be reviewed at the start of a project. Associated guidance is linked.\n\nWhere will data/code be stored and how will it be organized?\nHow will the team use Microsoft Teams for file storage and communication? What types of files will be stored in the Teams folder versus the project’s L:/ drive folder? What will be communicated over Teams chat versus email or GitHub?\nWho will be responsible for disposing / archiving data?\nWho will be responsible for publishing data/code and attaching appropriate documentation and use licenses?\nWhat will the version control / git / GitHub workflow be?\nWhat coding software and main libraries will be used?\nHow will code be reviewed for quality?\nWhat are expectations around data quality and code quality?\nHow will data sources, code, and major methodological decisions be documented?\nHas the appropriate budget been allocated to implement this DMP?\n\nCertain projects may require additional considerations. For development of a more thorough DMP, refer to the UCSB NCEAS data management planning section.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html",
    "href": "docs/software/additional-resources.html",
    "title": "Additional Learning Resources",
    "section": "",
    "text": "Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510\nBest Practices when Writing Code by Daniel M. Sullivan\nData science for economists by Grant McDermott",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#general",
    "href": "docs/software/additional-resources.html#general",
    "title": "Additional Learning Resources",
    "section": "",
    "text": "Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510\nBest Practices when Writing Code by Daniel M. Sullivan\nData science for economists by Grant McDermott",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#r",
    "href": "docs/software/additional-resources.html#r",
    "title": "Additional Learning Resources",
    "section": "2 R",
    "text": "2 R\n\n2.1 General\n\nR for Reproducible Scientific Analysis by Software Carpentry\nR for Data Science (2e) by Hadley Wickham et al.\n\n\n\n2.2 Beginner\n\nLearn R by Codecademy\nHands-On Programming with R by Garrett Grolemund\n\n\n\n2.3 Intermediate/Advanced\n\nNCEAS Learning Hub’s coreR Course\nAdvanced R by Hadley Wickham\nEfficient R programming by Colin Gillespie\n\n\n\n2.4 Visualization\n\nData Visualization: A Practical Introduction by Kieran Healy\nggplot2: Elegant Graphics for Data Analysis (3e) by Hadley Wickham et al.\n\n\n\n2.5 GIS\n\nGeocomputation with R by Robin Lovelace et al. \nGeographic Data Science with R: Visualizing and Analyzing Environmental Change by Michael Wimberly\nSpatial Data Science by Edzer Pebesma and Roger Bivand\nRFF GIS Learning Lab",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#python",
    "href": "docs/software/additional-resources.html#python",
    "title": "Additional Learning Resources",
    "section": "3 Python",
    "text": "3 Python\n\nThe Python Tutorial\nPython Datascience Handbook by Jake VanderPlas\nData Science from Scratch by Joel Grus\nAutomate the Boring Stuff with Python by Al Sweigart\nPlotting and Programming in Python by Software Carpentry\nPython Web Scraping: Full Tutorial With Examples (2025) by Kevin Sahin\nFluent Python by Luciano Ramalho",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#julia",
    "href": "docs/software/additional-resources.html#julia",
    "title": "Additional Learning Resources",
    "section": "4 Julia",
    "text": "4 Julia\n\nJulia Manual\nJulia Academy\nJulia Data Science by Jose Storopoli et al\nGetting Started · DataFrames.jl\nTutorial · Plots\nJulia Performance Tips",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#stata",
    "href": "docs/software/additional-resources.html#stata",
    "title": "Additional Learning Resources",
    "section": "5 Stata",
    "text": "5 Stata\n\nStata Users Guide\nUCLA Stata Learning Modules",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/additional-resources.html#matlab",
    "href": "docs/software/additional-resources.html#matlab",
    "title": "Additional Learning Resources",
    "section": "6 MATLAB",
    "text": "6 MATLAB\n\nMATLAB Onramp Course\nMATLAB: A Practical Approach by Stormy Attaway\nNumerical Methods using MATLAB by George Lindfield and John Penny",
    "crumbs": [
      "Software Quality",
      "Additional Learning Resources"
    ]
  },
  {
    "objectID": "docs/software/reproducibility.html",
    "href": "docs/software/reproducibility.html",
    "title": "Reproducibility",
    "section": "",
    "text": "Reproducibility refers to the ability to obtain consistent results when using the same data, methods, and analyses. Ensuring that our work is reproducible is essential—not only so that our findings can be verified by ourselves and by external researchers, but also to facilitate transparency and credibility. A reproducible workflow also makes it easier to repurpose parts of the code for other projects, saving time and reducing the likelihood of introducing errors.\nHere we discuss two ways to enhance the reproducibility of the analysis:",
    "crumbs": [
      "Software Quality",
      "Reproducibility"
    ]
  },
  {
    "objectID": "docs/software/reproducibility.html#project-script-structure",
    "href": "docs/software/reproducibility.html#project-script-structure",
    "title": "Reproducibility",
    "section": "1 Project script structure",
    "text": "1 Project script structure\nFor reproducibility, it is important to break down the project workflow into a logical sequence of scripts and clearly document the order in which they should be executed. We recommend the following practices:\n\n1.1 Split a code base into multiple scripts\nBreaking a project into a series of scripts makes it easier to test, maintain, and reuse code compared to working in a single monolithic file. While there is no strict rule for how to divide code, the following are good practices:\n\nFocus on one substantial responsibility per script.\nEach script should perform a clearly defined task, such as pre-processing a large dataset, creating geographical overlays, merging supplementary datasets, generating descriptive statistics, or running a specific analysis.\nSeparate time-consuming steps.\nIf a procedure is computationally expensive, place it in its own script and save the output as an intermediate dataset for later steps. This avoids unnecessarily re-running lengthy processes, such as complex geospatial calculations or aggregating large datasets.\nIsolate key intermediate outputs.\nIf a block of code produces an output that will be used in multiple subsequent steps, place it in a dedicated script and save the output for reuse.\nExtract reusable code blocks.\nIf a section of code—such as a function—is repeated across multiple scripts, centralize it (along with the required libraries) in a single location. The code block or function can then be sourced or imported from other scripts. For example:\n\nIn R: source(\"script_name.R\")\n\nIn Python: import module_name\n\nIn Stata: do filename.do\n\n\n\n\n1.2 Number scripts and script folders\nScript names can begin with a two-digit number (e.g., 00_, 01_, 02_) to encode execution order. When a folder is sorted by name, the scripts appear in the intended run sequence. For steps that can run in parallel, append letters (e.g., 01a_, 01b_, 01c_).\nApply the same scheme to folders (e.g., 00_setup, 01_processing, 02_analysis) so dependencies are clear—for example, cleaning steps run before analysis.\nAn example of this structure is provided below.\n\nExample Script Folder\nscripts/\n|--00_tools/                          # Shared helpers, not part of pipeline\n|   |--filters.R                      # Reusable filter functions\n|   |--data_io.R                      # Reusable input/output wrapper functions\n|\n|--01_processing/                     # Data prep and integration\n|   |--01_clean.R                     # Clean raw data\n|   |--02_aggregate.R                 # Aggregate to analysis unit\n|   |--03_merge_external.R            # Merge with supplementary datasets\n|\n|--02_analysis/                       # Analysis and outputs\n|   |--01a_summary_stats.R            # Summary statistics\n|   |--01b_map.R                      # Descriptive spatial visualizations\n|   |--01c_time_series_figure.R       # Descriptive time series plots\n|   |--02_main_regressions.R          # Main statistical analysis\n|   |--03_robustness_checks.R         # Alternative specifications\n\n\n\n1.3 Create a run_all script\nA run_all script executes all other scripts in the project in the correct order. This script documents the full workflow and can be used to reproduce the project’s complete set of outputs, including intermediate data products and final results, starting from the raw data.\nWhen using script numbering together with a run_all script, it is important to keep both up to date whenever new scripts are added or existing scripts are reorganized.\nIf the project uses global parameters referenced across multiple scripts, place them in a dedicated configuration script and load that script at the beginning of the run_all file.\n\nExample run_all.R Script\n# This script runs all the code in my project, from scratch\n\n# Prepare analysis data\nsource(\"scripts/01_processing/01_clean.R\")                    \nsource(\"scripts/01_processing/02_aggregate.R\")       \nsource(\"scripts/01_processing/03_merge_external.R\")\n\n# Descriptive stats and figures\nsource(\"scripts/02_analysis/01a_summary_stats.R\")                    \nsource(\"scripts/02_analysis/01b_map.R\")       \nsource(\"scripts/02_analysis/01c_time_series_figure.R\")\n\n# Analysis\nsource(\"scripts/02_analysis/02_main_regressions.R\")\nsource(\"scripts/02_analysis/03_robustness_checks.R\")",
    "crumbs": [
      "Software Quality",
      "Reproducibility"
    ]
  },
  {
    "objectID": "docs/software/reproducibility.html#package-management",
    "href": "docs/software/reproducibility.html#package-management",
    "title": "Reproducibility",
    "section": "2 Package management",
    "text": "2 Package management\nAn analysis using the same code and data may not run identically across different machines if the machines have different operating systems, application versions, or package versions installed. Therefore, package management is a critical component of reproducibility.\nVirtual environments provide a way to isolate the software dependencies of a project, such as package versions and system libraries, so that they are consistent over time and independent of other projects on the same machine. Maintaining separate environments for different projects prevents conflicts between dependencies, allows older analyses to remain reproducible even as packages evolve, and supports collaborative work by ensuring that all contributors run the code under the same computational conditions.\nThree common tools for managing computational environments are renv, conda and Docker.\n\nrenv (for R) creates project-specific environments that record and restore the exact package versions used in an analysis. This ensures that results can be reproduced even if packages have been updated globally.\nconda (for Python, R, and other languages) manages project-specific environments by explicitly specifying package versions and system dependencies. conda helps ensure that analyses run consistently across machines and over time, particularly for workflows that rely on compiled libraries or mixed-language dependencies.\nDocker goes further by packaging the entire computational environment (including the operating system and versions of R, Python, Julia, and all associated packages) into a self-contained container that can be run on any machine.\n\nAt this time, the working group has not fully tested the functionality of these tools. The resources listed below are provided for reference. We would love to hear from users who have implemented any of these tools in RFF’s computing environment. Please get in touch with us to share your experience.\n\nrenv\n\nIntroduction to renv\nVideo walkthrough: Using renv to track the version of your packages in R\nRStudio user guide: renv\n\n\n\nconda\n\nGetting started with conda\nManaging environments\nLesson by the Carpentries Incubator: Introduction to Conda for (Data) Scientists\n\n\n\nDocker\n\nDocker 101 Tutorial\nGet started: Introduction\nUse of Docker for Reproducibility in Economics",
    "crumbs": [
      "Software Quality",
      "Reproducibility"
    ]
  },
  {
    "objectID": "docs/software/flexibility.html",
    "href": "docs/software/flexibility.html",
    "title": "Flexibility",
    "section": "",
    "text": "Flexibility refers to the extent to which code can be easily adapted and modified to accommodate changes in external factors, data, or methodology. Code with hard-coded values (i.e. specific values are directly embedded in the source) will be inflexible, because the script must be edited anytime those values change.\nWe recommend three practices to enhance program flexibility:",
    "crumbs": [
      "Software Quality",
      "Flexibility"
    ]
  },
  {
    "objectID": "docs/software/flexibility.html#use-functions",
    "href": "docs/software/flexibility.html#use-functions",
    "title": "Flexibility",
    "section": "1 Use functions",
    "text": "1 Use functions\nMost operations can be written as functions, which can then be applied to different datasets or variables. Because functions are reusable, they not only provide greater flexibility but also help avoid redundant code. Repeating the same operation by “copy-pasting” it into multiple parts of the code makes it harder to maintain, as any change must be replicated in several places. By contrast, when a repeated operation is written as a function, any change needs to be made only once, making the script more concise, readable, and less error-prone.\nTo use functions effectively:\n\nDefine a clear purpose.\nEach function should accomplish a single, well-defined task.\nDocument thoroughly.\nDescribe the function’s purpose, inputs, and outputs.\nMake the function configurable.\nUse arguments to control the behavior of the function. Do not comment and uncomment sections of code to change the function – this practice is prone to mistakes.\nGeneralize operations using loops and lists.\nInstead of repeating similar operations for each dataset or variable, store them in a list and write a single function to handle the task. Then, use a loop to apply that function to each element of the list automatically.\n\n\nExample: functions in R\nlibrary(tidyverse)\n\n# ----------------------------\n# Function\n# ----------------------------\n\n# Function to calculate the mean of a numeric vector\n# Args:\n#   x: A numeric vector\n#   na_as_zero: Logical; treat NA as zero? Default FALSE.\n#   na.rm: Logical; remove NA values? Default TRUE.\n# Returns:\n#   The mean of the numeric vector.\n\ncalculate_mean &lt;- function(x, na_as_zero = FALSE, na.rm = TRUE) {\n  \n  # Replace NAs with 0 if specified\n  if (na_as_zero) {\n    x &lt;- replace_na(x, 0)\n  }\n  \n  # Calculate the mean\n  mean_value &lt;- mean(x, na.rm = na.rm)\n  return(mean_value)\n}\n\n# ----------------------------\n# Example data\n# ----------------------------\nmonthly_sales &lt;- list(\n  January   = c(100, 200, 150, NA, 300),\n  February  = c(250, 300, NA, 400),\n  March     = c(200, 180, 220, 210)\n)\n\n# ----------------------------\n# Result\n# ----------------------------\n# Apply with na_as_zero = TRUE\nmean_sales_with_zero &lt;- lapply(monthly_sales, calculate_mean, na_as_zero = TRUE)\n\n# Apply with na_as_zero = FALSE\nmean_sales_without_zero &lt;- lapply(monthly_sales, calculate_mean, na_as_zero = FALSE)\n\n# Print the results\nprint(mean_sales_with_zero)\nprint(mean_sales_without_zero)\nNotes: This example defines a well-documented function, calculate_mean(), that performs a single task—computing a mean with configurable handling of missing values through the na_as_zero and na.rm parameters. A named list of monthly sales is processed using lapply(), which loops over each element and applies the function without duplicating code.",
    "crumbs": [
      "Software Quality",
      "Flexibility"
    ]
  },
  {
    "objectID": "docs/software/flexibility.html#use-parameters",
    "href": "docs/software/flexibility.html#use-parameters",
    "title": "Flexibility",
    "section": "2 Use parameters",
    "text": "2 Use parameters\nInstead of hard-coding values or constants, parameterization involves using variables, configuration files, or external resources to store data that can be adjusted without changing the code structure. This approach enhances the adaptability of the code to varying conditions, and is particularly useful when the parameters are referenced multiple times in the code.\n\nExample: parameterization\n# Parameters\nmin_hp       &lt;- 100   # Minimum horsepower required\nselected_cyl &lt;- 6     # Number of cylinders to filter on\n\n# Filter the data using the parameters\nfiltered_data &lt;- subset(mtcars, hp &gt;= min_hp & cyl == selected_cyl)\n\n# Compute the average MPG\navg_mpg &lt;- mean(filtered_data$mpg)\n\n# Report results using the parameters\ncat(\n  \"Average MPG for cars with at least\", min_hp, \"horsepower\",\n  \"and\", selected_cyl, \"cylinders:\", avg_mpg, \"\\n\"\n)\nNotes: This example uses variables min_hp and selected_cyl to store filter criteria, allowing the behavior of the code to be easily changed without modifying the filtering logic. These parameters control which rows of mtcars are selected, making the code adaptable to different analysis needs. Because the parameters are referenced multiple times, updating their values at the top automatically applies the changes everywhere they are used.",
    "crumbs": [
      "Software Quality",
      "Flexibility"
    ]
  },
  {
    "objectID": "docs/software/flexibility.html#use-relative-file-paths",
    "href": "docs/software/flexibility.html#use-relative-file-paths",
    "title": "Flexibility",
    "section": "3 Use relative file paths",
    "text": "3 Use relative file paths\nAbsolute file paths specify the complete location of a file or folder, beginning at the root of the file system. For example, a dataset might be referenced with an absolute path such as \"L:/my_project/data/input_data.csv\". These paths can create problems for collaborators, since the project may reside in different locations on their machines (for example, on a different drive letter or in another directory).\nInstead of using absolute file paths, relative file paths refer to files or directories in relation to the current working directory. This approach allows code to work on different user machines, accommodates changes in the project folder path, and makes the code easier to archive and share when the project is completed.\n\nExample: reading and writing files using relative paths\nDirectory structure\nmy_project/\n├── data/\n│   ├── input_data.csv\n│   └── output_data.csv\n└── scripts/\n    └── wilson/\n        └── analysis.R\nCode (analysis.R in scripts/wilson/):\n# Set the working directory to the script's location (optional, recommended)\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # Temporarily set WD to where the script is stored\n\n# Move up two levels to the project root ('my_project/')\nsetwd(\"../../\")\n\n# Define relative paths\ninput_path  &lt;- \"data/input_data.csv\"\noutput_path &lt;- \"data/output_data.csv\"\n\n# Read data using a relative path\ndata &lt;- read.csv(input_path)\n\n# Write data using a relative path\nwrite.csv(data, output_path, row.names = FALSE)\nNotes: This example demonstrates how to organize file access using relative paths rather than absolute paths, making the code portable across different machines and directory structures. In this example, the working directory is derived from the script’s location, but other configurations are possible—for instance, prompting the user once for the location of the project folder and using that as a consistent reference. In all cases, relative paths help ensure that the script remains functional even if the project is moved.",
    "crumbs": [
      "Software Quality",
      "Flexibility"
    ]
  },
  {
    "objectID": "docs/version-control/index.html",
    "href": "docs/version-control/index.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. You know how Microsoft Word will periodically save your file, and you can recover previous versions if you really mess up? That is a simple form of version control. Other forms offer far more functionality, including the ability to save and recover versions of an entire project, tools for collaboration, and more.\nSome commonly used version control systems are Git, Mercurial, and Subversion (SVN). Here at RFF, we strongly advise using Git, which we will discuss for the remainder of the section. We also recommend using GitHub to host Git repositories online. Here are some of the benefits of using Git with GitHub for version control:\n\nGrants peace of mind knowing work is stored safely and can be recalled with minimal effort, which gives greater liberty to test out new ideas (even when a computer breaks!)\nFacilitates collaboration by allowing multiple versions to coexist and efficiently borrow code from one another\nEnables efficient review and discussion of code changes before incorporating them into the main branch of the repository\nDocuments when, why, and by whom specific changes were made, which helps with debugging and troubleshooting\nConsolidates and Organizes project code and documentation in the same, easily-accessible, backed-up location.\nPreserves easy web browser access to all versions of code.\nProvides public access to datasets and code under the auspices of a software license\nEmpowers audiences to answer their own questions about the data and methods used in publications, reducing overhead for researchers who would otherwise be given that task\n\nThis section of the guidance will help you get started in making version control a standard part of the project life cycle.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "href": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "title": "Version Control",
    "section": "",
    "text": "Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. You know how Microsoft Word will periodically save your file, and you can recover previous versions if you really mess up? That is a simple form of version control. Other forms offer far more functionality, including the ability to save and recover versions of an entire project, tools for collaboration, and more.\nSome commonly used version control systems are Git, Mercurial, and Subversion (SVN). Here at RFF, we strongly advise using Git, which we will discuss for the remainder of the section. We also recommend using GitHub to host Git repositories online. Here are some of the benefits of using Git with GitHub for version control:\n\nGrants peace of mind knowing work is stored safely and can be recalled with minimal effort, which gives greater liberty to test out new ideas (even when a computer breaks!)\nFacilitates collaboration by allowing multiple versions to coexist and efficiently borrow code from one another\nEnables efficient review and discussion of code changes before incorporating them into the main branch of the repository\nDocuments when, why, and by whom specific changes were made, which helps with debugging and troubleshooting\nConsolidates and Organizes project code and documentation in the same, easily-accessible, backed-up location.\nPreserves easy web browser access to all versions of code.\nProvides public access to datasets and code under the auspices of a software license\nEmpowers audiences to answer their own questions about the data and methods used in publications, reducing overhead for researchers who would otherwise be given that task\n\nThis section of the guidance will help you get started in making version control a standard part of the project life cycle.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#version-control-basics",
    "href": "docs/version-control/index.html#version-control-basics",
    "title": "Version Control",
    "section": "2 Version Control Basics",
    "text": "2 Version Control Basics\nBefore we get started in using Git for version control, let’s cover a few basic principles. Git is a popular version control system, which is software that we can download and use on our computer. Git works by monitoring the changes of the contents of an otherwise ordinary file folder, called a repository. In a Git repository, a user can tell Git which file changes to keep, which to discard, and can label those changes, go back to previous file versions, and much more!\nWhile it is possible to use Git only on your computer without posting it online, it is often advisable to host repositories online so that they are synced in the cloud, making it easy to share them and back them up. There are many websites that can host Git repositories, but the most popular is called GitHub, which is what we recommend using at RFF. (Some others you may come across GitLab and BitBucket) Not only does GitHub host these repositories, but it also provides convenient ways to host documentation, discuss code changes, and report bugs.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html",
    "href": "docs/version-control/tutorial-branching.html",
    "title": "Tutorial: Branching",
    "section": "",
    "text": "Before you begin this tutorial, make sure you have:\nThis is one of the most amazing features of Git. Git allows us to create different working versions, or branches of our repository. This lets me take a snapshot of the repository and try out a new idea without affecting my main branch. Then, once I (and all my collaborators) are satisfied with the changes in my new branch, we can merge it back into the main branch using a Pull Request, which gives tools for reviewing and providing feedback on the proposed changes. This is how the majority of modern software is developed. It allows easy review of the portions of the code that have changed, and gives me confidence that my changes are not messing up the main branch until they are fully developed.\n---\ntitle: Example Git Branching\n---\ngitGraph\n   commit id: \"initial commit\"\n   commit id: \"starting point for the new feature\"\n   branch my-feature\n   checkout main\n   commit id: \"ongoing development of main\"\n   checkout my-feature\n   commit id: \"trying new feature\"\n   commit id: \"perfecting new feature\"\n   checkout main\n   merge my-feature id: \"now it is merged!\"\n   commit id: \"continued development\"\nHere’s an overview of the process:",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html#creating-branches",
    "href": "docs/version-control/tutorial-branching.html#creating-branches",
    "title": "Tutorial: Branching",
    "section": "1 Creating Branches",
    "text": "1 Creating Branches\nYou may have realized that Git creates a branch by default, called main. You could think of the main branch as the trunk of a tree. (If you encounter any older repositories, the default was the master branch, but the default was changed in mid-2020). You may notice that Git Bash has the branch name main written to the right of the prompt, which designates that main is the active branch, or the branch that is currently “checked out”.\n\n\nTo create a new branch, simply enter the command git branch &lt;my-branch-name&gt;. Generally, branch names should be:\n\nconcise yet descriptive. plot-results &gt; plot-all-results-with-numpy\nlowercase and hyphen-separated. plot-results rather than PlotResults or plot_results\nno special characters (numbers, letters, and hyphens only)\nSee more info in the branch naming tip\n\nYou will notice that after running the git branch command, the main branch is still designated as the active branch. To work on the newly created branch, simply use the command git checkout &lt;new-branch-name&gt;.\nAfter running that command, your new branch should be active.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use the following command to create a new branch and check it out in one step:\ngit checkout -b &lt;new-branch-name&gt;\n\n\nNow that your new branch is active, you can safely make changes and commits to this branch without worrying about modifying the main branch. When you push the branch, Git will give you an error saying that there is no “upstream” branch for the branch you are pushing. You can simply run the command it gives you, i.e. git push --set-upstream origin &lt;new-branch-name&gt;, and it should push with no problems.",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html#merging-branches",
    "href": "docs/version-control/tutorial-branching.html#merging-branches",
    "title": "Tutorial: Branching",
    "section": "2 Merging Branches",
    "text": "2 Merging Branches\n\n2.1 Creating a Pull Request\nOnce you have made some commits/changes and are ready to merge the new branch back into the main branch, now it is time to make a Pull Request in GitHub. To do that, navigate to the repository GitHub page, and click the “Pull requests” tab, shown below.\n\n\nNow select the “New pull request” button at the top of the page. We want to select the main branch as the “base” branch, and our new branch as the “compare” branch, as shown below:\n\n\nNow to finish creating the Pull Request, click the “Create pull request” button on the right side of the page. You may enter a title and description for the Pull Request, which is helpful for contributors to know what is contained in the Pull Request, and you could even include instructions for the contributors who you would like to review the Pull Request. Clicking the “Create pull request” button at the bottom will finish creating the request.\n\n\n2.2 Reviewing and Merging a Pull Request\nPull requests provide a great opportunity for collaborators to review one anothers’ code, to ensure accuracy. Code review can be daunting, especially if there are only changes to specific parts of a larger codebase. GitHub makes this process very easy, by identifying lines of code that have changed, and allowing collaborators to comment on them.\nTo review a pull request, the reviewer would navigate to the Pull Request and click on the “files changed” tab. From there, they can see each of the lines added, changed, or deleted, and insert comments on particular lines of code. After reviewing all the files changed, they can complete their review and choose whether or not to approve the pull request. To learn more about reviewing Pull Requests, see the appropriate section of the GitHub documentation.\nIf the reviewer has requested changes to the code based on their review, you can simply make changes, commit, and push to the same branch, and the Pull Request will be updated with those changes. Once the Pull Request has been reviewed and approved, you can go back to the “Conversation” tab of the Pull Request and click the “Merge pull request” button at the bottom of the page, and your branch’s changes will be reflected in the main branch!\n\n\n2.3 Conflicting Branches\nSometimes, when we try merging our new branch into the main branch, the Pull Request indicates that there have been changes made to the main branch that conflict with the changes in the new branch. When this happens, let’s go back to Git Bash and merge any new changes from the main branch into our branch, then resolve any potential conflicts. To do this, let’s pull updates to main and then merge main into our new branch by running the following commands:\ngit checkout main\ngit pull\ngit checkout &lt;new-branch-name&gt;\ngit merge main\nTo handle these conflicts, use the process described in the Handling Conflicts section to resolve them.\n\n\n2.4 Merging Branches without Pull Requests\nMerging with a Pull Request may, at times, feel a bit unwieldy, especially if you are in a hurry or working on a project by yourself. It is still possible to merge into the main branch without making and reviewing a Pull Request. We just advise caution with this method because it does not facilitate easy review, and so could easily introduce mistakes into the main branch. If you are confident that the branch is ready to merge without additional review, in Git Bash, simply enter the following commands:\ngit checkout main\ngit pull\ngit merge &lt;new-branch-name&gt;\nIf any conflicts come up, follow the instructions in the Handling Conflicts section. Once you have dealt with the conflicts (if any), you are done, and can push the main branch, which now has updates from your new branch.",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html",
    "href": "docs/version-control/tutorial-basics.html",
    "title": "Tutorial: Basics",
    "section": "",
    "text": "Before you begin this tutorial, make sure you have:",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#introduce-yourself-to-git",
    "href": "docs/version-control/tutorial-basics.html#introduce-yourself-to-git",
    "title": "Tutorial: Basics",
    "section": "1 Introduce Yourself to Git",
    "text": "1 Introduce Yourself to Git\nFirst, open Git Bash. You can hit the windows key and type “bash” and it should pop up for you to open.\nIn the Git Bash window, you can type all kinds of commands, including Git commands, which are always prependended with “git”, or bash commands that are commonly run in terminals.\nLet’s start with:\ngit --version\nIt should have printed out what version you are running, something like git version 1.47.0.windows.2. Now, let’s introduce ourselves to Git:\ngit config --global user.name 'Your Name Here'\ngit config --global user.email 'your-name-here@rff.org'\ngit config --global --list\nNow, Git will include your name and email when you publish changes.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#creating-a-repository-in-github",
    "href": "docs/version-control/tutorial-basics.html#creating-a-repository-in-github",
    "title": "Tutorial: Basics",
    "section": "2 Creating a Repository in GitHub",
    "text": "2 Creating a Repository in GitHub\nThis section is useful for learning, but you can also skip to the next section on cloning repositories if you already have a repository (i.e. for a project) that you’d like to clone.\n\nTo create a repository hosted on GitHub, first navigate to github.com and click the plus button in the top right corner, then click the “new repository” button.\n\n\n\n\nNext, you may select who you would like to own the repository. It could either be owned by your user account, or an organization you are a member of. This may be something to discuss with your project team. The repository owner will show up in the URL of the repository, in the structure of: https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;. For example, for the E4ST.jl repo created in the e4st-dev organization, the URL is: https://github.com/e4st-dev/E4ST.jl. If in doubt, you can specify yourself as the owner, and change ownership later on. See the Workflows section on repository ownership for more information.\n\n\n\n\nNow you must enter the repository name. We recommend all lowercase names with dashes separating words, like my-repo-name. The only exception is when a programming language’s best practices prescribe a specific repository naming convention. (i.e. julia package repositories are supposed to be camelcase as in MyRepoName.jl)\nNow you must choose whether the repository is to be public or private. Generally, choose private for repositories that will contain sensitive information, and public if the project requires it to be public. It is easy to change from private to public later on, so when in doubt choose private. See the public/private section in Workflows for more information. If you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\nCheck the box to add a README file. This will create a file called README.md located in the repository’s root folder, where you can add basic documentation for the repository.\nChoose a .gitignore template from the dropdown menu. A .gitignore file specifies certain file types that Git will not track the changes of, by default. For example, it is best to ignore an auto-created config file made by R studio that is user-specific. Generally it is a good idea to select the .gitignore template for the programming language you will be using. Please see the section on .gitignore files for more information.\nClick the “Create Repository” button!! This should take you to the home page of your new repository.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#cloning-your-first-repository-with-git",
    "href": "docs/version-control/tutorial-basics.html#cloning-your-first-repository-with-git",
    "title": "Tutorial: Basics",
    "section": "3 Cloning Your First Repository with Git",
    "text": "3 Cloning Your First Repository with Git\nNow that we’ve made a remote repository, let’s get it copied onto our computer. Copying a remote Git repository is called cloning. This process will work the same way for any existing repository, including the one that we made in the steps above.\nFirst, it’s important to choose a good file location to store the git repository. While it’s not required, many people like to have a single folder to store all of their Git repositories. An alternative would be to store the Git repository in an associated project folder.\nGit Bash has a working directory, which is the location that it is operating in. To see what the current working directory is, we can enter the pwd command, which stands for primary working directory.\npwd\nNow lets try changing the working directory with the cd command, to whatever directory you would like to store your Git repository in. For example:\ncd 'C:/Users/&lt;my-user-name&gt;/OneDrive - rff/repos'\n\n\n\n\n\n\nNote\n\n\n\nIt is only necessary to use quotes for filenames in Git Bash if there is a space in the path, as in the example above.\n\n\nNow, in an internet browser, navigate to the home page of the repository you wish to clone.\nClick the button labeled &lt; &gt; Code, then copy the URL to clipboard by clicking the logo with intersecting squares.\nIn the Git bash window, type git clone then paste in the repository URL from your clipboard by right-clicking. Altogether this would look like:\ngit clone https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;\nIf the repository is private, Git Bash may prompt you for your GitHub credentials. After entering them, you should be left with the cloned repository located in the working directory!\n\n\n\n\n\n\nNote\n\n\n\nIf you have set up SSH keys as in the optional setup section, you can copy the SSH address for the repository. To do this, after clicking the &lt; &gt; Code button, select SSH before copying the URL to your clipboard. If properly set up, when cloning from an SSH URL, you should not be prompted for GitHub credentials, even for a private repository.\n\n\nFinally, you can navigate into that newly created repo using the cd command:\ncd &lt;repository-name&gt;",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#making-and-publishing-file-changes",
    "href": "docs/version-control/tutorial-basics.html#making-and-publishing-file-changes",
    "title": "Tutorial: Basics",
    "section": "4 Making and Publishing File Changes",
    "text": "4 Making and Publishing File Changes\nNow that we have a Git repository on our computer, let’s explore the process of making file changes and saving versions. To give a conceptual outline, there are four steps.\n\nPull changes from others. This is to ensure we are working on the latest version of the repository. git pull\nMake changes to files. You can do this with any text editor.\nStage changes. This tells Git which changes you would like to select for the next version. git add\nCommit staged changes. This publishes any staged changes as a new version. git commit\nPush commit(s) to the remote/online repository. git push\n\nLet’s walk through that process together. In Git Bash, type git status to show the status of the repository. It should give you a short report about which branch you are on (more about branches later), whether your local copy is up to date with the remote copy, and any file changes you’ve made.\nOn branch main                                 # this is the default branch\nYour branch is up to date with 'origin/main'.  # the local version matches the version in GitHub\n\nnothing to commit, working tree clean          # we haven't made file changes\nAt this point, it is ALWAYS a good idea to run the git pull command to make sure we are up to date with the latest version. This command first checks to see if the remote repository version (the version stored in GitHub) has had any commits since the last time we pushed or pulled. If there are any, it will download them incorporating those changes into your working version.\nNow, let’s open up the README file and make some changes.\nYou can either navigate to the file in Windows File Explorer or type notepad README.md to open the README file in the notepad text editor. Add a few lines to your README, then save and close the file. In the future, you can use whatever editor you prefer, such as RStudio, MATLAB, Visual Studio Code, etc. After making that change, let’s type our git status command into Git Bash again.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nThis message is telling us that there are some file changes that have not yet been staged for commit. Let’s stage the changes from the README file with the command git add README.md.\n\n\n\n\n\n\nNote\n\n\n\nYou can use the command git add -p in order to interactively select which files, and even sections of files you would like to stage for commit. You can also use the command git add * to add all of the files that have been changed since the previous commit.\n\n\nNow let’s check the status once more:\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   README.md\nNow, our README.md file is under the heading Changes to be committed, so it is ready to be published. Let’s publish with the command git commit. It should open up notepad and prompt you to enter a commit message. In the first line, enter a concise commit message describing the changes that you are publishing, more info on that here. Save and close the file, and you have successfully authored your first commit!\n\nIf you find typing your commit message in notepad arduous, we have the command for you!\ngit commit -m \"Simply type your messsage here\"\nOR if you want to be even more lazy, see the section for the commit alias\n\nThe last step is to push commits to the online repository. At this point, git status indicates that the local version of the repository is 1 commit ahead of the online version. To send the commits to the online repository, we will simply enter the command git push! Now, if you navigate to the online version of the repository in GitHub, you should be able to see the changes you made. Congratulations!!",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#handling-conflicts",
    "href": "docs/version-control/tutorial-basics.html#handling-conflicts",
    "title": "Tutorial: Basics",
    "section": "5 Handling Conflicts",
    "text": "5 Handling Conflicts\nHave you ever been working on a Word document with another person, and maybe your computer gets disconnected, and you accidentally both edit the same paragraph? Usually Word will get smart and show both sets of changes and allow you to choose which one you would like. Sometimes, the same thing happens in Git/GitHub.\nSay, for example, my collaborator pushes a commit to our project repository without me realizing. In an ideal world, I would run the git pull command to make sure I have that commit incorporated into my local copy. However, maybe I forgot to pull, or my collaborator pushed that commit as I was already making file changes. Then, I commit my changes and push them and am greeted with the following ugly message:\n$ git push\nTo github.com:Ethan-Russell/MyTestRepo.jl.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'github.com:Ethan-Russell/MyTestRepo.jl.git'\nhint: Updates were rejected because the remote contains work that you do not\nhint: have locally. This is usually caused by another repository pushing to\nhint: the same ref. If you want to integrate the remote changes, use\nhint: 'git pull' before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nDon’t panic! Git is built for this. This message is Git telling us that we can’t push because of the other commit, and it also tells us that the solution is to git pull before pushing again. When we run git pull, Git will try to automatically merge the changes for us. Ideally, if changes are made in separate sections of the file (or different files altogether), Git will be able to merge everything, and you can simply git push again to push your commit(s). However, if both versions have changed the same file, Git will flag it as a conflict and it will enter “MERGING” mode (denoted in git bash to the right of the prompt). This is the type of message Git will print if there is a conflict:\n$ git pull\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nUnpacking objects: 100% (3/3), 947 bytes | 55.00 KiB/s, done.\nFrom github.com:Ethan-Russell/MyTestRepo.jl\n   6449e68..a3b9c32  main       -&gt; origin/main\nAuto-merging README.md\nCONFLICT (content): Merge conflict in README.md\nAutomatic merge failed; fix conflicts and then commit the result.\nFortunately, Git has identified which file(s) have conflicts: in this case, only the README.md file has a conflict. So let’s open the file where there is a conflict. Git identifies which lines have conflicts by adding some helpful syntax, which we can search for to find the conflict. You can do a search for “====” and that will take you to the conflict:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# My Test Repo\n=======\n# MyTestRepo\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a3b9c323e36238db4a26a91addc5c699add04b6a\nGit will list the local version first, right after printing &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, followed by a line of =======, then the version that was pulled in from the remote repository, then &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a unique identifier for the commit in the remote repository. Translating the above chunk of text, it looks like someone else had added the title MyTestRepo to the README.md file, and I added the title My Test Repo. All we need to do here is:\n\nSelect which version of the changes we want (or we could edit it to be a combination of the two!)\ndelete the other version, as well as all of the notation that Git has added (&lt;&lt;&lt;, ===, &gt;&gt;&gt;)\nSave and commit the resulting file.\n\nIn the example above, I would change that portion of the file to be:\n# MyTestRepoFinalName\nAfter adding and committing that change, we can then safely run git push.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  }
]