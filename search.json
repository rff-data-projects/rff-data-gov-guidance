[
  {
    "objectID": "docs/software/index.html",
    "href": "docs/software/index.html",
    "title": "Software Quality",
    "section": "",
    "text": "Content coming soon.\nExisting style guide resources:\n\nR: Tidyverse Style Guide by Hadley Wickham\nPython: Google Python Style Guide\nStata: Suggestions on Stata programming style by Nicholas Fox\nOther languages: Google style guides for other languages",
    "crumbs": [
      "Software Quality"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html",
    "href": "docs/data-management/archival.html",
    "title": "Archival & Disposal",
    "section": "",
    "text": "Archiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:\n\nensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\n\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see sharing API data.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\n\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\n\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#archival",
    "href": "docs/data-management/archival.html#archival",
    "title": "Archival & Disposal",
    "section": "",
    "text": "Archiving refers to the secure, long-term storage of data in its final state, upon project completion. Archiving often involves moving data to dedicated storage solutions designed for long-term retention, like archive servers or cloud storage. This is sometimes referred to as “cold storage.”\nArchival is important because it:\n\nensures long-term and secure storage to projects for reproducibility and reuse,\nimproves organization, accessibility, and usability of both active and completed project files, and\nreleases computational resources for active projects, reducing energy consumption and storage costs.\n\n\n\n\n\n\n\nNote\n\n\n\nArchival takes place when projects are complete. To preserve the state of code and data at major milestones, such as journal article publication, see Publication.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt RFF, archived files can still be accessed, read, and copied to active folders.\n\n\nWhen RFF data projects are archived, they are migrated to a new storage location, but are still configured to be accessible to specified team members. The folder can be accessed in a way similar to the L drive, except that the files will be read-only to prevent accidental deletion or modification (they can still be copied or fully restored to the L drive).\n\n\n\nDelete obsolete and intermediate files.\n\nEnsure that irrelevant or outdated files are removed, so that only files necessary for reproduction or understanding are retained.\nIn general, intermediate data generated by code does not need to be archived, since it can be easily re-created from raw data and code. However, use discretion: in some cases, intermediate data that are likely to be used again and are time-consuming to re-create should be retained.\n\nThe files to be retained may vary by project, but in general should include:\n\nSource (raw) data\n\nWhen possible, source data should be preserved without modification, as external data sources may be modified or become unavailable.\nHowever, for certain reliable data sources, citation and documentation may be sufficient (make sure to include the access date and dataset version).\nIf data were accessed via an API, see sharing API data.\n\nFinal analysis data\nResults and visualizations\nCode\nDocumentation\n\none project-level README\nall raw data README files\nany metadata files\n\n\n\nThis is generalized guidance. For additional guidance choosing which files to archive, see Decide what data to preserve.\n\n\n\nEnsure the project-level README file, raw data README files, and any metadata files are up to date.\n\n\n\nContact IT at IThelp@rff.org to arrange and configure archival storage of the folder. Include the following with the email:\n\nProject-level README file\nList of researchers that should retain folder access\nApproximate folder size (e.g., 5 GB)\nThe nature of sensitive/proprietary datasets",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/data-management/archival.html#disposal",
    "href": "docs/data-management/archival.html#disposal",
    "title": "Archival & Disposal",
    "section": "2 Disposal",
    "text": "2 Disposal\nSome data may need to be deleted to protect sensitive information or comply with regulations, data agreements, or funder requirements. This is often referred to as data disposition. If any of these requirements applies to a project, follow these best practices when deleting data:\n\nVerify Requirements: Confirm funder agreements and legal obligations regarding data retention and deletion.\nSource Deletion: Confirm with IT that the files were fully deleted in accordance with requirements (e.g., backup files).\nDocumentation: Record when and how data was deleted.\n\nThese practices should apply to data stored the RFF network, OneDrive, or Microsoft Teams.",
    "crumbs": [
      "Data Management",
      "Archival & Disposal"
    ]
  },
  {
    "objectID": "docs/data-management/publication.html",
    "href": "docs/data-management/publication.html",
    "title": "Publication",
    "section": "",
    "text": "Publication is making code/data available to the broader community, often through formal dissemination channels such as data repositories, journal articles, or public databases. Publication ensures that data is discoverable and can be accessed by other researchers, stakeholders, or the public. Documentation and metadata are included to facilitate understanding and reuse. Publication may also involve adherence to specific standards and best practices to enhance the visibility and impact of the data.\nWhile RFF does not mandate the publication of code and data, it is highly encouraged.\nIncreasingly, journals require code and data to be submitted along with the article. In addition, many funders and stakeholders value open source software and data availability. Planning for this in the early stages of a project facilitates reproducibility, access, and the ability of others to use and cite your work."
  },
  {
    "objectID": "docs/data-management/publication.html#licensing",
    "href": "docs/data-management/publication.html#licensing",
    "title": "Publication",
    "section": "1 Licensing",
    "text": "1 Licensing\nA well-chosen license clarifies permissions, prevents misunderstandings, and encourages responsible use. The next section provides guidance on selecting and attaching appropriate licenses to ensure your data and code remain accessible and properly credited.\n\n\n\n\n\n\nNote\n\n\n\nData and code products are licensed separately from RFF publication products. All work on the RFF.org and Resources.org websites (working papers, reports, issue briefs, explainers, Common Resources blog posts, Resources magazine articles, Resources Radio podcast episodes, graphs, charts, photographs, audio, and video) are listed under the Deed - Attribution-NonCommercial-NoDerivatives 4.0 International - Creative Commons license. This Creative Commons license is not suitable for either software or data.\n\n\n\n1.1 Verify Data and Code Rights and Ownership\nBefore proceeding with choosing licenses, confirm that your team owns IP rights to the work produced and has full discretion over licensing the project’s data and code—without restrictions from funders, institutional policies, or legal/data agreements.\nIn some cases the research team may have joint IP ownership with partners or funders. In this case, licensing options must be agreed upon by both parties.\n\n\n1.2 Choose Appropriate Licenses\n\nCommon Licenses\nFor software and data, there are three main license suites common in the academic space: MIT, GNU GPU, and ODC. MIT and GNU GPU are separate software licenses, while ODC has two commonly-used data licenses. All four are described below.\n\nSoftware licenses\n\nMIT: The more permissive and flexible. Users, including commercial entities, can view, use, modify, and distribute the work freely. Allows for commercial use and enclosure. Attribution is required.\n\n(Enclosure is the process of restricting access, usage, or modification of software or its source code through the imposition of licensing terms. It involves the application of intellectual property rights (such as copyright, patents, or trade secrets) to create boundaries around software, typically to protect the developer’s or organization’s control over its distribution and use.)\n(Attribution refers to crediting to the original creator, author, or source as specified in the licensing terms. Attribution ensures recognition of the intellectual property and efforts of the creators while allowing others to use, modify, or distribute the licensed material.)\n\nGNU General Public Use (GPU): Users, including commercial entities, can view, use, modify, and distribute the work freely. However, it carries copyleft, so all distributions must be released under the same GNU GPU license, ensuring open access (viral). Attribution is required.\n\n(Copyleft is a concept in software (code/script) licensing that ensures any derivative works (modifications or adaptations) of a particular work remain subject to the same licensing terms as the original work. For example, if a developer modifies a program released under the GNU General Public License (GPL), they are required to distribute their modified version under the same GPL license. This means they must also make the source code available and allow others to freely use, modify, and redistribute it under the same terms. Copyleft aims to preserve software freedom by preventing proprietary restrictions from being reintroduced into derivative works.)\n(Viral: Licenses like the GNU General Public License (GPL) and the Open Database License (ODbL) are often described as “viral” because they require derivative works to comply with the same licensing terms as the original work. This characteristic ensures that the freedoms granted by the license (such as the ability to use, modify, and share) are preserved in all subsequent versions or derivatives, but it also imposes specific obligations on those who modify or build upon the original.)\n\n\nData licenses by Open Data Commons (ODC)\n\nODC-By: The more permissive and flexible. Users, including commercial entities, can view, use, modify, and distribute the work freely. Allows for commercial use and enclosure. Attribution is required.\nODCbL: Users, including commercial entities, can view, use, modify, and distribute the work freely. However, it carries share-alike, so all distributions must be released under the same ODCbL license, ensuring open access (viral). Attribution is required.\n\n(Share-alike, similar to copyleft but applied to datasets, requires that any derivative works of content (adaptations or modifications) are licensed under the same or a compatible license. This ensures that future users of the adapted work can use it under similar terms.)\n\n\n\nRecommendations for selecting licenses\n\nAdhere to any product licensing requirements from the funding agreement and the source data/software.\nConsider whether the common academic licenses that are more permissive and flexible (MIT for software, ODC-By for data) are appropriate. If so, use those. These licenses are suitable for most applications.\nIf project authors/partners and RFF would prefer that all derivative works remain open source and accessible, use GNU GPLv3 plus citation request for software and ODbL for data. Caution that these licenses, because they are more restrictive, are more complex, so review license terms carefully.\nIf there are other requirements or the team would like to review a broader range of software licenses and their specifications, visit ChooseALicense.\n\n\n\n\n1.3 Create and customize license files\n\nOnce the license has been selected, download or create a .txt license file from the license website. Save the file as LICENSE.txt in the project folder and/or repository.\nReview the license terms and modify where necessary (most open-source and open-data licenses are designed to be used as-is, but others may require you to fill in specific details, such as name or organization).\nIf adding additional terms, include them in a separate README or license appendix to avoid conflicting with the main license."
  },
  {
    "objectID": "docs/data-management/publication.html#publishing",
    "href": "docs/data-management/publication.html#publishing",
    "title": "Publication",
    "section": "2 Publishing",
    "text": "2 Publishing\nFor publishing both code and data, ensure that the project-level README is up to date.\n\n2.1 Code\nWhen your project is ready to publish code—whether alongside a journal article, report, or other research output—there are a few options for how to share it, especially if your code is hosted on GitHub.\n\nNote on journal requirements\nSome journals require authors to:\n\nMake the codebase publicly accessible\nInclude a link to the GitHub code repository in the manuscript\nReference the codebase DOI in the manuscript\n\n\n\nSharing code via the RFF GitHub Organization\nYou can link your GitHub repository to the RFF GitHub organization in one of two ways:\n\nOption 1: Forking into the RFF GitHub Organization\n\nThis approach is suitable for most situations and provides significant flexibility.\nA fork creates a copy of your repository under the RFF organization account.\nThis snapshot can be tagged to align with a publication version (e.g., v1.0-paper-release).\nThe fork can be updated to stay in sync with the original repo, or kept as a fixed version.\nInclude a note in the README linking the snapshot to the relevant publication.\nNote that while the RFF GitHub organization takes control of the new fork, the original repository owner retains administrative control of the upstream repository.\n\n\n\nOption 2: Transferring Ownership to the RFF Organization\nFor institutional or long-term projects, it may be most effective to transfer ownership and administrative privileges of the repository to the GitHub organization. This simplifies management and ensures continuity, especially during researcher transitions, by centralizing control in the organization rather than with an individual. Researchers can retain collaborator-level permissions, allowing them to push code, create branches, and submit pull requests.\n\nThe repository is permanently transferred to the RFF GitHub organization.\nAll existing collaborators retain their roles.\nThe repository URL will change, but redirects will be preserved by GitHub.\nOrganization owners (e.g., staff managing RFF’s GitHub) gain administrative control.\n\n\n\n\nRecommended Publishing Workflow\nIf your repository is already in the RFF GitHub organization:\n\nMake the repository public once you’re ready to publish if it isn’t already.\nTag the publication version (e.g., v1.0-publication-name) as described here.\nArchive the release on Zenodo if you need a DOI.\n\nIf your repository is under a personal GitHub account:\n\nDecide whether you will continue development:\n\n\nIf you are finished working on the project and no longer want administrative duties, consider transferring the repository to the RFF organizational GitHub.\nIf you intend to continue developing the code for purposes unrelated to RFF, request that the RFF GitHub account fork your repository to retain a version tied to the publication.\n\n\nContact the RFF GitHub organization admins at DGWG@rff.org to initiate the fork or transfer and add a tag associated with the publication (note: tags do not transfer automatically when a repo is forked).\n(Optional) Archive the tagged version on Zenodo and link the DOI in your README.\n\n\n\n\n2.2 Data\n\nData-level documentation (metadata)\nIt is recommended to attach metadata files to published datasets. Metadata (“data about data”) documents the “who, what, when, where, how, and why” of a data resource. Metadata not only allows users (your future self included) to understand and use datasets, but also facilitates search and retrieval of the data when deposited in a data repository.\nBelow are the key components of metadata. They can be stored in a simple text or markdown file.\n\nTitle: Descriptive name of the dataset.\nDOI number: Associated with the final publication, dataset, or both\nAbstract: Summary of the dataset’s content\nKeywords: Relevant terms for search and discovery\nTemporal Extent: Time period covered by the data\nData Format: File format\nData Source(s): Origin of the data\nAccuracy and Precision: Information about data quality\nAccess Constraints: Restrictions on data use\nAttribute / field definitions: Define all abbreviations and coded entries\nAdditional geospatial metadata components, if applicable\nSpatial Extent: Geographic coverage (bounding coordinates)\nProjection Information: Coordinate system details\n\nIn some contexts, generating machine-readable metadata that adheres to certain disciplinary standards is useful. There are various metadata formats and standards for specific disciplines. Additional guidance and resources for generating machine-readable metadata are here: Metadata and describing data – Cornell Data Services.\n\n\nUploading data to Zenodo\nZenodo is an online repository for sharing research data, software, and other scientific outputs. It has a broad disciplinary focus and is safe, citeable (every upload is assigned a DOI), compatible with GitHub, and free for up to 50GB of storage.\n\nStep 1: Prepare the research data\nBefore uploading, ensure that:\n\nThe data is well-organized (e.g., structured folders, clear file names).\nMetadata file are prepared for each data file or sets of data files, including dataset title, description, author names, and relevant keywords.\nA license is attached.\nAny sensitive or restricted data is removed or anonymized (if applicable).\nThe project-level README is up to date.\n\nFor guidance on choosing which files to publish and how to handle API-accessed data, see Finalize data organization.\n\n\nStep 2: Create a Zenodo account & access the upload dashboard\n\nGo to Zenodo and sign in (or create an account). Note that you can create an account using your GitHub profile.\nClick the “New Upload” button on the Zenodo dashboard.\n\n\n\nStep 3: Upload the data files, fill in metadata, set access\n\nUpload data, metadata, and the project-level README file.\nEnter metadata information in applicable fields (contributors, associated journal article or conference presentation, etc.)\nInclude a link to the GitHub code repository.\nChoose an Access Level\nOpen Access: Publicly available for anyone.\nEmbargoed: Set a release date if the data must remain private for a certain period.\nRestricted Access: Requires users to request access.\n\n\n\nStep 4: Publish & Get a DOI\n\nReview all details and make any necessary edits.\nClick “Publish” to finalize the upload.\nZenodo will generate a DOI — use this when citing the dataset in publications.\n\n\n\nVersioning & Updates\nIf the dataset needs to be udpated:\n\nUse the “New Version” option in Zenodo instead of creating a separate upload.\nZenodo will link versions together and maintain persistent DOIs."
  },
  {
    "objectID": "docs/data-management/index.html",
    "href": "docs/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Data management encompasses the methods used to collect, store, organize, and use data. Please review Foundations for an overview of the role of data management in the research process.",
    "crumbs": [
      "Data Management"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html",
    "href": "docs/data-management/storage-options.html",
    "title": "Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#summary",
    "href": "docs/data-management/storage-options.html#summary",
    "title": "Storage Options",
    "section": "",
    "text": "Store data and code for RFF projects in a project-specific L:/ drive folder.\n\nCreate and configure your new project folder\nOrganize your project folder to enable version control\n\nUse GitHub to share and version control code.\nIf working with external collaborators:\n\nUse OneDrive to share select data. Only store data in OneDrive that is necessary for collaboration.\nUse GitHub to share and review code.\nFor small datasets, GitHub may be used for both data and code storage.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#internal-rff-projects",
    "href": "docs/data-management/storage-options.html#internal-rff-projects",
    "title": "Storage Options",
    "section": "2 Internal RFF Projects",
    "text": "2 Internal RFF Projects\n\n2.1 Data: L:/ drive\nThe L:/ drive is the primary location for storing project data and code. All internal projects should have a designated L:/ drive folder—even when working with external collaborators.\nThe L:/ drive is optimized for data-intensive workflows. It offers:\n\nHigh storage capacity\nRegular backups\nEnhanced security compared to personal drives\n\nAccess to the L:/ drive requires an RFF network account.\nSee instructions for setting up a project L:/ drive folder.\n\n\n2.2 Code: L:/ drive and GitHub\nProject code (e.g., .R, .py, .do files) should be stored in the project’s L:/ drive folder alongside data, and should be version controlled within a Git repository synced to GitHub.\nGitHub’s distributed version control system allows team members to:\n\nWork on scripts independently without disrupting others\nTrack changes with clear commit messages\nReconcile and sync updates across folders\n\nFor example, you can revise a script locally and commit changes, with documentation, when they’re ready—without interfering with your colleague’s workflow.\nTo prevent users from simultaneously editing scripts stored within a shared folder, and to support use of version control, each team member who will be viewing, running, or editing code should have their own personal folder containing a clone of the project’s GitHub repository . Specific suggestions for how to organize your project folder are in the Organization section.\nSee Version Control for setup instructions.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#collab",
    "href": "docs/data-management/storage-options.html#collab",
    "title": "Storage Options",
    "section": "3 Solutions for collaborative projects",
    "text": "3 Solutions for collaborative projects\n\n3.1 Sharing code\nThe GitHub repository method (see Organization section) for storing and sharing scripts is ideal for collaborating with people who do not have access to the RFF network and L:/ drive. If they are made collaborators on the GitHub repository, they’re able to clone a copy of the repository codebase to their local folder directly from the web browser.\n\n\n3.2 Sharing and accessing data\nIn addition to storing data and code on the L:/ drive project folder, it may be necessary to store, access, and share files in other ways when collaborating with team members without RFF network access. In that case, there are a few options.\n\nAccessing raw data via Application Program Interfaces (APIs)\nAPIs (Application Programming Interfaces) allow programs to access data directly from remote servers. Many public datasets — such as the US Census or USDA NASS — can be queried through APIs, enabling efficient workflows without needing to download and store large raw files. If APIs are not available, similar workflows can often be built by downloading data directly from URLs using tools like curl.\nBenefits of Using APIs\n\nReduced storage: APIs allow you to retrieve only the data you need, rather than storing large or comprehensive datasets locally.\nUp-to-date data: API calls can return the most recent available data each time code is run.\nReproducibility: API-based workflows can support reproducible research when combined with version control and well-documented queries.\n\nConsiderations and Caveats\n\nStability and longevity: APIs can change or become deprecated. Keep this in mind when building workflows that rely on long-term access.\nAccessibility: Not all APIs have well-documented libraries in your programming language of choice. Some require constructing URLs manually or navigating limited documentation.\nReproducibility and archiving API-sourced data: To support reproducibility, consider the following approaches when publishing or archiving projects that rely on API-sourced data. Choose the strategy that best balances reproducibility, storage constraints, and project scope.\n\nDownload and archive locally: Save a copy of the queried dataset in your project directory at the time of analysis or publication.\nDeposit in a repository: Archive the data on platforms like Zenodo using tools such as zen4R (R) or zenodo-client (python).\nDocument query details: If data is too large and/or static, document the API source, parameters, and access date to support partial reproducibility.\n\n\n\n\nOneDrive\nOneDrive can be used to share data with external collaborators, but the L:/ drive should remain the primary storage location due to better access from RFF lab computers, greater computing capacity, more storage, fewer sync issues, and improved data security.\nStorage considerations for OneDrive:\n\nProject folders start with 5 TB of storage (expandable).\nFiles over 10 GB may fail to sync — OneDrive is not ideal for large files.\nShare only essential files (e.g., inputs/outputs — not intermediate files).\nMirror (replicate) the folder structure between L:/ and OneDrive to ensure clarity and consistency.\nSensitive data should be handled carefully; set access permissions appropriately.\n\nHow to set up collaboration in OneDrive.\n\n\nGitHub\nWhile we recommend only using GitHub to version control scripts, figures and tables intended for publication, and code documentation, experienced users can also use it to share and version control other small data files (well below 100 MB). This is also discussed in the Organization section.\nConsider security, sensitivity, and license restrictions when hosting data on GitHub.\nFiles larger than 100 MB should be shared using other tools.\n\n\nOther cloud storage options for large data\nAlternatives to OneDrive, such as Azure, Google Bucket, AWS S3, or Dropbox may be well suited to your project, especially for short-term storage and file transfer. However, note that these storage options may incur additional costs, depending on data size (even the “free tiers” of these services may incur pay-as-you-go costs). For Azure setups, contact IT at IThelp@rff.org.\n\n\nArcGIS Online for sharing and exploring spatial data\nArcGIS Online is a cloud-based browser platform that allows users to upload, host, and share datasets (both geospatial and tabular). In order to access the data and exploratory mapping interface, users need an ArcGIS Online account. Online accounts cost $100 per user and data storage costs vary by data size. Contact RFF’s GIS Coordinator at Thompson@rff.org for more information.\n\n\nEnabling external collaborator access to the L:/ drive\nWhile not recommended, it is possible to enable access to the L:/ drive for non-RFF staff. See Server Access for Non-Employees. Temporary accounts can be requested by contacting IT at IThelp@rff.org.",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/storage-options.html#L-drive-storage-request",
    "href": "docs/data-management/storage-options.html#L-drive-storage-request",
    "title": "Storage Options",
    "section": "4 L:/ drive storage request",
    "text": "4 L:/ drive storage request\nAt the start of a project (or upon major changes to specifications, such as timeline or disk space), email IThelp@rff.org with the following information. Example answers are provided.\n\n\n\n\n\n\n\nField\nResponse\n\n\n\n\nProject name\nLUFA-AgSubsidies\n\n\nStorage location\nL drive\n\n\nFolder name\nL:/Project-lufa-agsubsidies\n\n\nShort description\nThis project analyzes how variations in agricultural subsidy structures across U.S. counties influence land-use change\n\n\nPrincipal Investigator(s)\nOtgonbayar Aquila and Léonce Dominique\n\n\nRFF collaborators\nEvelyn Loren\n\n\nExternal collaborators\nSeveral University of Eastern Colorado collaborators but they will not need access to the project folder\n\n\nData types\nR, Stata, GIS datasets\n\n\nSize requested\n80 GB\n\n\nEstimated max. size\n150GB\n\n\nArchival date\nDecember 2028\n\n\nData agreement or sensitive data security considerations\nProprietary data will be in raw data folder and will need to be made read-only",
    "crumbs": [
      "Data Management",
      "Storage Options"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html",
    "href": "docs/data-management/file-formats.html",
    "title": "Data & File Types",
    "section": "",
    "text": "In coding environments and other software, data are classified into specific “types”, which determine how the values are interpreted and stored in memory. Recognizing the data types of variables in a dataset is essential for ensuring analytical accuracy, understanding precision, and avoiding errors during data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData type\nDefinition\nTypical memory\nPrecision\nCommon Operations\nExamples\n\n\n\n\nCharacter (string/text)\ntext or string values\n~1 byte per character\nNot applicable (stores characters, not numeric values)\nConcatenation, substring, pattern matching\n\"Hello, world!\",'#23',\"'Why?', they asked.\"\n\n\nInteger\nWhole numbers (no decimal point)\n4 bytes (32-bit)\nExact for values within allowed range\nArithemtic, comparisons, indexing\n0, 42, -6,2e+30 (scientific notation)\n\n\nFloating point\nNumbers with decimals (real numbers, including fractions)\n4 bytes (float), 8 bytes (double)\nApproximate - may introduce small rounding errors\nArithmetic, scientific calculations\n-54.3,3.14159\n\n\nBoolean / Logical\nBinary values\nTypically 1 byte\nExact\nLogical operations (e.g., AND, OR, NOT)\nTRUE, FALSE,0, 1\n\n\nDate/time\nCalendar dates and/or clock times\nVaries by system (~8+ bytes)\nHigh precision in supported range\nOften require specialized functions; format inconsistencies can cause import errors\n2025-05-29, 1990-01-24 14:30:00\n\n\n\n\n\n\n\nFloating point rounding errors Floating point types represent real numbers, including fractions. However, not all decimal values can be represented exactly in binary. As a result, small rounding errors may occur during arithmetic operations. These are usually minor but can accumulate in complex calculations. Example: 0.1 + 0.2 might result in 0.30000000000000004 due to binary representation limits.\nDate formats\n\nUse the conventional ISO 8601 format (YYYY-MM-DD)\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as DD/MM/YYYY but is mistakenly interpreted as MM/DD/YYYY during conversion to YYYY-MM-DD, the resulting date will be incorrect.\n\nChanging data types Changing a variable’s data type (e.g., from float to integer or from number to string) can alter how the data is stored and interpreted. Even if the displayed values seem the same (e.g., 5, 5.0, or \"5\"), the underlying representation differs—and in some cases (like converting from float to integer), it may result in loss of precision or information. Always check whether a conversion is appropriate for your analysis.\nfloat_value = 3.7\nint_value = int(float_value)  # Becomes 3\nMemory efficiency Choosing efficient data types can improve performance for large datasets. For example, storing whole numbers (like counts of people) as double-precision floats instead of integers can use more memory than necessary. While both floats and integers can be 4 or 8 bytes depending on the system, using types with more precision than needed (e.g., doubles for integers) can lead to unnecessary memory overhead.",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-types",
    "href": "docs/data-management/file-formats.html#data-types",
    "title": "Data & File Types",
    "section": "",
    "text": "In coding environments and other software, data are classified into specific “types”, which determine how the values are interpreted and stored in memory. Recognizing the data types of variables in a dataset is essential for ensuring analytical accuracy, understanding precision, and avoiding errors during data processing.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData type\nDefinition\nTypical memory\nPrecision\nCommon Operations\nExamples\n\n\n\n\nCharacter (string/text)\ntext or string values\n~1 byte per character\nNot applicable (stores characters, not numeric values)\nConcatenation, substring, pattern matching\n\"Hello, world!\",'#23',\"'Why?', they asked.\"\n\n\nInteger\nWhole numbers (no decimal point)\n4 bytes (32-bit)\nExact for values within allowed range\nArithemtic, comparisons, indexing\n0, 42, -6,2e+30 (scientific notation)\n\n\nFloating point\nNumbers with decimals (real numbers, including fractions)\n4 bytes (float), 8 bytes (double)\nApproximate - may introduce small rounding errors\nArithmetic, scientific calculations\n-54.3,3.14159\n\n\nBoolean / Logical\nBinary values\nTypically 1 byte\nExact\nLogical operations (e.g., AND, OR, NOT)\nTRUE, FALSE,0, 1\n\n\nDate/time\nCalendar dates and/or clock times\nVaries by system (~8+ bytes)\nHigh precision in supported range\nOften require specialized functions; format inconsistencies can cause import errors\n2025-05-29, 1990-01-24 14:30:00\n\n\n\n\n\n\n\nFloating point rounding errors Floating point types represent real numbers, including fractions. However, not all decimal values can be represented exactly in binary. As a result, small rounding errors may occur during arithmetic operations. These are usually minor but can accumulate in complex calculations. Example: 0.1 + 0.2 might result in 0.30000000000000004 due to binary representation limits.\nDate formats\n\nUse the conventional ISO 8601 format (YYYY-MM-DD)\nEnsure that date formats are consistent within columns and are correctly interpreted when converting to a standard format. For example, if a date is formatted as DD/MM/YYYY but is mistakenly interpreted as MM/DD/YYYY during conversion to YYYY-MM-DD, the resulting date will be incorrect.\n\nChanging data types Changing a variable’s data type (e.g., from float to integer or from number to string) can alter how the data is stored and interpreted. Even if the displayed values seem the same (e.g., 5, 5.0, or \"5\"), the underlying representation differs—and in some cases (like converting from float to integer), it may result in loss of precision or information. Always check whether a conversion is appropriate for your analysis.\nfloat_value = 3.7\nint_value = int(float_value)  # Becomes 3\nMemory efficiency Choosing efficient data types can improve performance for large datasets. For example, storing whole numbers (like counts of people) as double-precision floats instead of integers can use more memory than necessary. While both floats and integers can be 4 or 8 bytes depending on the system, using types with more precision than needed (e.g., doubles for integers) can lead to unnecessary memory overhead.",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#data-file-formats",
    "href": "docs/data-management/file-formats.html#data-file-formats",
    "title": "Data & File Types",
    "section": "2 Data file formats",
    "text": "2 Data file formats\nIn general, data should be stored and/or archived in open formats. Open formats are non-proprietary, and therefore maximize accessibility because they have freely available specifications and generally do not require proprietary software to open them (UC Santa Barbara’s Standard Operating Procedures section on Data Formats). The best file format to use will depend on the type and structure of data.\n\n2.1 Key characteristics of data file formats\n\nProprietary vs. non-proprietary: Non-proprietary software formats can be easily imported and accessed using open-source software. This enhances their interoperability, or how easily a file format can be used across different software platforms and systems. Formats that are widely supported and compatible with various tools are generally more versatile.\nTabular vs. hierarchical: Tabular data is organized into rows and columns, resembling a table, while hierarchical data is organized in a tree-like structure, with elements nested within others.\nStructured vs. unstructured: Structured data refers to data that is organized in a predefined format, typically in rows and columns, like databases or spreadsheets, which allows for easy search, analysis, and processing. Unstructured data, on the other hand, lacks a predefined format and is often textual or multimedia in nature, such as emails, social media posts, or video files.\nRetention of data types: Some file formats retain metadata about data types (e.g., whether a column is an integer or string), while others lose this information upon saving.\n\n\n\n2.2 Tabular formats\nUse open-source formats whenever possible.Plain text formats like CSV (.csv) are preferred for their transparency, interoperability, and long-term accessibility. Excel (.xlsx) and Stata (.dta) are proprietary and should generally be avoided—except when essential features (like Stata variable labels) are required.\nChoosing an open source format.The preferred open source format will depend on the project team’s preferences for accessibility, software, interoperability, need for computational efficiency, etc. The features of common formats are described below.\n\n\n\n\n\n\nNote\n\n\n\nIn cases where the native format of source data is in a proprietary software format, it is often necessary to use that software to view and edit data. For example, Stata dataset variables may have labels, a kind of embedded metadata that can only be accessed in Stata.\n\n\n\nCharacteristics of tabular formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nRecommended: comma-separated values\n.csv\nOpen-source\nNo\nStructured\n\n\nTab-separated values\n.tsv\nOpen-source\nNo\nStructured\n\n\nPlain text\n.txt\nOpen-source\nNo\nSemi-structured\n\n\nMicrosoft Excel spreadsheet/workbook\n.xls or .xlsx\nProprietary\nYes\nStructured\n\n\nFeather\n.feather\nOpen-source\nYes\nStructured\n\n\nParquet\n.parquet\nOpen-source\nYes\nStructured\n\n\nRData\n.rdata or .rds\nOpen-source\nYes\nStructured\n\n\nLightning Fast Serialization of Data Frames\n.fst\nOpen-source\nYes\nStructured\n\n\nSQLite\n.sqlite, .db\nOpen-source\nYes\nStructured\n\n\nStata data file\n.dta\nProprietary\nYes\nStructured\n\n\nSAS dataset\n.sas7bdat\nProprietary\nYes\nStructured\n\n\nDatabase File\n.dbf\nOpen-source\nYes\nStructured\n\n\n\n\n\nDescriptions of tabular formats\n\nText-based formats These formats are highly accessible and can be opened with common tools like Excel, Notepad, or any text editor, making them ideal for sharing output/final datasets. Text-based formats also work well with version control systems. However, be aware of their drawbacks and follow best practices.\n\nComma-separated values (.csv) delimited text files widely used for data exchange and simple data storage. Each row contains the same number of values separated by commas.\nTab-separated values (.tsv) files similar to CSV files but with values separated by tabs.\nPlain text (.txt) files which can contain unformatted or formatted (schema) text. Not recommended for storing complex datasets.\n\nExcel spreadsheets/workbooks (.xls, .xlsx) files designed for use with Microsoft Excel software. XLS is a binary file format compatible only with Excel, both older and newer versions. XLSX was developed more recently. It is XML-based, making it compatible with open-source software such Google Sheets as well as versions of Excel released since 2007. Generally avoid relying on these files for data storage due to complex formatting, data formats, formulas, etc. They also complicate quality assurance. XLS is not version-control friendly and XLSX requires special version-control techniques because it is stored in a compressed state. Excel spreadsheets can easily be exported to CSV files.\nCommon data science formats These formats are well suited for working with data during analysis and collaboration (e.g., intermediate data), because they retain metadata (e.g., data types) and are optimized for reading and writing. They are more computationally efficient in terms of input/output speed and file size, but often less suited to version control than text-based formats.\n\nFeather (.feather) a fast, lightweight binary columnar data format used for data exchange between data analysis languages like R and Python. Optimized for performance and efficiency, especially when working with large tables of data. Faster than Parquet at in-memory work, reading, and writing.\nParquet (.parquet) a binary columnar data format designed for efficient storage and processing of large datasets. Supports compression and is optimized for performance across data tools like R, Python, and SQL engines. Better file size efficiency than Feather.\nRData (.rds, .rdata) files used to store one R object (.rds) or an R environment with several objects (.rdata). Useful if working within an R project for efficiency and organization features, but providing limited interoperability.\nLightning Fast Serialization of Data Frames (.fst) a fast, compressed, columnar binary format designed specifically for R, optimized for high-speed reading and writing of large data frames. Ideal for efficient storage and selective column access in R workflows, but not designed for cross-language interoperability.\nSQLite (.sqlite, .db) files used by the SQLite relational database engine, which supports SQL queries and transactions and is used for lightweight, portable databases.\nStata data file (.dta) binary files created by the statistical analysis software Stata. Note that they sometimes include metadata (e.g., variable labels) that isn’t automatically loaded when importing into other software (e.g. R using the haven package).\nSAS Dataset (.sas7bdat) the proprietary file format used by SAS for storing datasets. It supports metadata and variable attributes. Datasets should be converted to open-source formats after processing.\nDatabase File (.dbf) files used by dBASE and other database systems to store tabular data. They support a fixed schema and metadata. DBF files cannot store full precision. Avoid creating this type of file.\n\n\n\n\nWorking with text-based file formats\n\nLimitations of text-based formats\nText-based formats like CSVs do not store data type metadata.\nBe cautious when reading and writing CSVs or other plain-text formats. Understand that data types will be inferred, not preserved, and this may introduce rounding or formatting errors. All values are saved as character strings in text-based file formats — there is no embedded information about whether something is an integer, float, etc.\nThis has several implications:\n\nType guessing on import When reading a CSV, software tools typically infer column types automatically. This can result in:\n\nInconsistent interpretation across tools (e.g., a column read as numeric in one program might be read as text in another).\nConversion errors, especially with dates, floating points, or missing values.  \n\nPrecision loss on export If you’re exporting a floating point column to CSV, the software may:\n\nRound or truncate values.\nDrop trailing digits or use limited decimal precision.\nThese changes might not be obvious but can affect downstream calculations.  \n\nNo built-in support for special types Since all values are strings, CSV files cannot inherently distinguish between:\n\n3 as an integer vs. 3.0 as a float\n2023-01-01 as a string vs. a date object\nTRUE as a logical value vs. a text label\n\n\n\n\nPractices for working with text-based formats\n\nReading/importing text-based data\n\nExplicitly specify column types when importing data (e.g., using read_csv(..., col_types=...) in R or dtype=... in python pandas). Another strategy is to specify all column types as character (col_types = 'character), view how they are stored, and then decides which columns to convert and how.\nValidate types after import, either by manually inspecting or using scripted tests.  \n\nExporting/sharing text-based data\n\nUse text-based formats only when useful, such as when publishing / sharing data with those without specialized software, or when version-controlling data.\nUse formats that preserve numeric precision (e.g., binary formats like .rds, .feather, .parquet) during analysis.\nClearly document data types in metadata or a readme when sharing text-based formats.\n\n\n\n\n\n\n2.3 Hierarchical formats\nHierarchical data formats are best suited for storing and exchanging complex, nested data structures with parent-child relationships, such as configurations, scientific datasets, or web APIs, where flexibility and the ability to represent variable levels of detail are essential.\n\nCharacteristics of hierarchical formats\n\n\n\n\n\n\n\n\n\n\nFormat\nExtension\nOpen-source or Proprietary\nRetains Individual Data Types?\nLevel of Structure\n\n\n\n\nHierarchical Data Format version 5 (HDF5)\n.h5, .hdf5\nOpen-source\nYes\nStructured\n\n\nNetwork Common Data Form (NetCDF)\n.nc\nOpen-source\nYes\nStructured\n\n\nJavaScript Object Notation\n.json\nOpen-source\nNo\nSemi-structured\n\n\neXtensible Markup Language\n.xml\nOpen-source\nNo\nSemi-structured\n\n\nYAML\n.yml or .yaml\nOpen-source\nNo\nUnstructured\n\n\n\n\n\nDescriptions of hierarchical formats:\n\nHierarchical Data Format version 5 (.hdf5, .h5), commonly called HDF5, files for storing complex and hierarchical datasets, supporting large data volumes and complex data structures.\nNetwork Common Data Form (.nc), commonly called NetCDF, files designed for array-oriented scientific data. They work especially well for multi-dimensional data like time-series and spatial data.\nJavaScript Object Notation (.json), text-based files used for storing structured data. Often used to transfer data between a server and a web application, as well as when sending and receiving data via an API.\neXtensible Markup Language (.xml), files organizing data hierarchically with customizable tags, making them both machine-readable and human-readable. XML is widely used in web services, data exchange, and configuration files.\nYAML (.yaml or .yml), human-readable files using a data serialization format well suited for configuration files and data exchange. It uses indentation to define structure and supports key-value pairs, lists, and nested data, making it simpler and more concise compared to XML or JSON. “YAML” is a recursive acronym: YAML Ain’t Markup Language.\n\n\n\n\n2.4 Geospatial file formats\nGeospatial data are stored as either vector data or raster data. The format of input spatial data typically dictates which geospatial tools can be applied to it.\n\nVector data\n\nVector data is stored as pairs of coordinates. Points, lines, and polygons are all vector data.\nRecommended open-source vector file formats:\n\nGeopackage (.gpkg, recommended for its advantages over the shapefile format)\nKeyhole markup language (.kml, .kmz)\nGeoJSON (.json, .geojson)\nTables with coordinates (e.g., a CSV file)\n\nCommon proprietary vector file formats:\n\nShapefiles (.shp)\nFeature classes in geodatabases (.gdb)\n\n\n\n\n\n\n\nNote\n\n\n\nA shapefile is actually a collection of several files, including geometry (.shp), projection information (.prj), tabular data (.dbf), and more. Make sure to store all component files together within the same folder.\n\n\nAll vector data files should have three critical metadata components:\n\nCoordinate reference system\nExtent: the geographic area covered by the data, represented by four coordinate pairs\nObject type: whether the data consists of points, lines, or polygons\n\n\n\nRaster data\n \nRaster data formats store values across a regular grid containing cells of equal size, with each cell containing a value. A raster is similar to a pixelated image, except that it’s accompanied by information linking it to a particular geographic location. All cell values within a single raster variable are of the same scalar data type (integer, float, string, etc.). Common examples of raster data are elevation, land cover, and satellite imagery.\nThe recommended general purpose raster file format is GeoTIFF (.tif), as it supports multiple bands, retention of spatial reference metadata, large file sizes, high compression, and use in a variety of languages/software. Other formats may work better for specific use cases. All of the following common formats are open-source:\n\nGeoTIFF (.tif), the most widely used format for raster data\nASCII grid (.asc), plain-text-based files for elevation models and basic raster grids\nNetCDF (.nc) and HDF5 (.hdf5, .h5), both described in Section 2.3\n\nAvoid saving rasters as proprietary software file formats, including ESRI grid/tile and ERDAS Imagine (.img) files.\nAll raster files should have five critical metadata components:\n\nCoordinate reference system\nExtent: how large the raster is, often represented by the number of rows and columns\nOrigin point: a pair of coordinates pinpointing the bottom left corner of the image\nResolution: cell size\nNo data value: the value that represents when a cell’s data value is missing\n\nFor a more in-depth introduction to spatial data types, see Introduction to Geospatial Concepts: Summary and Setup (datacarpentry.org) and GIS Training (RFF intranet).",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/data-management/file-formats.html#figure-file-formats",
    "href": "docs/data-management/file-formats.html#figure-file-formats",
    "title": "Data & File Types",
    "section": "3 Figure file formats",
    "text": "3 Figure file formats\nIt is helpful to think ahead when generating and saving data visualizations and plots. Academic journals often accept TIFF and PNG formats, but they frequently have resolution and dimension requirements. Export figures with a minimum resolution of 300 dots per inch (DPI).\nFor RFF communications, however, vector formats are best. because it can be easily modified as needed. These include:\n\nScalable Vector Graphics (.svg)\nEncapsulated PostScript (.eps)\n\nConsider that you may want to be able to share the underlying data with the RFF Communications team so that they and their external design partners can create custom figures for presentation on the website, in the magazine, etc. This means clearly documenting the processing code that created the underlying data / figures, so that output data can be easily reproduced and shared as needed. If figure data is time-consuming to reproduce, you may want to save a copy of it to the L:/ drive or to your GitHub repository.\nFor more information on figure (and table) style guidelines, refer to the RFF Style Guide.\n\n“Sharing the underlying data of any maps and figures is always helpful for the Communications Team!”\n– Elizabeth Wason (Editorial Director, RFF)\n\n\n3.1 Other resources\nSee these websites for additional information about data types:\n\nR-focused:\n\nR for Data Science: Data import\nR for Data Science: Transforming data types\n\nPython-focused:\n\nSoftware Carpentry: Data types and type conversion\npandas.pydata.org: Data file formats and input/output tools",
    "crumbs": [
      "Data Management",
      "Data & File Types"
    ]
  },
  {
    "objectID": "docs/version-control/index.html",
    "href": "docs/version-control/index.html",
    "title": "Version Control (Beta)",
    "section": "",
    "text": "Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. You know how Microsoft Word will periodically save your file, and you can recover previous versions if you really mess up? That is a simple form of version control. Other forms offer far more functionality, including the ability to save and recover versions of an entire project, tools for collaboration, and more.\nSome commonly used version control systems are Git, Mercurial, and SVN. Here at RFF, we strongly advise using Git, which we will discuss for the remainder of the section. We also recommend using GitHub to host Git repositories online. Here are some of the benefits of using Git with GitHub for version control:\n\nGrants peace of mind knowing work is stored safely and can be recalled with minimal effort, which gives greater liberty to test out new ideas (even when a computer breaks!)\nFacilitates collaboration by allowing multiple versions to coexist and efficiently borrow code from one another\nEnables efficient review and discussion of code changes before incorporating them into the main branch of the repository\nDocuments when, why, and by whom specific changes were made, which helps with debugging and troubleshooting\nConsolidates project code and documentation in the same, easily-accessible, backed-up location.\nPreserves easy web browser access to all versions of code.\nOrganizes documentation so it can be kept alongside the code itself\nProvides public access to datasets and code under the auspices of a software license\nEmpowers audiences to answer their own questions about the data and methods used in publications, reducing overhead for researchers who would otherwise be given that task\n\nThis section of the guidance will help you get started in making version control a standard part of the project life cycle.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "href": "docs/version-control/index.html#what-is-version-control-and-what-can-it-do-for-you",
    "title": "Version Control (Beta)",
    "section": "",
    "text": "Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. You know how Microsoft Word will periodically save your file, and you can recover previous versions if you really mess up? That is a simple form of version control. Other forms offer far more functionality, including the ability to save and recover versions of an entire project, tools for collaboration, and more.\nSome commonly used version control systems are Git, Mercurial, and SVN. Here at RFF, we strongly advise using Git, which we will discuss for the remainder of the section. We also recommend using GitHub to host Git repositories online. Here are some of the benefits of using Git with GitHub for version control:\n\nGrants peace of mind knowing work is stored safely and can be recalled with minimal effort, which gives greater liberty to test out new ideas (even when a computer breaks!)\nFacilitates collaboration by allowing multiple versions to coexist and efficiently borrow code from one another\nEnables efficient review and discussion of code changes before incorporating them into the main branch of the repository\nDocuments when, why, and by whom specific changes were made, which helps with debugging and troubleshooting\nConsolidates project code and documentation in the same, easily-accessible, backed-up location.\nPreserves easy web browser access to all versions of code.\nOrganizes documentation so it can be kept alongside the code itself\nProvides public access to datasets and code under the auspices of a software license\nEmpowers audiences to answer their own questions about the data and methods used in publications, reducing overhead for researchers who would otherwise be given that task\n\nThis section of the guidance will help you get started in making version control a standard part of the project life cycle.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/index.html#version-control-basics",
    "href": "docs/version-control/index.html#version-control-basics",
    "title": "Version Control (Beta)",
    "section": "2 Version Control Basics",
    "text": "2 Version Control Basics\nBefore we get started in using Git for version control, let’s cover a few basic principles. Git is a popular version control system, which is software that we can download and use on our computer. Git works by monitoring the changes of the contents of an otherwise ordinary file folder, called a repository. In a Git repository, a user can tell Git which file changes to keep, which to discard, and can label those changes, go back to previous file versions, and much more!\nWhile it is possible to use Git only on your computer without posting it online, it is often advisable to host repositories online so that they are synced in the cloud, making it easy to share them and back them up. There are many websites that can host Git repositories, but the most popular is called GitHub, which is what we recommend using at RFF. (Some others you may come across GitLab and BitBucket) Not only does GitHub host these repositories, but it also provides convenient ways to host documentations, discuss code changes, and report bugs.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html",
    "href": "docs/version-control/faq.html",
    "title": "Tips and FAQ",
    "section": "",
    "text": "In general, git works well with:\n\nSmall files. It is not advisable to store large data files in git, because it makes the repository large and subsequent operations get slow and unwieldy. For storing large files, see the Storage Options section\nHuman-readable files. One of the biggest perks of version control is the ability to see specific changes to files. That feature only works for files composed of human-readable characters, examples of which include comma-separated values (.csv) and plain text (.txt), as well as source code. However, there are some commonly-used filetypes that are not human-readable, and thus do not work very well with version control, including:\n\nAny Microsoft office format (.docx, .xlsx, .pptx, etc.)\nMATLAB notebooks (.mlx)\n\n\n\n\n\nGreat question! If I may want to add that file to the repository later, I could simply avoid staging it for current commit, and stage it when it is ready later. If that file or folder should never be tracked, you can also create a file called .gitignore in the repository. Inside the file, you can simply add the names of the file(s), folder(s), or file extension types (i.e. *.log) that you wish for Git to ignore.Generally, it is good to ignore files/folders that get automatically generated by existing scripts, are too big for git, or are not relevant to other users of the repository. Here are some examples of files that you may consider adding to a .gitignore file:\n\na folder containing intermediate data files\na folder containing dependencies that get downloaded using a script\na project file created by a code editor\n\n\n\n\nThe importance and benefits of code review was presented in the Software Quality section of the guidance &lt;TODO: LINK TO SUBSECTION&gt;. If you don’t have anyone working on a project with you who is available to review your code, feel free to reach out to &lt;TODO: figure out if this is something we can support&gt;",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#faq",
    "href": "docs/version-control/faq.html#faq",
    "title": "Tips and FAQ",
    "section": "",
    "text": "In general, git works well with:\n\nSmall files. It is not advisable to store large data files in git, because it makes the repository large and subsequent operations get slow and unwieldy. For storing large files, see the Storage Options section\nHuman-readable files. One of the biggest perks of version control is the ability to see specific changes to files. That feature only works for files composed of human-readable characters, examples of which include comma-separated values (.csv) and plain text (.txt), as well as source code. However, there are some commonly-used filetypes that are not human-readable, and thus do not work very well with version control, including:\n\nAny Microsoft office format (.docx, .xlsx, .pptx, etc.)\nMATLAB notebooks (.mlx)\n\n\n\n\n\nGreat question! If I may want to add that file to the repository later, I could simply avoid staging it for current commit, and stage it when it is ready later. If that file or folder should never be tracked, you can also create a file called .gitignore in the repository. Inside the file, you can simply add the names of the file(s), folder(s), or file extension types (i.e. *.log) that you wish for Git to ignore.Generally, it is good to ignore files/folders that get automatically generated by existing scripts, are too big for git, or are not relevant to other users of the repository. Here are some examples of files that you may consider adding to a .gitignore file:\n\na folder containing intermediate data files\na folder containing dependencies that get downloaded using a script\na project file created by a code editor\n\n\n\n\nThe importance and benefits of code review was presented in the Software Quality section of the guidance &lt;TODO: LINK TO SUBSECTION&gt;. If you don’t have anyone working on a project with you who is available to review your code, feel free to reach out to &lt;TODO: figure out if this is something we can support&gt;",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/version-control/faq.html#tips",
    "href": "docs/version-control/faq.html#tips",
    "title": "Tips and FAQ",
    "section": "2 Tips",
    "text": "2 Tips\n\n2.1 Commit Messages\nIf you want to learn to write better commit messages, this is a great resource!\n\n\n2.2 Branch Naming\nHere is a great resource for learning more about branch naming conventions.\n\n\n2.3 Tags\nSometimes it is helpful to name a specific commit of a repository. For example, say you have a version of the repository that you used to generate analysis for a publication. You can easily use the git tag command to “name” that specific commit. For example,\ngit tag &lt;tag-name&gt;\ngit push --tags\n\n\n2.4 Aliases\nAn Alias is like creating a shortcut for a git command so that you don’t have to type out the whole command.\nFor example, we type git status fairly often and it would be nice to simply type git st. We can tell Git that we want to create an alias called git st that simply executes git status instead. Here is how we tell Git Bash to save that into its configuration file.\ngit config --global alias.st status\n\nCommit Message Alias\nThis alias makes it easier to type a commit message without opening notepad.\n\nSetup\ngit config --global alias.cm 'commit -m'\n\n\nUsage\ngit cm \"Commit message here\"\n\n\n\nTree Visualization\nGit Bash offers tools to visualize commits/branches. Usually these are based on the command git log, which has many different options for customization. Below is a customized git log command that is nicely color-coded, shows user, date, and commit message.\ngit log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\n\n\nAs you can see, it prints the commit hash, the date of the commit, the author, the name of the branch, and the commit message, as well as the branching structure at the far left.\n\nSetup\nTo make an alias for it with the name tree, enter the following command in Git Bash:\ngit config --global alias.tree \"log --graph --oneline --exclude=origin/gh-pages --branches --pretty=format:'%C(yellow)%h %Cred%ad %Cblue%an%Cgreen%d %Creset%s' --date=short\"\n\n\nUsage\nNow, to print the visualization, simply enter:\ngit tree\ngit tree -10     # this option makes it only print the latest 10 commits.",
    "crumbs": [
      "Version Control",
      "Tips and FAQ"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html",
    "href": "docs/version-control/tutorial-branching.html",
    "title": "Tutorial: Branching",
    "section": "",
    "text": "Before you begin this tutorial, make sure you have:\nThis is one of the most amazing features of Git. Git allows us to create different working versions, or branches of our repository. This lets me take a snapshot of the repository and try out a new idea without affecting my main branch. Then, once I (and all my collaborators) are satisfied with the changes in my new branch, we can merge it back into the main branch using a Pull Request, which gives tools for reviewing and providing feedback on the proposed changes. This is how the majority of modern software is developed. It allows easy review of the portions of the code that have changed, and gives me confidence that my changes are not messing up the main branch until they are fully developed.\n---\ntitle: Example Git Branching\n---\ngitGraph\n   commit id: \"initial commit\"\n   commit id: \"starting point for the new feature\"\n   branch my-feature\n   checkout main\n   commit id: \"ongoing development of main\"\n   checkout my-feature\n   commit id: \"trying new feature\"\n   commit id: \"perfecting new feature\"\n   checkout main\n   merge my-feature id: \"now it is merged!\"\n   commit id: \"continued development\"\nHere’s an overview of the process:",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html#creating-branches",
    "href": "docs/version-control/tutorial-branching.html#creating-branches",
    "title": "Tutorial: Branching",
    "section": "1 Creating Branches",
    "text": "1 Creating Branches\nYou may have realized that Git creates a branch by default, called main. You could think of the main branch as the trunk of a tree. (If you encounter any older repositories, the default was the master branch, but the default was changed in mid-2020). You may notice that Git Bash has the branch name main written to the right of the prompt, which designates that main is the active branch, or the branch that is currently “checked out”.\n\n\nTo create a new branch, simply enter the command git branch &lt;my-branch-name&gt;. Generally, branch names should be:\n\nconcise yet descriptive. plot-results &gt; plot-all-results-with-numpy\nlowercase and hyphen-separated. plot-results rather than PlotResults or plot_results\nno special characters (numbers, letters, and hyphens only)\nSee more info in the branch naming tip\n\nYou will notice that after running the git branch command, the main branch is still designated as the active branch. To work on the newly created branch, simply use the command git checkout &lt;new-branch-name&gt;.\nAfter running that command, your new branch should be active.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use the following command to create a new branch and check it out in one step:\ngit checkout -b &lt;new-branch-name&gt;\n\n\nNow that your new branch is active, you can safely make changes and commits to this branch without worrying about modifying the main branch. When you push the branch, Git will give you an error saying that there is no “upstream” branch for the branch you are pushing. You can simply run the command it gives you, i.e. git push --set-upstream origin &lt;new-branch-name&gt;, and it should push with no problems.",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-branching.html#merging-branches",
    "href": "docs/version-control/tutorial-branching.html#merging-branches",
    "title": "Tutorial: Branching",
    "section": "2 Merging Branches",
    "text": "2 Merging Branches\n\n2.1 Creating a Pull Request\nOnce you have made some commits/changes and are ready to merge the new branch back into the main branch, now it is time to make a Pull Request in GitHub. To do that, navigate to the repository GitHub page, and click the “Pull requests” tab, shown below.\n\n\nNow select the “New pull request” button at the top of the page. We want to select the main branch as the “base” branch, and our new branch as the “compare” branch, as shown below:\n\n\nNow to finish creating the Pull Request, click the “Create pull request” button on the right side of the page. You may enter a title and description for the Pull Request, which is helpful for contributors to know what is contained in the Pull Request, and you could even include instructions for the contributors who you would like to review the Pull Request. Clicking the “Create pull request” button at the bottom will finish creating the request.\n\n\n2.2 Reviewing and Merging a Pull Request\nPull requests provide a great opportunity for collaborators to review one anothers’ code, to ensure accuracy. Code review can be daunting, especially if there are only changes to specific parts of a larger codebase. GitHub makes this process very easy, by identifying lines of code that have changed, and allowing collaborators to comment on them.\nTo review a pull request, the reviewer would navigate to the Pull Request and click on the “files changed” tab. From there, they can see each of the lines added, changed, or deleted, and insert comments on particular lines of code. After reviewing all the files changed, they can complete their review and choose whether or not to approve the pull request. To learn more about reviewing Pull Requests, see the appropriate section of the GitHub documentation.\nIf the reviewer has requested changes to the code based on their review, you can simply make changes, commit, and push to the same branch, and the Pull Request will be updated with those changes. Once the Pull Request has been reviewed and approved, you can go back to the “Conversation” tab of the Pull Request and click the “Merge pull request” button at the bottom of the page, and your branch’s changes will be reflected in the main branch!\n\n\n2.3 Conflicting Branches\nSometimes, when we try merging our new branch into the main branch, the Pull Request indicates that there have been changes made to the main branch that conflict with the changes in the new branch. When this happens, let’s go back to Git Bash and merge any new changes from the main branch into our branch, then resolve any potential conflicts. To do this, let’s pull updates to main and then merge main into our new branch by running the following commands:\ngit checkout main\ngit pull\ngit checkout &lt;new-branch-name&gt;\ngit merge main\nTo handle these conflicts, use the process described in the Handling Conflicts section to resolve them.\n\n\n2.4 Merging Branches without Pull Requests\nMerging with a Pull Request may, at times, feel a bit unwieldy, especially if you are in a hurry or working on a project by yourself. It is still possible to merge into the main branch without making and reviewing a Pull Request. We just advise caution with this method because it does not facilitate easy review, and so could easily introduce mistakes into the main branch. If you are confident that the branch is ready to merge without additional review, in Git Bash, simply enter the following commands:\ngit checkout main\ngit pull\ngit merge &lt;new-branch-name&gt;\nIf any conflicts come up, follow the instructions in the Handling Conflicts section. Once you have dealt with the conflicts (if any), you are done, and can push the main branch, which now has updates from your new branch.",
    "crumbs": [
      "Version Control",
      "Tutorial: Branching"
    ]
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Learning Lab: Version Control Part 1",
    "crumbs": [
      "Presentations"
    ]
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\nFirst, open Git Bash. (Hit the windows key and type “bash”)"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-1",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-1",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\nFirst, open Git Bash. (Hit the windows key and type “bash”)\nRFF@Theophilus MINGW64 ~\n$"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-2",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-2",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\nNow, let’s ask Git what version we are running:\nRFF@Theophilus MINGW64 ~\n$ git --version"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-3",
    "href": "presentations/learning-lab-vc1/index.html#introduce-yourself-to-git-3",
    "title": "Version Control Tutorial: Basics",
    "section": "Introduce Yourself to Git",
    "text": "Introduce Yourself to Git\nNow, let’s ask Git what version we are running:\nRFF@Theophilus MINGW64 ~\n$ git --version\ngit version 2.47.0.windows.2"
  },
  {
    "objectID": "presentations/learning-lab-vc1/index.html#new-page",
    "href": "presentations/learning-lab-vc1/index.html#new-page",
    "title": "Version Control Tutorial: Basics",
    "section": "new page",
    "text": "new page\nIn the Git Bash window, you can type all kinds of commands, including Git commands, which are always prependended with “git”, or bash commands that are commonly run in terminals.\nLet’s start with:\ngit --version\nIt should have printed out what version you are running, something like git version 1.47.0.windows.2. Now, let’s introduce ourselves to Git:\ngit config --global user.name 'Your Name Here'\ngit config --global user.email 'your-name-here@rff.org'\ngit config --global --list\nNow, Git will include your name and email when you publish changes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-this-resource",
    "href": "index.html#about-this-resource",
    "title": "Home",
    "section": "",
    "text": "This guidance is designed to help RFF research teams:\n\nSave time through clear data practices, templates, and reusable workflows\nIncrease flexibility for collaboration, future reuse, and reproducibility\nReduce risk by supporting consistent and transparent workflows\n\nIt includes practical support to:\n\nBuild foundational skills and concepts\nPlan and manage data-driven research from start to finish\nNavigate RFF-specific systems like storage and access\nSet up new projects effectively\nImprove existing workflows",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-outline",
    "href": "index.html#site-outline",
    "title": "Home",
    "section": "Site Outline",
    "text": "Site Outline\n\nFoundations\nData Management\nSoftware Quality\nVersion Control",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-and-feedback",
    "href": "index.html#questions-and-feedback",
    "title": "Home",
    "section": "Questions and feedback",
    "text": "Questions and feedback\nThis is a living resource — it will continue to evolve as needs shift and feedback is incorporated.\nTo submit questions, bugs (e.g., broken hyperlinks), suggestions, or feedback on this guidance, click Report an issue on the right-hand side of this page and submit an issue to the repository. Note that your comment will be publicly visible.\nTo submit a question or comment over email, reach out to the Data Governance Working Group.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#site-authors",
    "href": "index.html#site-authors",
    "title": "Home",
    "section": "Site authors",
    "text": "Site authors\nMembers of the RFF Data Governance Working Group\n\nAris Awang\nPenny Liao\nEthan Russell\nJohn Valdez\nMatthew Wibbenmeyer\nJordan Wingenroth\nAlexandra Thompson",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html",
    "href": "docs/version-control/tutorial-basics.html",
    "title": "Tutorial: Basics",
    "section": "",
    "text": "Before you begin this tutorial, make sure you have:",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#introduce-yourself-to-git",
    "href": "docs/version-control/tutorial-basics.html#introduce-yourself-to-git",
    "title": "Tutorial: Basics",
    "section": "1 Introduce Yourself to Git",
    "text": "1 Introduce Yourself to Git\nFirst, open Git Bash. You can hit the windows key and type “bash” and it should pop up for you to open.\nIn the Git Bash window, you can type all kinds of commands, including Git commands, which are always prependended with “git”, or bash commands that are commonly run in terminals.\nLet’s start with:\ngit --version\nIt should have printed out what version you are running, something like git version 1.47.0.windows.2. Now, let’s introduce ourselves to Git:\ngit config --global user.name 'Your Name Here'\ngit config --global user.email 'your-name-here@rff.org'\ngit config --global --list\nNow, Git will include your name and email when you publish changes.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#creating-a-repository-in-github",
    "href": "docs/version-control/tutorial-basics.html#creating-a-repository-in-github",
    "title": "Tutorial: Basics",
    "section": "2 Creating a Repository in GitHub",
    "text": "2 Creating a Repository in GitHub\nThis section is useful for learning, but you can also skip to the next section on cloning repositories if you already have a repository (i.e. for a project) that you’d like to clone.\n\nTo create a repository hosted on GitHub, first navigate to github.com and click the plus button in the top right corner, then click the “new repository” button.\n\n\n\n\nNext, you may select who you would like to own the repository. It could either be owned by your user account, or an organization you are a member of. This may be something to discuss with your project team. The repository owner will show up in the URL of the repository, in the structure of: https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;. For example, for the E4ST.jl repo created in the e4st-dev organization, the URL is: https://github.com/e4st-dev/E4ST.jl. If in doubt, you can specify yourself as the owner, and change ownership later on. See the Workflows section on repository ownership for more information.\n\n\n\n\nNow you must enter the repository name. We recommend all lowercase names with dashes separating words, like my-repo-name. The only exception is when a programming language’s best practices prescribe a specific repository naming convention. (i.e. julia package repositories are supposed to be camelcase as in MyRepoName.jl)\nNow you must choose whether the repository is to be public or private. Generally, choose private for repositories that will contain sensitive information, and public if the project requires it to be public. It is easy to change from private to public later on, so when in doubt choose private. See the public/private section in Workflows for more information. If you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\nCheck the box to add a README file. This will create a file called README.md located in the repository’s root folder, where you can add basic documentation for the repository.\nOptional: Choose a .gitignore template from the dropdown menu. A .gitignore file specifies certain file types that Git will not track the changes of, by default. For example, it is best to ignore an auto-created config file made by R studio that is user-specific. Generally it is a good idea to select the .gitignore template for the programming language you will be using.\nClick the “Create Repository” button!! This should take you to the home page of your new repository.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#cloning-your-first-repository-with-git",
    "href": "docs/version-control/tutorial-basics.html#cloning-your-first-repository-with-git",
    "title": "Tutorial: Basics",
    "section": "3 Cloning Your First Repository with Git",
    "text": "3 Cloning Your First Repository with Git\nNow that we’ve made a remote repository, let’s get it copied onto our computer. Copying a remote Git repository is called cloning. This process will work the same way for any existing repository, including the one that we made in the steps above.\nFirst, it’s important to choose a good file location to store the git repository. While it’s not required, many people like to have a single folder to store all of their Git repositories. An alternative would be to store the Git repository in an associated project folder.\nGit Bash has a working directory, which is the location that it is operating in. To see what the current working directory is, we can enter the pwd command, which stands for primary working directory.\npwd\nNow lets try changing the working directory with the cd command, to whatever directory you would like to store your Git repository in. For example:\ncd 'C:/Users/&lt;my-user-name&gt;/OneDrive - rff/repos'\n\n\n\n\n\n\nNote\n\n\n\nIt is only necessary to use quotes for filenames in bash if there is a space in the path, as in the example above.\n\n\nNow, in an internet browser, navigate to the home page of the repository you wish to clone.\nClick the button labeled &lt; &gt; Code, then copy the URL to clipboard by clicking the logo with intersecting squares.\nIn the Git bash window, type git clone then paste in the repository URL from your clipboard by right-clicking. Altogether this would look like:\ngit clone https://github.com/&lt;owner-name&gt;/&lt;repository-name&gt;\nIf the repository is private, Git Bash may prompt you for your GitHub credentials. After entering them, you should be left with the cloned repository located in the working directory!\n\n\n\n\n\n\nNote\n\n\n\nIf you have set up SSH keys as in the optional setup section, you can copy the SSH address for the repository. To do this, after clicking the &lt; &gt; Code button, select SSH before copying the URL to your clipboard. If properly set up, when cloning from an SSH URL, you should not be prompted for GitHub credentials, even for a private repository.\n\n\nFinally, you can navigate into that newly created repo using the cd command:\ncd &lt;repository-name&gt;",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#making-and-publishing-file-changes",
    "href": "docs/version-control/tutorial-basics.html#making-and-publishing-file-changes",
    "title": "Tutorial: Basics",
    "section": "4 Making and Publishing File Changes",
    "text": "4 Making and Publishing File Changes\nNow that we have a Git repository on our computer, let’s explore the process of making file changes and saving versions. To give a conceptual outline, there are four steps.\n\nPull changes from others. This is to ensure we are working on the latest version of the repository. git pull\nMake changes to files. You can do this with any text editor.\nStage changes. This tells Git which changes you would like to select for the next version. git add\nCommit staged changes. This publishes any staged changes as a new version. git commit\nPush commit(s) to the remote/online repository. git push\n\nLet’s walk through that process together. In Git Bash, type git status to show the status of the repository. It should give you a short report about which branch you are on (more about branches later), whether your local copy is up to date with the remote copy, and any file changes you’ve made.\nOn branch main                                 # this is the default branch\nYour branch is up to date with 'origin/main'.  # the local version matches the version in GitHub\n\nnothing to commit, working tree clean          # we haven't made file changes\nAt this point, it is ALWAYS a good idea to run the git pull command to make sure we are up to date with the latest version. This command first checks to see if the remote repository version (the version stored in GitHub) has had any commits since the last time we pushed or pulled. If there are any, it will download them incorporating those changes into your working version.\nNow, let’s open up the README file and make some changes.\nYou can either navigate to the file in Windows File Explorer or type notepad README.md to open the README file in the notepad text editor. Add a few lines to your README, then save and close the file. In the future, you can use whatever editor you prefer, such as RStudio, MATLAB, Visual Studio Code, etc. After making that change, let’s type our git status command into Git Bash again.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nThis message is telling us that there are some file changes that have not yet been staged for commit. Let’s stage the changes from the README file with the command git add README.md.\n\n\n\n\n\n\nNote\n\n\n\nYou can use the command git add -p in order to interactively select which files, and even sections of files you would like to stage for commit. You can also use the command git add * to add all of the files that have been changed since the previous commit.\n\n\nNow let’s check the status once more:\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   README.md\nNow, our README.md file is under the heading Changes to be committed, so it is ready to be published. Let’s publish with the command git commit. It should open up notepad and prompt you to enter a commit message. In the first line, enter a concise commit message describing the changes that you are publishing, more info on that here. Save and close the file, and you have successfully authored your first commit!\n\nIf you find typing your commit message in notepad arduous, we have the command for you!\ngit commit -m \"Simply type your messsage here\"\nOR if you want to be even more lazy, see the section for the commit alias\n\nThe last step is to push commits to the online repository. At this point, git status indicates that the local version of the repository is 1 commit ahead of the online version. To send the commits to the online repository, we will simply enter the command git push! Now, if you navigate to the online version of the repository in GitHub, you should be able to see the changes you made. Congratulations!!",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/tutorial-basics.html#handling-conflicts",
    "href": "docs/version-control/tutorial-basics.html#handling-conflicts",
    "title": "Tutorial: Basics",
    "section": "5 Handling Conflicts",
    "text": "5 Handling Conflicts\nHave you ever been working on a Word document with another person, and maybe your computer gets disconnected, and you accidentally both edit the same paragraph? Usually Word will get smart and show both sets of changes and allow you to choose which one you would like. Sometimes, the same thing happens in Git/GitHub.\nSay, for example, my collaborator pushes a commit to our project repository without me realizing. In an ideal world, I would run the git pull command to make sure I have that commit incorporated into my local copy. However, maybe I forgot to pull, or my collaborator pushed that commit as I was already making file changes. Then, I commit my changes and push them and am greeted with the following ugly message:\n$ git push\nTo github.com:Ethan-Russell/MyTestRepo.jl.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'github.com:Ethan-Russell/MyTestRepo.jl.git'\nhint: Updates were rejected because the remote contains work that you do not\nhint: have locally. This is usually caused by another repository pushing to\nhint: the same ref. If you want to integrate the remote changes, use\nhint: 'git pull' before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nDon’t panic! Git is built for this. This message is Git telling us that we can’t push because of the other commit, and it also tells us that the solution is to git pull before pushing again. When we run git pull, Git will try to automatically merge the changes for us. Ideally, if changes are made in separate sections of the file (or different files altogether), Git will be able to merge everything, and you can simply git push again to push your commit(s). However, if both versions have changed the same file, Git will flag it as a conflict and it will enter “MERGING” mode (denoted in git bash to the right of the prompt). This is the type of message Git will print if there is a conflict:\n$ git pull\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nUnpacking objects: 100% (3/3), 947 bytes | 55.00 KiB/s, done.\nFrom github.com:Ethan-Russell/MyTestRepo.jl\n   6449e68..a3b9c32  main       -&gt; origin/main\nAuto-merging README.md\nCONFLICT (content): Merge conflict in README.md\nAutomatic merge failed; fix conflicts and then commit the result.\nFortunately, Git has identified which file(s) have conflicts: in this case, only the README.md file has a conflict. So let’s open the file where there is a conflict. Git identifies which lines have conflicts by adding some helpful syntax, which we can search for to find the conflict. You can do a search for “====” and that will take you to the conflict:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# My Test Repo\n=======\n# MyTestRepo\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; a3b9c323e36238db4a26a91addc5c699add04b6a\nGit will list the local version first, right after printing &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, followed by a line of =======, then the version that was pulled in from the remote repository, then &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a unique identifier for the commit in the remote repository. Translating the above chunk of text, it looks like someone else had added the title MyTestRepo to the README.md file, and I added the title My Test Repo. All we need to do here is:\n\nSelect which version of the changes we want (or we could edit it to be a combination of the two!)\ndelete the other version, as well as all of the notation that Git has added (&lt;&lt;&lt;, ===, &gt;&gt;&gt;)\nSave and commit the resulting file.\n\nIn the example above, I would change that portion of the file to be:\n# MyTestRepoFinalName\nAfter adding and committing that change, we can then safely run git push.",
    "crumbs": [
      "Version Control",
      "Tutorial: Basics"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html",
    "href": "docs/version-control/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Before using GitHub, it is necessary to have a GitHub account. To make an account, go to the GitHub website and click Sign Up, following the prompts to create your account. As part of the process you will need to verify your email address. Note that GitHub accounts are associated with people, along the lines of a LinkedIn account, and it may be helpful to have a single personal GitHub account even if you plan to use it outside of your work at RFF. It is simple to associate multiple email addresses with your account later on, so it is unimportant which email address you create the account with.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#make-a-github-account",
    "href": "docs/version-control/setup.html#make-a-github-account",
    "title": "Setup",
    "section": "",
    "text": "Before using GitHub, it is necessary to have a GitHub account. To make an account, go to the GitHub website and click Sign Up, following the prompts to create your account. As part of the process you will need to verify your email address. Note that GitHub accounts are associated with people, along the lines of a LinkedIn account, and it may be helpful to have a single personal GitHub account even if you plan to use it outside of your work at RFF. It is simple to associate multiple email addresses with your account later on, so it is unimportant which email address you create the account with.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#git-installation",
    "href": "docs/version-control/setup.html#git-installation",
    "title": "Setup",
    "section": "2 Git Installation",
    "text": "2 Git Installation\nIt is necessary to install Git onto any computer from which you wish to interact with local repositories. Normally, IT has installed Git on RFF computers. If not, please send a request to IThelp@rff.org to request a git install on an RFF machine. For personal computers, you may install manually following these instructions.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#choose-a-user-interface",
    "href": "docs/version-control/setup.html#choose-a-user-interface",
    "title": "Setup",
    "section": "3 Choose a User Interface",
    "text": "3 Choose a User Interface\nWhen you install Git, it generally will already install a couple of applications you can use to interface with git.\n\nGit Bash/CMD – (Installed with Git) These options allow you to enter text-based commands for Git in an interface similar to the command prompt window in Windows. Git Bash allows for git commands in addition to standard UNIX commands, whereas Git CMD allows for git commands in addition to standard windows CMD prompt commands. This is a great environment for learning the main processes of Git as a new user, and we will use Git Bash for tutorials on this site.\n\n\n\n\nGit GUI – (Installed with Git) This is a minimalist graphical user interface letting you use git without having to use a command line. It also allows for simplistic visualization of branches. While our tutorials will use Git Bash, it should be fairly straightforward to use the Git GUI instead.\n\n\n\n\nRStudio Git Integration - (Installed Separately) RStudio, a popular editor for R code, comes with a Git user interface built in. This may be a good option for R users once they have learned the fundamentals with Git Bash.\n\nIf you wish to learn a different user interface, check out the list here, and check with the IT team before installing. Most of the concepts will be fairly similar to what we share in the guide.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "href": "docs/version-control/setup.html#optional-set-up-ssh-keys",
    "title": "Setup",
    "section": "4 Optional: Set up SSH Keys",
    "text": "4 Optional: Set up SSH Keys\nIf you will be using Git with private repositories, it is worth setting up a key on your PC so that you can freely pull and push from a remote repository without having to enter your password each time. Follow these instructions for setting up an SSH key, and these instructions for adding it to your GitHub account.\nThere is a small configuration change we need in order to enable the SSH protocol to work with RFF IT systems. From a git bash window, run the following lines:\n\ntouch ~/.ssh/config - This command creates a new empty file in your user home folder /.ssh directory.\nprintf \"Host github.com\\n  Hostname ssh.github.com\\n  Port 443\" &gt;&gt; ~/.ssh/config - This command writes 3 lines to the newly created config file.\n\nNote that these commands can be run from a Git Bash window open to any directory. The ~ sign represents your home directory. After running the commands, if you were to open the file in a text editor, it would look like:\nHost github.com\n  Hostname ssh.github.com\n  Port 443\nOnce you have that set up, when cloning a repository to your local folder, simply select the “SSH” option before copying the URL.",
    "crumbs": [
      "Version Control",
      "Setup"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html",
    "href": "docs/version-control/workflows.html",
    "title": "Workflows",
    "section": "",
    "text": "Now that we know some basic Git principles, how do we make a workflow for a given project? First, let’s start by talking about some key principles for any workflow, then we can get into some of the specific considerations for a project team at RFF.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#key-principles",
    "href": "docs/version-control/workflows.html#key-principles",
    "title": "Workflows",
    "section": "1 Key Principles",
    "text": "1 Key Principles\n\nUsers should commit changes frequently, with descriptive messages.\n\nFrequent commits make it so that it is easier to find discrete check-points in the repository, rather than having everything committed at the same time.\nTry to have commits provide incremental improvements. For example, a commit might include the addition of a single function, or a 2-line fix to an existing script.\nIf you find yourself coding for hours between commits, consider trying to break up your task into discrete sub-tasks.\n\nPull often to ensure that you are incorporating the changes of others.\nA local clone of a repository should generally ONLY have a single user.\n\nThis is why in the Directory Structure section of the Data Management guidance, we recommend that each user works from a repository clone stored in their own individual subfolder within the L:/ drive project folder.\n\n\nTake care with encoding file paths within a repository. You want other people to be able to use the repository without having to move files around or change all the file paths in the repository. You could either:\n\nUse relative file paths to point to other files that are located in the repository.\nUse absolute file paths (i.e. on a shared network drive like the L drive) and consider making a variable for the path directory, especially if used multiple times. That way, if a collaborator has their data at a different path on their system, they only need to change a single path.\ndata_path = “L:/ProjectName/Data”\ndata_file_1 = joinpath(data_path, “data_file_1.csv”)\ndata_file_2 = joinpath(data_path, “data_file_2.csv”)\nIf you use relative file paths pointing to files outside the repository, specify in the project README how the external files and the repository should be positioned relative to each other.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/version-control/workflows.html#workflow-planning",
    "href": "docs/version-control/workflows.html#workflow-planning",
    "title": "Workflows",
    "section": "2 Workflow Planning",
    "text": "2 Workflow Planning\nWhen a project team is deciding to use Git, it is important to think through the following considerations. We recommend answering these questions at the beginning of a version-controlled project.\n\n2.1 Where should each team member store their local repositories?\nHere are two examples of how a project / team might choose to store their repositories.\n\nGenerally Recommended: Each team member keeps their own working version of a repository on a shared drive, i.e. the L drive, which could (but probably shouldn’t) be accessed by other team members., i.e. inside of a larger project directory\n\nPros: individual users’ repositories are available on all lab computers; repos are stored within the project folder, allowing easy access to shared project files (e.g. raw or processed data files)\nCons: files could be accidentally changed by others if they are working in the incorrect version of the repository\n\nEach team member keeps their own working version of a repository somewhere private, either on the C:/ drive of their personal computer, or on the RFF servers, with the intention of no other team members having access to it. This is suitable for teams that have many repositories where it would get very confusing to store all the repositories in the same location for the whole team.\n\nPros: it is “clean”, with no accidental file changes from other users.\nCons: It would require care to be taken for access (file paths) to non-version-controlled data files.\n\n\n\n\n2.2 Where should the GitHub repository be hosted?\n\nDo you work on a team with their own GitHub Organization (i.e. E4ST)?\n\nIf so, verify with the other members of the team that it would be acceptable to use the organization to host the repository\nIf your team doesn’t already have one, does it make sense to create an organization for your team? Are there likely to be multiple repositories that would make sense to group under the same organization? If so, follow the steps in the GitHub Documentation for creating organizations.\n\nDoes this repository belong in the RFF Organization?\n\nGenerally, if the repository is associated with a publication, the code should be forked or transferred to the organization.\nWe suggest creating the repository as a personal repository and changing ownership or forking.\nSee the Data Management section on code publication for additional guidance.\n\nIf it doesn’t make sense to host the repository in an organization, we recommend having one of project members host it with their GitHub account.\n\n\n\n2.3 Should our repository be public or private?\nPublic repositories are visible to anyone, whereas private repositories allows for controlled access. For public repositories, it is still possible to control who is able to contribute to them, while private reporitories allow you to control who can even see them.\nGenerally, select for your repository to be private for repositories that will contain sensitive/proprietary information, and public if the project requires it to be public. Availability of data/code is important to ensure reproducability so making a repository public is a good thing in most cases. If you have sensitive data but would like for the repository to remain public, consider storing code and documentation in the repository, and sensitive data in a different way. See the Sensitive and Proprietary data section It is easy to change from private to public later on, so when in doubt choose private.\nIf you have chosen to make your project public, you will need to also select which license to use. See the section on software licenses to help make this decision.\n\n\n2.4 Should our team use branches?\nTo learn about branches, what they are for, and how to use them, see the Branching section of the Tutorial. There are a few aspects to consider with this question.\n\nHow many people will be making commits?\n\nIf only one person will be making commits, it may save time and effort to maintain a single branch, without creating additional branches.\nWith more people making commits, using multiple branches may help avoid unnecessary merge conflicts.\n\nIs it important to have a fully functional version at all times?\n\nBranches can be very good for preserving a functional version of the code, while allowing for risk-free experimental features to be developed.\n\nIs it important to have code review for this project?\n\nBranches facilitate streamlined code review, where reviewers only review sections that have changed (via Pull Requests).\n\n\nHere are some examples of when a branch might be useful:\n\nMajor Changes/Experimental Feature: Say you have an idea on how to improve your code, and it requires significantly re-vamping the code. It would make sense to create a branch for this experimental feature, so that you can develop the feature without worrying about making changes to the main branch. Then the whole team can use the Pull Request to evaluate whether or not they’d like to merge into the main branch.\nNew Team Member: You have an existing model that is already working well. A new researcher joins the project team and is tasked with a code improvement. They are new to the model, so you want to review their changes carefully before applying them to the main branch. It makes sense to have them create a branch for their development, and review it with the pull request.\nConcurrent Development: Your team has a couple of features that need to be developed in tandem. To prevent extensive conflicts and streamline the process of combining the new features, each feature gets its own branch.\nProject: If your team has a project that requires making project-specific changes, it could be a good idea to create a project branch, with no intention to merge the project branch into main. ⚠️However, for features that would be useful to have in the main branch, implementing those features in the project branch would make it difficult to merge them back into the main branch later on. In that case, it is best to implement those features in the main branch or in a separate branch (to be merged with main), and merge the main branch back into the project branch. It is difficult to selectively merge changes from the project branch into the main branch.\n\n\n\n2.5 If using branches, how do we want to handle code review?\nIt would be good to talk about the following questions with your team:\n\nShould code review be required for every change to the code?\n\nOne example of this would be requiring that every change be made in a branch and submitted via Pull Request, and that at least one person who did not author the Pull Request must approve it in order for the branch to be merged into the main branch.\nYour team may decide to only require a Pull Request and code review for major features.\n\nWho is allowed/required to approve Pull Requests?\n\nOne example would be to require every Pull Request created by a newer team member to be reviewed by a more experienced team member.",
    "crumbs": [
      "Version Control",
      "Workflows"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html",
    "href": "docs/data-management/organization.html",
    "title": "Organization",
    "section": "",
    "text": "Standardized practices for file organization and storage save time and ensure consistency, enhancing the overall quality of research outputs. A simple and flexible folder structure not only promotes long-term data stability but also supports seamless project growth, adaptability, and researcher transitions. Such an approach reduces the complexity of project management and aligns effectively with version control systems, enhancing collaborative efforts and preserving institutional knowledge.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#summary",
    "href": "docs/data-management/organization.html#summary",
    "title": "Organization",
    "section": "1 Summary",
    "text": "1 Summary\n\nKeep raw data in a dedicated folder (raw/) and never modify it directly.\nUse a simple, descriptive folder structure that reflects project logic and is easy for new users to understand.\nSupport version control by organizing code and outputs in a way that works with Git, i.e. by creating separate repository folders for each user.\nDesign for flexibility, allowing new data, methods, or collaborators without major reorganization.\nName files/folders to be human- and machine-readable: use clear names, avoid spaces and special characters, and include numeric prefixes for order.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#sec-directory-structure",
    "href": "docs/data-management/organization.html#sec-directory-structure",
    "title": "Organization",
    "section": "2 Directory structure",
    "text": "2 Directory structure\nRegardless of the specific method deployed, your data project organization should have the following qualities:\n\nRaw data are kept in a distinct folder and never modified or overwritten. Always keep an unaltered version of original data files, “warts and all.” Avoid making any changes directly to this file; instead, perform corrections using a scripted language and save the modified data as separate output files. Consider making raw datasets “read-only” so they cannot be accidentally modified.\nSimple: The folder structure should be easy to navigate and understand, even for someone new to the project. It should mirror the logical flow of the project and use clear, descriptive names that reflect the contents and purpose of each folder.\nFlexible: The structure should be adaptable to evolving project needs, allowing for the addition of new data, methods, or collaborators without disrupting the existing organization. It should support different types of data and workflows, making it easy to integrate new elements as the project evolves.\n\nThese qualities also facilitate version control practices. There is additional guidance on organization for version control here.\nBelow is an example of a directory structure that would be compatible with version control implementation on RFF’s L:/ drive. This version illustrates a personal repository folder model of code version control, which operates best on the L:/ drive.\nproject_name/\n├── data/\n│   ├── raw/\n│   ├── intermediate/\n│   ├── clean/ (optional)\n├── results*/\n├── repos/\n│   ├── edgar/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n│   ├── caudle/\n│   │   ├── scripts/\n│   │   │   ├── processing/\n│   │   │   ├── analysis/\n│   │   │   ├── tools/\n│   │   ├── results**/\n│   │   ├── docs/\n\n2.1 Data folder\n\nThe data folder contains all project data sets.\nRaw data is preserved in its own subfolder.\nThe intermediate folder contains datasets created during data cleaning and processing.\nIf practical, a clean folder can contain cleaned output datasets, but note that it’s often unclear when datasets are truly “clean” until late project stages.\n\n\n\n\n\n\n\nNote\n\n\n\nFor projects with small datasets, this folder can be version controlled, with larger files ignored using the .gitignore file. In this case, this folder would be a subfolder of individuals’ repository folders.\n\n\n\n\n2.2 Results folder\n\nThe results folder contains analysis results, and model outputs. For example, it can be used to store tables, figures, and model estimates.\n*For internal RFF projects, the results folder can be stored in the main directory and not within repositories.\n**For projects with external collaborators, it may be useful to store the results folder within repositories. These files are generally smaller than 100 MB and can be stored in a main repository using GitHub; however, some formats (such as SVG) can be quite large. The .gitignore file can be configured to ignore certain file types (such as SVG files, when both PNG and SVG file formats are generated).\n\n\n\n2.3 Repos folder\n\nThis directory structure is based on personal repositories. The repos, or repositories, directory contains version-controlled files. To allow individuals to work on version controlled files without interfering with others’ versions, each researcher should have their own folder that’s linked to the GitHub remote repository. Team members can then clone the project’s Git repository into their respective folders and work exclusively within their own copies. Changes can be synced and reconciled using GitHub, preventing simultaneous edits of the same file and ensuring effective version control.\nIndividual folders (e.g. Edgar, Caudle) should have mirrored (the same) directory structures.\n\n\n\n2.4 Scripts folders\n\nThe scripts folder contains all code files (e.g. .R, .py, .do) for the project. These scripts provide explicit instructions for processing data and performing analyses. It should be possible to reproduce the entirety of the project’s processed data sets and results using only these scripts and the raw data.\nThe scripts folder can be parsed into subfolders containing scripts that process raw data (processing), analyze processed data (analysis), and tools. The tools folder (sometimes called util, modules, or helpers) contains scripts with distinct functions that can be “called” (referenced) in the main processing scripts. This is especially useful if functions are used multiple times or are lengthy. Separately storing functions that may be used in multiple source code scripts is an important practice in creating quality software.\n\n\n\n2.5 Docs folder\nThe documents folder should contain any version-controlled shared documents (e.g. LaTeX, Markdown, Overleaf).\n\n\n2.6 Other project files\nThis template does not include specific folders for meeting notes, literature reviews, presentations, project management, etc., because those types of files are not the focus of this guidance.\nWe recommend storing these types of files in the project-level Microsoft Teams folder. See the RFF Communication Norms and Guidance for more information.",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#other-organization-practices",
    "href": "docs/data-management/organization.html#other-organization-practices",
    "title": "Organization",
    "section": "3 Other organization practices",
    "text": "3 Other organization practices\n\n3.1 Subfolders\nOrganizing files into subfolders can help manage complexity and improve workflow. Subfolders are particularly useful when a single folder grows too large, making it hard to locate specific scripts, data, or results. By creating logical groupings you can keep related files together and streamline collaboration. Examples of logical groupings for subfolder names are by\n\ndata source (e.g., usda),\nvariable (precipitation),\nprocessing step (merge), or\nresults category (e.g., regressions or projections).\n\nIdeally, project folders should be organized so that each file’s complete path is informative about the role of the folder’s contents in the project. Some examples are:\n\nA script for cleaning Corelogic transactions data: scripts/corelogic/01_cleaning/01_clean_trans_data.R\nRaw Corelogic transactions data from Los Angeles County (FIPS code 06037): raw_data/corelogic/transactions/06037.dta\nA regression table for the outcome variable property values, with heterogeneity results by region: results/regressions/property_value/table_region.tex",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "href": "docs/data-management/organization.html#naming-folders-files-and-scripts",
    "title": "Organization",
    "section": "4 Naming folders, files and scripts",
    "text": "4 Naming folders, files and scripts\nWhen creating folders:\n\nAvoid ambiguous/overlapping categories and generic catch-all folders (e.g. “temp” or “new”).\nAvoid creating or storing copies of the same file in different folders.\n\nWhen creating data or script files, make them:\n\nHuman readable: Create brief but meaningful names that can be interpreted by colleagues.\n\nMake names descriptive: they should convey information about file/folder content. For example, if you’re generating output visualizations of the same metric, instead of county_means_a and county_means_b, use county_means_map and county_means_boxplot.\nAvoid storing separate versions of files (e.g. county_means_map_v2), and instead rely on version control tools to save and document changes.\nIf you must create different versions of files, make sure to document the distinction in a README file.\nIf you use abbreviations or acronyms, make sure they are defined in documentation such as a project-level README file.\n\nMachine readable:\n\nUse only ASCII characters (letters, numbers, and underscores).\nDo not include spaces or special characters (/  : * ? &lt;&gt; &).\nUse hyphens or underscores instead of spaces (the “snake_case” method).\nFiles and folders should be easy to search and filter based on name using structured file names. The ability to sort and read files by name is useful and helps organization but requires specific conventions. See examples in the table below, keeping in mind that:\n\nIt is not recommended to use dates in script file names to denote when a script was created or modified. Instead, leverage version control to save and document different script versions.\nMake sure to “left pad” numbers with zeros. For example, use 01 instead of 1. This is to allow default sorting to still apply if and when the file name prefixes enter the double digits.\n\n\n\n\n\n\n\n\n\n\n\nPractice for structured file names\nDescription\nExample\n\n\n\n\nID-based names\nStructure file paths and names in a way that makes them easy to access programmatically—e.g., enabling batch loading or iteration across identifiers like years/dates, counties, or scenarios. For example, storing county-level data as shown would allow data to be read into memory by simply looping over FIPS codes as they appear in the directory file names.\n53019_data.csv53033_data.csv53061_data.csv\n\n\nChronological order\nUse ISO 8601 format for date-based files: YYYY-MM-DD. This ensures dates sort correctly by default.\n2021_01_01_precipitation_mm.csv2021_01_02_precipitation_mm.csv2021_01_01_temperature_statistics_f.csv2021_01_02_temperature_statistics_f.csv\n\n\nLogical processing order\nFor scripts or folders that must run in sequence, use numeric prefixes to indicate the intended order of execution.\n01_clean_raw_data.R02_merge_clean_data.R03_descriptive_statistics.R04_regressions.R",
    "crumbs": [
      "Data Management",
      "Organization"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html",
    "href": "docs/data-management/quality-preparation.html",
    "title": "Quality & Preparation",
    "section": "",
    "text": "Always use and save a scripted program for data processing and analysis. Although it may seem more expeditious to take manual steps, writing code creates a documented and repeatable account of the processing steps taken and will save time and effort in the long-run.\nIf it is impossible to write code for processing steps, create a detailed record of the workflow in a document.",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "href": "docs/data-management/quality-preparation.html#quality-assurance-for-data-integrity",
    "title": "Quality & Preparation",
    "section": "1 Quality Assurance for Data Integrity",
    "text": "1 Quality Assurance for Data Integrity\nQuality assurance (QA) is ensuring the accuracy, consistency, and reliability of data. Quality assurance measures should be implemented on both raw data from external sources and your project’s subsequent datasets; for example, after a major processing step such as data merging.\nEight basic quality assurance measures are listed below, some of which were adapted from (Hutchinson et al. 2015).\n\nRead documentation / metadata that accompanies source datasets. It often comes in separate files (text, pdf, word, xml, etc.).\nAlways keep raw data in its original form. Do not modify raw datasets; save modified versions in the project’s “intermediate” data folder.\nVisually inspect data throughout processing. Visual checks can reveal issues with the data (e.g., repeated values or delimitation errors) that would affect analysis. This habit not only aids debugging processing code but also builds an understanding of the dataset.\nAssure data are delimited and line up in proper columns. Check that data is correctly delimited and parsed when imported into the processing program.\nCheck for missing values. Identify any missing or NA values in critical fields that could impact analysis. If there are missing values, identify the type of missingness and discuss solutions (applied example in R).\nIdentify impossible and anomalous values. Anomalies include values that are outside the expected range, logically impossible, outliers, or inconsistently formatted. In addition to checking for errors, identifying outliers can aid in data exploration by flagging rare events, errors, or interesting phenomena that require further investigation.\nPerform and review statistical summaries. Generate summary statistics to understand data distribution and identify inconsistencies or errors. Use these summaries to guide further cleaning, transformation, or data integrity checks.\nVisualize data through maps, boxplots, histograms, etc.\nFollow good software quality practices described in the software quality section of this guidance, such as pseudocoding, code review, and defensive programming.",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#tidy-data",
    "href": "docs/data-management/quality-preparation.html#tidy-data",
    "title": "Quality & Preparation",
    "section": "2 Tidy Data",
    "text": "2 Tidy Data\nRaw data rarely comes in structures compatible with your team’s analysis needs. Once the raw data has been checked for quality, additional processing may be required to start exploration and analysis.\nTidy data is a framework for data organization that facilitates efficient exploration, wrangling, and analysis (Wickham 2014). The benefits of storing data in the tidy format are:\n\nEasier data exploration and analysis\nSimpler manipulation and transformation of data\nCompatibility with data analysis tools (e.g., R and Python)\nImproved reproducibility of analysis\n\nData in this format are easy to work with, analyze, and combine with other datasets. However, once analyses and data merges start taking place, the structure of newly generated datasets are likely to be more complex and dependent upon the modeling or analysis needs. Discuss the role of tidy data with your team, and if/when in the process project datasets should deviate from the tidy data structure.\n\n2.1 The Three Core Principles of Tidy Data\n\nEach variable forms a column: In a tidy dataset, each variable has its own dedicated column. This means all values associated with that variable are listed vertically within that single column. These are also often referred to as fields or attributes.\nEach observation forms a row: Define an observation and emphasize that each row should represent a single data point. In a tidy dataset, each observation occupies a single row in the table. All the information pertaining to that specific observation is listed horizontally across the columns. These are also often referred to as records.\nEach type of observational unit forms a table: In a tidy dataset, data pertaining to different types of observational units should be separated into distinct tables.\n\n\n\n\nimage from https://r4ds.had.co.nz/tidy-data.html#tidy-data-1\n\n\n\n\n2.2 Practical applications\n\nR: The R tidyverse is a set of R packages designed to work together within the tidy data framework. It includes dplyr, readr, ggplot2, and other packages useful for wrangling data.\nPython: The Python pandas library is useful for creating and working with tidy data, as it uses data frames and includes functions for cleaning, transforming, and manipulating data.\nJulia: The DataFrames.jl library is useful for working with tabular tidy data, and has many powerful tools for manipulating data. The Tidier.jl framework builds on DataFrames.jl and emulates the R tidyverse.\n\n\n\n2.3 Resources\n\nGeneral:\n\nIntroduction to Data Wrangling and Tidying | Codecademy\nA Gentle Introduction to Tidy Data in R | by Arimoro Olayinka | Medium\n\nR focus:\n\nTidy data | tidyr\nData tidying – R for Data Science\nHelpful libraries and functions:\n\ntidyr::separate\njanitor::clean_names\n\n\nPython focus:\n\nTidy Data in Python",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "href": "docs/data-management/quality-preparation.html#data-type-conversion-and-standardization",
    "title": "Quality & Preparation",
    "section": "3 Data Type Conversion and Standardization",
    "text": "3 Data Type Conversion and Standardization\nData types, which define how data is stored and interpreted, were presented in the previous section. This section introduces data type conversion and standardization in ensuring consistent and meaningful analysis.\nData type conversion, or typecasting, is the process of transforming a value from one data type to another. Converting data types ensures compatibility between different datasets and allows for proper analysis. For example, converting age values from string (“25 years”) to integer (25) enables mathematical operations.\nWarning: Data type conversion can sometimes lead to loss of information, so it’s crucial to understand the implications of conversion before applying it. Examples:\n\nWhen converting the age column from string (“25 years”) to integer (25), information about the unit (years) was lost.\nConverting a float (2.96) to an integer (3) truncates decimals.\nIf a date is formatted as “DD/MM/YYYY” (03/12/2015) but is mistakenly interpreted as “MM/DD/YYYY” during conversion to “YYYY-MM-DD”, the resulting date will be incorrect (2015-03-12, instead of the accurate 2015-12-03).\n\nProper type conversion ensures data is correctly interpreted and can prevent errors in calculations, data analysis, and visualization.\n\n3.1 Key Points for Type Conversion\n\nUnderstand the source and target types: Knowing the data types involved in conversion helps ensure accurate transformations.\nHandle missing or invalid data: Make sure to manage missing or improperly formatted data that could cause errors during conversion.\nTest conversions: Always verify that conversions produce the expected results to avoid downstream errors in your analysis.\nRemember to always use the ISO 8601 standard format for dates (YYYY-MM-DD).\nBe aware that importing, reimporting, or saving files in certain formats can lead to loss or changes in column data types. For instance, saving data in CSV format often results in date columns being interpreted as text upon re-import, or numeric columns losing precision. This issue arises because formats like CSV lack built-in metadata to store data types, meaning they rely on the importing program to infer types, which can cause inconsistencies and data integrity issues over time. In these cases, columns may need to be re-typecast.\n\n\n\n3.2 Resources\n\nPlotting and Programming in Python: Data Types and Type Conversion\nR for Data Science - Transform",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "href": "docs/data-management/quality-preparation.html#preparation-for-analysis",
    "title": "Quality & Preparation",
    "section": "4 Preparation for Analysis",
    "text": "4 Preparation for Analysis\nAdditional cleaning and transformation steps (often referred to as “data wrangling”) are often necessary, and are highly variable depending on project needs. Examples include:\n\nfiltering based on criteria\nrestructuring/reshaping/pivoting\nremoving duplicates\ncorrecting errors in the source data (e.g. misspellings)\nmerging\n\n\n4.1 Resources\nThe approach depends on specific project and data needs. The following resources go into greater detail, with examples:\n\nR\n\nNCEAS Learning Hub’s coreR Course - 7 Cleaning & Wrangling Data (ucsb.edu)\nTransform – R for Data Science (2e) (hadley.nz)\nR for Reproducible Scientific Analysis: Subsetting Data (swcarpentry.github.io)\nR for Reproducible Scientific Analysis: Data Frame Manipulation with dplyr (swcarpentry.github.io)\n\nPython\n\nData Analysis and Visualization in Python for Ecologists: Indexing, Slicing and Subsetting DataFrames in Python (datacarpentry.org)\nData Analysis and Visualization in Python for Ecologists: Combining DataFrames with Pandas (datacarpentry.org)",
    "crumbs": [
      "Data Management",
      "Quality & Preparation"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html",
    "href": "docs/data-management/sensitive-proprietary.html",
    "title": "Sensitive & Proprietary Data",
    "section": "",
    "text": "When using sensitive or proprietary data in your research project, it’s crucial to ensure data security, privacy, and compliance with any agreements or regulations governing its use. There are five steps to addressing this.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "href": "docs/data-management/sensitive-proprietary.html#identify-and-categorize-sensitive-data",
    "title": "Sensitive & Proprietary Data",
    "section": "1 Identify and categorize sensitive data",
    "text": "1 Identify and categorize sensitive data\nDetermine if any data used in your project is subject to data use agreements, or are otherwise sensitive or proprietary.\n\nTo determine whether any of your project data is sensitive, consider the following:\n\nWas the data acquired through special means, such as a purchase, personal contact, subscription, or a data use agreement?\nDoes the data include:\n\nIdentifying information about individuals (e.g., names, addresses, personal records)?\nSensitive environmental information (e.g., locations of endangered species, private property soil samples)?\nSensitive infrastructure information (e.g., detailed electricity grid data)?\nSensitive economic information (e.g. trade data)\nInformation concerning sovereign tribal governments or vulnerable communities?\n\nDid accessing the data require Institutional Review Board (IRB) approval or human subjects research training?\nDid accessing the data require special security training?\nWas the data collected via surveys, interviews, or focus groups?\n\nIf the answer to any of these questions was Yes, classify the sensitivity of the data into one or more of three categories:\n\nProprietary Data has been paid for or for which special access has been granted. This type of data is often owned by a third party and comes with specific use restrictions, such as licensing agreements or purchase conditions.\nRegulated Data is governed by specific regulations or laws, such as federal or state laws, Institutional Review Board (IRB) regulations, or other oversight requirements. This includes data that involves privacy concerns, such as personally identifiable information (PII) or data subject to HIPAA or GDPR compliance.\nConfidential Data is sensitive due to its content or potential impact if disclosed. This includes data on sensitive environmental information, sensitive infrastructure details, or vulnerable communities.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "href": "docs/data-management/sensitive-proprietary.html#document-data-sensitivity-and-restrictions",
    "title": "Sensitive & Proprietary Data",
    "section": "2 Document data sensitivity and restrictions",
    "text": "2 Document data sensitivity and restrictions\n\nDocument data sensitivity class and details in both the project-level README and, if the data are secondary, the associated raw data README file. Include details about the data’s source, use restrictions, and sensitivity.\nKeep Track of Data Agreements: Maintain organized and secure digital copies of all data use agreements, licenses, and contracts. These should be easily accessible to those managing the data.\nCheck with data providers or experts for recommended security measures",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "href": "docs/data-management/sensitive-proprietary.html#determine-appropriate-security-and-privacy-measures",
    "title": "Sensitive & Proprietary Data",
    "section": "3 Determine appropriate security and privacy measures",
    "text": "3 Determine appropriate security and privacy measures\n\nContact IT to inform them of your data sensitivity and ask for guidance on ensuring the sensitive data is backed up and secure. Implement suitable security measures based on the sensitivity of the data. This may include storing sensitive data in read-only folders accessible only to authorized team members. It is important for IT to know ahead of time if data need to be deleted, so that backups can be managed.\nEnsure all current and prospective team members are aware of data use and sharing constraints. Include sensitivity documentation when sharing data with outside collaborators.\nDo not version control sensitive data, only the code that processes it. Using version control on sensitive data makes it difficult to delete comprehensively.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "href": "docs/data-management/sensitive-proprietary.html#data-derivatives-masking-and-aggregation",
    "title": "Sensitive & Proprietary Data",
    "section": "4 Data derivatives: masking and aggregation",
    "text": "4 Data derivatives: masking and aggregation\n\nData derivatives are transformed versions of original datasets, generated through processes such as aggregation, summarization, and integration with other data sources. If the raw data is subject to sensitivity restrictions, additional precautions may be necessary when sharing these derivatives.\nExample techniques are shown below. These are not always applicable and specific techniques vary case by case. Sometimes it’s necessary to match specific requirements for proprietary/licensed data, which might be different from those listed here.\n\nStatistical Disclosure Control: Ensure that summary statistics or figures can’t be reverse-engineered to re-create the sensitive data. Specific requirements might vary by data agreements.\nGeneralization and Anonymization: When developing derivatives from sensitive data, use generalization techniques to obscure sensitive details, such as aggregating location data into broader regions rather than showing exact points.\nStorage Considerations: Ensure that any derived datasets are stored securely and in compliance with applicable data protection regulations. Implement access controls to restrict who can view or modify these datasets.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "href": "docs/data-management/sensitive-proprietary.html#review-before-publication-or-sharing",
    "title": "Sensitive & Proprietary Data",
    "section": "5 Review before publication or sharing",
    "text": "5 Review before publication or sharing\nRevisit and review data sensitivity documentation and agreements prior to sharing or publishing derived data products (data, figures, results). Ensure the whole research team agrees that sharing would not violate data sensitivity agreements or security measures. If appropriate, check with the data provider or expert before moving forward with publication.",
    "crumbs": [
      "Data Management",
      "Sensitive & Proprietary Data"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html",
    "href": "docs/data-management/documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).\n\n\n\nImage from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#introduction",
    "href": "docs/data-management/documentation.html#introduction",
    "title": "Documentation",
    "section": "",
    "text": "“If we are not conscientious documenters, we can easily end up… without the ability to coherently describe our research process up to that point” (Stoudt, Vásquez, and Martinez (2021)).\nQuality documentation is critical for ensuring that your work is understandable, reusable, and interpretable over time by external users, your colleagues, and your future self. It reduces errors, facilitates smooth project team transitions, and helps avoid confusion and duplication of efforts.\nWithout documentation, projects lose their usefulness over time, as illustrated below (see also Michener et al. 1997).\n\n\n\nImage from Michener et al. 1997",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#types-of-documentation",
    "href": "docs/data-management/documentation.html#types-of-documentation",
    "title": "Documentation",
    "section": "2 Types of documentation",
    "text": "2 Types of documentation\nThis summary can help guide team conversations around documentation strategies.\n\n2.1 Preliminary\nPreliminary documentation refers to early-stage descriptions created during the planning phase. This often includes a data management plan (DMP), which outlines data collection, organization, storage, and sharing strategies.\n\n\n2.2 Process\nProcess documentation involves capturing step-by-step procedures and workflows used to collect, process, analyze, and model data, including:\n\nDocumenting raw data sources\nDocumenting methods with code comments\nDocumenting methods with version control tools\n\n\n\n2.3 Product\nProduct documentation provides information to accompany code and data of completed projects, ensuring usability and transparency. This documentation can be a part of or accompanying written products.",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/data-management/documentation.html#core-documentation",
    "href": "docs/data-management/documentation.html#core-documentation",
    "title": "Documentation",
    "section": "3 Core documentation",
    "text": "3 Core documentation\n\n3.1 The project-level README file\nAt a minimum, the project should have a README file in text or markdown format with the information listed below. This can be an evolving, living document. While it’s technically product documentation, it’s easiest to start it early in the project’s development.\n\nProject name and description\nProject PI and contact information\nList of staff responsible for data management and code development\nAssociated final product(s), date of release, and DOI (if applicable)\nLink to published data/code (if applicable)\nLicense associated with final product(s)\nNature of sensitive or proprietary data (if applicable)\nAny other important notes for navigating folder or using data/code\n\n\n\n3.2 Raw data\nBest practices:\n\nThe source of all downloaded raw datasets should be documented in a README file.\nCreate a README file associated with each raw data file, or each logical “cluster” of related raw data files, in the same folder as the data.\nIf there are multiple data files in a folder, name the README so that it is easily associated with the data file(s) it describes (e.g., README_PRISM_Daily_Temperature.txt).\nFormat README files consistently.\nWrite the README document in an plain text and open source file format, such as .txt or .md.\n\nBelow is a README template and example. Include in the README file the information shown in the example.\nFilename: README_PRISM_Daily_Temperature.txt\nDataset name & format: PRISM_Daily_Temperature_2024.csv\n\nData source: Downloaded from the PRISM Climate Group website: https://prism.oregonstate.edu/ \n\nDate acquired: 2024-02-28.\n\nAcquired by: [Name of researcher who downloaded the data]\n\nData description: This dataset contains daily minimum and maximum temperature data for Washington State for the year 2024, with a spatial resolution of 4km.\n\nPreprocessing: No modifications were made to the raw dataset. The file is stored exactly as downloaded from the source.\n\nLicense & Usage Restrictions: This dataset is publicly available under the PRISM Climate Group's data use policy. Refer to https://prism.oregonstate.edu/documents/PRISM_terms_of_use.pdf for more details.\n\n\n\n\n\n\nNote\n\n\n\nAs described in the Organization section, all raw data should be retained in their raw form and not directly modified.\n\n\n\n\n3.3 Methods\nThere are two critical strategies for documenting methods effectively:\n\nUse version control to track and explain changes to scripts over time (e.g., through clear and consistent commit messages).\nUse code comments to document the purpose and logic of scripts and key functions, helping others (and your future self) understand how the code works.",
    "crumbs": [
      "Data Management",
      "Documentation"
    ]
  },
  {
    "objectID": "docs/foundations/index.html",
    "href": "docs/foundations/index.html",
    "title": "Foundations",
    "section": "",
    "text": "Good data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):\n\nShort-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#introduction",
    "href": "docs/foundations/index.html#introduction",
    "title": "Foundations",
    "section": "",
    "text": "Good data practices can not only save time and headaches but increase the usefulness of your data and code and enhance the reproducibility of the whole project. Good data practices provide (Langseth et al. (2015)):\n\nShort-term benefits\n\nAllow you to spend less time doing data management and more time doing research\nMake it easier to prepare and use data\nEnsure that collaborators can readily understand and use data files\n\nLong-term benefits\n\nMake your work more transparent, reproducible, and rigorous\nAllow other researchers to find, understand, and use your data to address broad questions\nEnsure that you get credit for data products and for their use in other products",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#the-data-life-cycle",
    "href": "docs/foundations/index.html#the-data-life-cycle",
    "title": "Foundations",
    "section": "The data life cycle",
    "text": "The data life cycle\nA common axiom among data scientists is the application of the 80/20 rule to effort: 80% of time is spent wrangling (managing and preparing) data, while 20% is spent on analysis. Most activities in the data life cycle come before the analysis phase and are closely tied to data management. There are many different models of the data life cycle, and the relevant model for your individual project will vary. A general data life cycle is depicted below (see also Langseth et al. 2015).\n\n\n\n\n\ngraph LR\nA(Plan) --&gt; B(Collect)\nB --&gt; C(Process)\nC --&gt; D(Explore / Visualize)\nD --&gt; E(Analyze / Model)\nE --&gt; F(Archive, Publish, Share)\nE --&gt; C\nE --&gt; A\n\n\n\n\n\n\nThe data life cycle is often iterative and nonlinear, and does not always follow the order shown. Your actual analysis workflow may include dead ends or repeated steps. Regardless, it is helpful to plan and discuss your data-oriented research using these common components of the data life cycle:\n\nStep 1: Plan Identify data that will be collected and how it will be managed. Create a data management plan.\nStep 2: Collect Acquire and store raw data.\n\na. Acquire Retrieve data from the appropriate source.\nb. Describe Document the raw data source, format, variables, measurement units, coded values, and known problems.\nc. Quality assurance Inspect the raw data for quality and fit for analysis purpose.\nd. Store Store the raw data in the appropriate folder, as determined in the planning stage. Consider access, resilience (backing up), security, and, if relevant, data agreement stipulations. Make raw data files read-only so they cannot be accidentally modified.\n\nStep 3: Process Prepare the data for exploration and analysis.\n\na. Clean Preprocess the data to correct errors, standardize missing values, standardize formats, etc.\nb. Transform Convert data into appropriate format and spatiotemporal scale (e.g., convert daily values to annual statistics).\nc. Integrate Combine datasets.\n\nStep 4: Explore Describe, summarize, and visualize statistics and relationships.\nStep 5: Analyze / Model Develop, refine, and implement analysis and model specifications.\nStep 6: Archive, Publish, and Share Finalize documentation (project-level README and metadata). Dispose and/or archive data. Publish final data products and documentation.",
    "crumbs": [
      "Foundations"
    ]
  },
  {
    "objectID": "docs/foundations/index.html#sec-dmp",
    "href": "docs/foundations/index.html#sec-dmp",
    "title": "Foundations",
    "section": "The first step: Data management planning for reproducibility",
    "text": "The first step: Data management planning for reproducibility\nData management planning, a form of preliminary documentation, is the process of thinking ahead about how your team will access, use, create, modify, store, share, and describe data related to your research project. Data management plans (DMPs) enhance collaboration by establishing baseline expectations, make projects resilient to turnover, and save time in the long run. In addition, many funders require data management plans be submitted with grant proposals, so thinking about these issues early can facilitate the proposal process.\nThis guidance resource provides general instructions for data practices and addresses many of the core questions that are part of the DMP process. At a minimum, the questions below should be reviewed at the start of a project. Associated guidance is linked.\n\nWhere will data/code be stored and how will it be organized?\nHow will the team use Microsoft Teams for file storage and communication? What types of files will be stored in the Teams folder versus the project’s L:/ drive folder? What will be communicated over Teams chat versus email or GitHub?\nWho will be responsible for disposing / archiving data?\nWho will be responsible for publishing data/code and attaching appropriate documentation and use licenses?\nWhat will the version control / git / GitHub workflow be?\nWhat coding software and main libraries will be used?\nHow will code be reviewed for quality?\nWhat are expectations around data quality and code quality?\nHow will data sources, code, and major methodological decisions be documented?\nHas the appropriate budget been allocated to implement this DMP?\n\nCertain projects may require additional considerations. For development of a more thorough DMP, refer to the UCSB NCEAS data management planning section.",
    "crumbs": [
      "Foundations"
    ]
  }
]